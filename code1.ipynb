{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from textblob import TextBlob\n",
    "import language_check\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "\n",
       "   domain1_score  \n",
       "0       6.666667  \n",
       "1       7.500000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('test2.xlsx')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  Dear local newspaper, I think effects computer...   \n",
      "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "   domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "0       6.666667       338.0            16.0         4.550296   \n",
      "1       7.500000       419.0            20.0         4.463007   \n",
      "2       5.833333       279.0            14.0         4.526882   \n",
      "3       8.333333       524.0            27.0         5.041985   \n",
      "4       6.666667       465.0            30.0         4.526882   \n",
      "\n",
      "   num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
      "0                    4.0                 2.0          168.0      ...         \n",
      "1                    1.0                 1.0          189.0      ...         \n",
      "2                    0.0                 0.0          140.0      ...         \n",
      "3                    2.0                 1.0          222.0      ...         \n",
      "4                    0.0                 0.0          236.0      ...         \n",
      "\n",
      "   verb_count  foreign_count  adj_count  conj_count  adv_count  beauty_score  \\\n",
      "0        39.0            0.0       13.0        69.0       22.0           NaN   \n",
      "1        56.0            0.0       14.0        80.0       21.0           NaN   \n",
      "2        33.0            0.0       13.0        50.0       17.0           NaN   \n",
      "3        57.0            0.0       29.0        84.0       30.0           NaN   \n",
      "4        60.0            0.0       18.0        63.0       41.0           NaN   \n",
      "\n",
      "   maturity_score  vocab  sentiment_essay  Grammar_check  \n",
      "0             NaN    NaN         0.310471             11  \n",
      "1             NaN    NaN         0.274000             19  \n",
      "2             NaN    NaN         0.340393              9  \n",
      "3             NaN    NaN         0.266828             35  \n",
      "4             NaN    NaN         0.199684             17  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "num_rows = df.shape[0]\n",
    "essays = df['essay'].values\n",
    "\n",
    "#Initialize dataframe columns\n",
    "df['word_count'] = np.nan \n",
    "df['sentence_count'] = np.nan\n",
    "df['avg_word_length'] = np.nan \n",
    "df['num_exclamation_marks'] = np.nan\n",
    "df['num_question_marks'] = np.nan\n",
    "df['num_stopwords'] = np.nan\n",
    "\n",
    "df['noun_count'] = np.nan\n",
    "df['verb_count'] = np.nan\n",
    "df['foreign_count'] = np.nan\n",
    "df['adj_count'] = np.nan\n",
    "df['conj_count'] = np.nan\n",
    "df['adv_count'] = np.nan\n",
    "df['beauty_score'] = np.nan\n",
    "df['maturity_score'] = np.nan\n",
    "df['vocab'] = np.nan\n",
    "def get_pos_tags(essay):\n",
    "    nouns = verbs = foreign = adj = adv = conj = 0\n",
    "    tokens = nltk.word_tokenize(essay)\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag(nltk.word_tokenize(token))\n",
    "        for (_, tag) in (pos_tag):\n",
    "            if tag[0] == \"N\":\n",
    "                nouns += 1\n",
    "            elif tag[0] == \"V\":\n",
    "                verbs += 1\n",
    "            elif tag[0:2] == \"FW\":\n",
    "                foreign += 1\n",
    "            elif tag[0] == \"J\":\n",
    "                adj += 1\n",
    "            elif tag[0] == \"R\":\n",
    "                adv += 1\n",
    "            elif tag[0:2] == \"CC\" or tag[0:2] == \"IN\":\n",
    "                conj += 1\n",
    "    \n",
    "    return [nouns, verbs, foreign, adj, adv, conj]\n",
    "\n",
    "\n",
    "for i in range(num_rows):\n",
    "    \n",
    "    # Turn essay into list of words\n",
    "    text = essays[i].split(\" \")\n",
    "    \n",
    "    # Set word count\n",
    "    df.set_value(i,'word_count', len(text))\n",
    "    \n",
    "    # Sentence count\n",
    "    df.set_value(i, 'sentence_count', len(nltk.tokenize.sent_tokenize(essays[i])))\n",
    "    \n",
    "    # Average word length\n",
    "    word_len = sum(len(word) for word in text) / len(text)\n",
    "    df.set_value(i, 'avg_word_length', word_len)\n",
    "    \n",
    "    # Number of exclamation marks\n",
    "    df.set_value(i, \"num_exclamation_marks\", sum(word.count(\"!\") for word in essays[i]))\n",
    "    \n",
    "    # Number of question marks\n",
    "    df.set_value(i, \"num_question_marks\", sum(word.count(\"?\") for word in essays[i]))\n",
    "    \n",
    "    # Number of stop words\n",
    "    df.set_value(i, \"num_stopwords\", sum([1 for word in text if word.lower() in stopwords]))\n",
    "\n",
    "    \n",
    "    # POS tag counts\n",
    "    pos_lst = get_pos_tags(essays[i])\n",
    "    df.set_value(i,'noun_count', pos_lst[0])\n",
    "    df.set_value(i,'verb_count', pos_lst[1])\n",
    "    df.set_value(i,'foreign_count', pos_lst[2])\n",
    "    df.set_value(i,'adj_count', pos_lst[3])\n",
    "    df.set_value(i,'adv_count', pos_lst[4])\n",
    "    df.set_value(i,'conj_count', pos_lst[5])\n",
    "def avg_sentence_sentiment(x):\n",
    "    sentiment_essay = TextBlob(x).sentiment.polarity\n",
    "    return sentiment_essay\n",
    "df['sentiment_essay'] = df['essay'].apply(avg_sentence_sentiment)\n",
    "def grammar_check(x):\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "    matches = tool.check(x)\n",
    "    return len(matches)\n",
    "df['Grammar_check'] = df['essay'].apply(grammar_check)\n",
    "\n",
    "\n",
    "print (df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = json.load(open('aoa_values.json'))\n",
    "dataform = str(temp).strip(\"'<>()\").replace('\\'','\\\"')\n",
    "words_acq_age = json.loads(dataform)\n",
    "\n",
    "# words_acq_age = json.loads('aoa_values.json')\n",
    "enchant_dict = enchant.Dict(\"en_US\")\n",
    "beauty_reference = {\n",
    "    'a' : 8.12, 'b' : 1.49, 'c' : 2.71, 'd' : 4.32, 'e' : 12.02, 'f' : 2.30,\n",
    "    'g' : 2.03, 'h' : 5.92, 'i' : 7.31, 'j' : 0.10, 'k' : 0.69, 'l' : 3.98,\n",
    "    'm' : 2.61, 'n' : 6.95, 'o' : 7.68, 'p' : 1.82, 'q' : 0.11, 'r' : 6.02,\n",
    "    's' : 1.68, 't' : 9.10, 'u' : 2.88, 'v' : 1.11, 'w' : 2.09, 'x' : 0.17,\n",
    "    'y' : 2.11, 'z' : 0.07,\n",
    "}\n",
    "\n",
    "\n",
    "def find_BScore(essay):\n",
    "    bs=0\n",
    "    tokenizer= RegexpTokenizer(r'\\w+')\n",
    "    words=tokenizer.tokenize(essay)\n",
    "    for word in words:\n",
    "        s = 1.0\n",
    "        for letter in word:\n",
    "           try:\n",
    "             s = s*beauty_reference[letter.lower()]\n",
    "           except:\n",
    "             pass\n",
    "           bs += 1/s\n",
    "    return bs\n",
    "\n",
    "BS=[]\n",
    "for i in range(num_rows):\n",
    "    BS.append(find_BScore(essays[i]))\n",
    "df.beauty_score=BS\n",
    "\n",
    "        \n",
    "def find_MScore(essay):        \n",
    "   vocab = 0\n",
    "   ms=0\n",
    "   tokenizer= RegexpTokenizer(r'\\w+')\n",
    "   words=tokenizer.tokenize(essay)\n",
    "   for word in words:\n",
    "       lower_word = word.lower()\n",
    "       if lower_word in words_acq_age and len(lower_word) > 3:\n",
    "             if words_acq_age[lower_word] != \"NA\":\n",
    "                 ms = ms + float(words_acq_age[lower_word])\n",
    "                 vocab += 1\n",
    "       if ms>0:\n",
    "         ms /= vocab\n",
    "   return ms\n",
    "        \n",
    "MS=[]\n",
    "for i in range(num_rows):\n",
    "    MS.append(find_MScore(essays[i]))\n",
    "df.maturity_score=MS\n",
    "\n",
    "def voc(essay):        \n",
    "   vocab = 0\n",
    "   tokenizer= RegexpTokenizer(r'\\w+')\n",
    "   words=tokenizer.tokenize(essay)\n",
    "   for word in words:\n",
    "       lower_word = word.lower()\n",
    "       if lower_word in words_acq_age and len(lower_word) > 3:\n",
    "             vocab += 1\n",
    "   return vocab\n",
    "        \n",
    "V=[]\n",
    "for i in range(num_rows):\n",
    "    V.append(voc(essays[i]))\n",
    "df.vocab=V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>foreign_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>conj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>beauty_score</th>\n",
       "      <th>maturity_score</th>\n",
       "      <th>vocab</th>\n",
       "      <th>sentiment_essay</th>\n",
       "      <th>Grammar_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>338.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>175.314893</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>183</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>419.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>239.987278</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>217</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>279.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>132.356485</td>\n",
       "      <td>0.060025</td>\n",
       "      <td>146</td>\n",
       "      <td>0.340393</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>524.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>239.253012</td>\n",
       "      <td>0.019373</td>\n",
       "      <td>271</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>465.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>260.246296</td>\n",
       "      <td>0.021028</td>\n",
       "      <td>249</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  word_count  sentence_count  avg_word_length  \\\n",
       "0       6.666667       338.0            16.0         4.550296   \n",
       "1       7.500000       419.0            20.0         4.463007   \n",
       "2       5.833333       279.0            14.0         4.526882   \n",
       "3       8.333333       524.0            27.0         5.041985   \n",
       "4       6.666667       465.0            30.0         4.526882   \n",
       "\n",
       "   num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
       "0                    4.0                 2.0          168.0      ...         \n",
       "1                    1.0                 1.0          189.0      ...         \n",
       "2                    0.0                 0.0          140.0      ...         \n",
       "3                    2.0                 1.0          222.0      ...         \n",
       "4                    0.0                 0.0          236.0      ...         \n",
       "\n",
       "   verb_count  foreign_count  adj_count  conj_count  adv_count  beauty_score  \\\n",
       "0        39.0            0.0       13.0        69.0       22.0    175.314893   \n",
       "1        56.0            0.0       14.0        80.0       21.0    239.987278   \n",
       "2        33.0            0.0       13.0        50.0       17.0    132.356485   \n",
       "3        57.0            0.0       29.0        84.0       30.0    239.253012   \n",
       "4        60.0            0.0       18.0        63.0       41.0    260.246296   \n",
       "\n",
       "   maturity_score  vocab  sentiment_essay  Grammar_check  \n",
       "0        0.029508    183         0.310471             11  \n",
       "1        0.023986    217         0.274000             19  \n",
       "2        0.060025    146         0.340393              9  \n",
       "3        0.019373    271         0.266828             35  \n",
       "4        0.021028    249         0.199684             17  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "writer = ExcelWriter('features.xlsx')\n",
    "df.to_excel(writer,'Sheet1',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>foreign_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>conj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>beauty_score</th>\n",
       "      <th>maturity_score</th>\n",
       "      <th>vocab</th>\n",
       "      <th>sentiment_essay</th>\n",
       "      <th>Grammar_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.300806</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.222340</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.296625</td>\n",
       "      <td>0.255864</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.053007</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.300330</td>\n",
       "      <td>0.655235</td>\n",
       "      <td>0.085938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.373321</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.216791</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.333925</td>\n",
       "      <td>0.315565</td>\n",
       "      <td>0.301075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.072601</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.247986</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.220851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246892</td>\n",
       "      <td>0.234542</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.160377</td>\n",
       "      <td>0.039991</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.239274</td>\n",
       "      <td>0.670196</td>\n",
       "      <td>0.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.467323</td>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.253594</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.392540</td>\n",
       "      <td>0.560768</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.072378</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.445545</td>\n",
       "      <td>0.633414</td>\n",
       "      <td>0.273438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.220851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417407</td>\n",
       "      <td>0.319829</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>0.078739</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.409241</td>\n",
       "      <td>0.599842</td>\n",
       "      <td>0.132812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain1_score  word_count  sentence_count  avg_word_len  \\\n",
       "0       0.666667    0.300806        0.157895      0.222340   \n",
       "1       0.750000    0.373321        0.200000      0.216791   \n",
       "2       0.583333    0.247986        0.136842      0.220851   \n",
       "3       0.833333    0.467323        0.273684      0.253594   \n",
       "4       0.666667    0.414503        0.305263      0.220851   \n",
       "\n",
       "   num_exclamation_marks  num_question_marks  num_stopwords  noun_count  \\\n",
       "0               0.148148            0.014493       0.296625    0.255864   \n",
       "1               0.037037            0.007246       0.333925    0.315565   \n",
       "2               0.000000            0.000000       0.246892    0.234542   \n",
       "3               0.074074            0.007246       0.392540    0.560768   \n",
       "4               0.000000            0.000000       0.417407    0.319829   \n",
       "\n",
       "   verb_count  foreign_count  adj_count  conj_count  adv_count  beauty_score  \\\n",
       "0    0.209677            0.0   0.188406    0.328571   0.207547      0.053007   \n",
       "1    0.301075            0.0   0.202899    0.380952   0.198113      0.072601   \n",
       "2    0.177419            0.0   0.188406    0.238095   0.160377      0.039991   \n",
       "3    0.306452            0.0   0.420290    0.400000   0.283019      0.072378   \n",
       "4    0.322581            0.0   0.260870    0.300000   0.386792      0.078739   \n",
       "\n",
       "   maturity_score     vocab  sentiment_essay  Grammar_check  \n",
       "0        0.003472  0.300330         0.655235       0.085938  \n",
       "1        0.002822  0.356436         0.637000       0.148438  \n",
       "2        0.007062  0.239274         0.670196       0.070312  \n",
       "3        0.002279  0.445545         0.633414       0.273438  \n",
       "4        0.002474  0.409241         0.599842       0.132812  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "df = df.fillna(0);\n",
    "# where_are_NaNs = np.isnan(df)\n",
    "# df[where_are_NaNs] = 0\n",
    "\n",
    "np_scaled = min_max_scaler.fit_transform(df.drop(['essay','essay_id','essay_set'],axis=1))\n",
    "df_normalized = pd.DataFrame(np_scaled, columns = ['domain1_score','word_count','sentence_count','avg_word_len','num_exclamation_marks','num_question_marks','num_stopwords','noun_count','verb_count','foreign_count','adj_count','conj_count','adv_count','beauty_score','maturity_score','vocab','sentiment_essay','Grammar_check'])\n",
    "df_normalized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x = df.drop(['domain1_score', 'essay','essay_id','essay_set'], axis=1)\n",
    "# y = df['domain1_score']\n",
    "\n",
    "x = df_normalized.drop(['domain1_score'],axis=1)\n",
    "# # df['A']=df['A'].fillna(0.0).astype(int)\n",
    "y = df['domain1_score'].fillna(0.0).astype(int)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "where_are_NaNs = np.isnan(x)\n",
    "x[where_are_NaNs] = 0\n",
    "where_are_NaNs = np.isnan(y)\n",
    "y[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kfold.split(x, y):\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "logistic_reg = LogisticRegression()\n",
    "logistic_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier accuracy: 0.379483224065\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_reg.predict(X_test)\n",
    "print('Logistic regression classifier accuracy:', logistic_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kfold.split(x, y):\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression classifier accuracy: 0.270323426626\n"
     ]
    }
   ],
   "source": [
    "predictions = lin_reg.predict(X_test)\n",
    "print('Linear regression classifier accuracy:', lin_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (linear) Regressor Accuracy: 0.249024921898\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVR(kernel=\"linear\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('SVM (linear) Regressor Accuracy:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestRegressor(random_state=0,n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Accuracy: 0.504137875794\n"
     ]
    }
   ],
   "source": [
    "print ('Random Forest Regressor Accuracy:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (rbf) Regressor Accuracy: 0.315502457661\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR(kernel=\"rbf\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('SVM (rbf) Regressor Accuracy:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_excel('test2.xlsx')\n",
    "X=X.drop(X.columns[0:2],axis=1)\n",
    "y = pd.DataFrame(X['domain1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  domain1_score\n",
       "0  Dear local newspaper, I think effects computer...       6.666667\n",
       "1  Dear @CAPS1 @CAPS2, I believe that using compu...       7.500000\n",
       "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...       5.833333\n",
       "3  Dear Local Newspaper, @CAPS1 I have found that...       8.333333\n",
       "4  Dear @LOCATION1, I know having computers has a...       6.666667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain1_score\n",
       "0       6.666667\n",
       "1       7.500000\n",
       "2       5.833333\n",
       "3       8.333333\n",
       "4       6.666667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential([\n",
    "        # 2D tensor for first layer: 1 timestep and 300 features\n",
    "        LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True),\n",
    "        LSTM(64, recurrent_dropout=0.4),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def essay_to_list(essay):\n",
    "    # Remove the tags\n",
    "    essay = re.sub(\"[^a-zA-Z]\", \" \", essay)\n",
    "    words = essay.lower().split()\n",
    "    return [w for w in words if not w in stopwords]\n",
    "\n",
    "def essay_to_sentences(essay):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_list(raw_sentence))\n",
    "    return sentences\n",
    "\n",
    "# Generate feature vector for the words\n",
    "def get_feature_vector(words, model, num_features, vec_type=\"sum\"):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    max_vec =  np.zeros((num_features,),dtype=\"float32\")\n",
    "    min_vec =  np.ones((num_features,),dtype=\"float32\")\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            max_vec = np.maximum(model[word], feature_vector)\n",
    "            min_vec = np.minimum(model[word], feature_vector)\n",
    "            feature_vector = np.add(feature_vector, model[word]) \n",
    "    \n",
    "    # return min vector + max vector\n",
    "    if vec_type == \"min+max\":\n",
    "        return np.add(min_vec, max_vec) \n",
    "    \n",
    "    # average of vectors\n",
    "    elif vec_type == \"average\":\n",
    "        return np.divide(feature_vector, num_words)\n",
    "\n",
    "    # default: return sum of word2vec vectors\n",
    "    return feature_vector\n",
    "\n",
    "# Generate word vectors from the mdoel\n",
    "def generate_essay_vectors(essays, model, num_features, vec_type=\"sum\"):\n",
    "    essayfeature_vectors = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for (i, essay) in enumerate(essays):\n",
    "        essayfeature_vectors[i] = get_feature_vector(essay, model, num_features, vec_type)\n",
    "    return essayfeature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def train_model(X, y, dataset, vec_type=\"sum\"):\n",
    "    count = 1\n",
    "    results = []\n",
    "    \n",
    "    for train_set, test_set in dataset:\n",
    "        print(\"Fold #\", count)\n",
    "        X_test, X_train, y_test, y_train = X.iloc[test_set], X.iloc[train_set], y.iloc[test_set], y.iloc[train_set]\n",
    "        \n",
    "        train_essays = X_train['essay']\n",
    "        test_essays = X_test['essay']\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        for essay in train_essays:\n",
    "            sentences += essay_to_sentences(essay)\n",
    "                \n",
    "        # Initialize variables for word2vec model\n",
    "        num_features = 300 \n",
    "        min_word_count = 40\n",
    "        num_workers = 4\n",
    "        context = 10\n",
    "        downsampling = 1e-7\n",
    "\n",
    "        # Train the word2vec model\n",
    "        model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "        model.init_sims(replace=True)\n",
    "        \n",
    "        # Generate training vectors\n",
    "        clean_train_essays = []\n",
    "        for essay_vec in train_essays:\n",
    "            clean_train_essays.append(essay_to_list(essay_vec))\n",
    "        train_vectors = generate_essay_vectors(clean_train_essays, model, num_features, vec_type)\n",
    "        \n",
    "        # Generate test vectors\n",
    "        clean_test_essays = []\n",
    "        for essay_vec in test_essays:\n",
    "            clean_test_essays.append(essay_to_list( essay_vec))\n",
    "        test_vectors = generate_essay_vectors(clean_test_essays, model, num_features, vec_type)\n",
    "        \n",
    "        train_vectors = np.array(train_vectors)\n",
    "        test_vectors = np.array(test_vectors)\n",
    "\n",
    "        # Reshape the train and test vectors to 3 dimensions - 1 represents one timestamp \n",
    "        train_vectors = np.reshape(train_vectors, (train_vectors.shape[0], 1, train_vectors.shape[1]))\n",
    "        test_vectors = np.reshape(test_vectors, (test_vectors.shape[0], 1, test_vectors.shape[1]))\n",
    "        \n",
    "        # Call the LSTM to get the score predictions \n",
    "        lstm_model = get_model()\n",
    "        lstm_model.fit(train_vectors, y_train, batch_size=64, epochs=50)\n",
    "        y_pred = lstm_model.predict(test_vectors)\n",
    "        \n",
    "        # Round the prediction to the nearest integer\n",
    "        y_pred = np.around(y_pred)\n",
    "        \n",
    "        # Evaluate the model: quadratic kappa score of predictions against human grading\n",
    "        result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "        print(\"QWK: \", result)\n",
    "        results.append(result)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10377/10377 [==============================] - 9s 840us/step - loss: 5.6215 - mean_absolute_error: 1.7773\n",
      "Epoch 2/50\n",
      "10377/10377 [==============================] - 6s 594us/step - loss: 3.4326 - mean_absolute_error: 1.4382\n",
      "Epoch 3/50\n",
      "10377/10377 [==============================] - 6s 597us/step - loss: 3.2107 - mean_absolute_error: 1.3988\n",
      "Epoch 4/50\n",
      "10377/10377 [==============================] - 7s 635us/step - loss: 3.0376 - mean_absolute_error: 1.3583\n",
      "Epoch 5/50\n",
      "10377/10377 [==============================] - 7s 634us/step - loss: 2.8718 - mean_absolute_error: 1.3239\n",
      "Epoch 6/50\n",
      "10377/10377 [==============================] - 7s 628us/step - loss: 2.7654 - mean_absolute_error: 1.2956\n",
      "Epoch 7/50\n",
      "10377/10377 [==============================] - 6s 595us/step - loss: 2.7125 - mean_absolute_error: 1.2827\n",
      "Epoch 8/50\n",
      "10377/10377 [==============================] - 7s 720us/step - loss: 2.5759 - mean_absolute_error: 1.2455\n",
      "Epoch 9/50\n",
      "10377/10377 [==============================] - 6s 620us/step - loss: 2.4962 - mean_absolute_error: 1.2332\n",
      "Epoch 10/50\n",
      "10377/10377 [==============================] - 7s 709us/step - loss: 2.5028 - mean_absolute_error: 1.2247\n",
      "Epoch 11/50\n",
      "10377/10377 [==============================] - 7s 691us/step - loss: 2.4041 - mean_absolute_error: 1.2064\n",
      "Epoch 12/50\n",
      "10377/10377 [==============================] - 6s 577us/step - loss: 2.3556 - mean_absolute_error: 1.1959\n",
      "Epoch 13/50\n",
      "10377/10377 [==============================] - 5s 478us/step - loss: 2.3685 - mean_absolute_error: 1.1963\n",
      "Epoch 14/50\n",
      "10377/10377 [==============================] - 6s 558us/step - loss: 2.2720 - mean_absolute_error: 1.1754\n",
      "Epoch 15/50\n",
      "10377/10377 [==============================] - 5s 506us/step - loss: 2.1833 - mean_absolute_error: 1.1449\n",
      "Epoch 16/50\n",
      "10377/10377 [==============================] - 5s 476us/step - loss: 2.1584 - mean_absolute_error: 1.1447\n",
      "Epoch 17/50\n",
      "10377/10377 [==============================] - 5s 490us/step - loss: 2.0747 - mean_absolute_error: 1.1169\n",
      "Epoch 18/50\n",
      "10377/10377 [==============================] - 5s 501us/step - loss: 2.0292 - mean_absolute_error: 1.1118\n",
      "Epoch 19/50\n",
      "10377/10377 [==============================] - 5s 480us/step - loss: 1.9681 - mean_absolute_error: 1.0928\n",
      "Epoch 20/50\n",
      "10377/10377 [==============================] - 5s 483us/step - loss: 1.9469 - mean_absolute_error: 1.0783\n",
      "Epoch 21/50\n",
      "10377/10377 [==============================] - 6s 537us/step - loss: 1.8621 - mean_absolute_error: 1.0599\n",
      "Epoch 22/50\n",
      "10377/10377 [==============================] - 7s 661us/step - loss: 1.8308 - mean_absolute_error: 1.0474\n",
      "Epoch 23/50\n",
      "10377/10377 [==============================] - 6s 583us/step - loss: 1.8112 - mean_absolute_error: 1.0495\n",
      "Epoch 24/50\n",
      "10377/10377 [==============================] - 5s 468us/step - loss: 1.7370 - mean_absolute_error: 1.0253\n",
      "Epoch 25/50\n",
      "10377/10377 [==============================] - 5s 460us/step - loss: 1.6732 - mean_absolute_error: 1.0057\n",
      "Epoch 26/50\n",
      "10377/10377 [==============================] - 5s 461us/step - loss: 1.6749 - mean_absolute_error: 1.0039\n",
      "Epoch 27/50\n",
      "10377/10377 [==============================] - 7s 689us/step - loss: 1.6291 - mean_absolute_error: 0.9962\n",
      "Epoch 28/50\n",
      "10377/10377 [==============================] - 6s 572us/step - loss: 1.6064 - mean_absolute_error: 0.9795\n",
      "Epoch 29/50\n",
      "10377/10377 [==============================] - 6s 574us/step - loss: 1.5455 - mean_absolute_error: 0.9698\n",
      "Epoch 30/50\n",
      "10377/10377 [==============================] - 6s 565us/step - loss: 1.5219 - mean_absolute_error: 0.9571\n",
      "Epoch 31/50\n",
      "10377/10377 [==============================] - 6s 568us/step - loss: 1.4884 - mean_absolute_error: 0.9500\n",
      "Epoch 32/50\n",
      "10377/10377 [==============================] - 6s 568us/step - loss: 1.4729 - mean_absolute_error: 0.9421\n",
      "Epoch 33/50\n",
      "10377/10377 [==============================] - 6s 566us/step - loss: 1.4388 - mean_absolute_error: 0.9359\n",
      "Epoch 34/50\n",
      "10377/10377 [==============================] - 6s 570us/step - loss: 1.3994 - mean_absolute_error: 0.9150\n",
      "Epoch 35/50\n",
      "10377/10377 [==============================] - 6s 571us/step - loss: 1.3635 - mean_absolute_error: 0.9017\n",
      "Epoch 36/50\n",
      "10377/10377 [==============================] - 6s 570us/step - loss: 1.3480 - mean_absolute_error: 0.9029\n",
      "Epoch 37/50\n",
      "10377/10377 [==============================] - 6s 571us/step - loss: 1.3470 - mean_absolute_error: 0.9012\n",
      "Epoch 38/50\n",
      "10377/10377 [==============================] - 6s 577us/step - loss: 1.3044 - mean_absolute_error: 0.8859\n",
      "Epoch 39/50\n",
      "10377/10377 [==============================] - 6s 571us/step - loss: 1.2834 - mean_absolute_error: 0.8763\n",
      "Epoch 40/50\n",
      "10377/10377 [==============================] - 6s 576us/step - loss: 1.2904 - mean_absolute_error: 0.8830\n",
      "Epoch 41/50\n",
      "10377/10377 [==============================] - 6s 573us/step - loss: 1.2367 - mean_absolute_error: 0.8623\n",
      "Epoch 42/50\n",
      "10377/10377 [==============================] - 6s 570us/step - loss: 1.2067 - mean_absolute_error: 0.8515\n",
      "Epoch 43/50\n",
      "10377/10377 [==============================] - 6s 575us/step - loss: 1.1839 - mean_absolute_error: 0.8408\n",
      "Epoch 44/50\n",
      "10377/10377 [==============================] - 6s 590us/step - loss: 1.1954 - mean_absolute_error: 0.8467\n",
      "Epoch 45/50\n",
      "10377/10377 [==============================] - 6s 579us/step - loss: 1.1588 - mean_absolute_error: 0.8314\n",
      "Epoch 46/50\n",
      "10377/10377 [==============================] - 6s 576us/step - loss: 1.1566 - mean_absolute_error: 0.8359\n",
      "Epoch 47/50\n",
      "10377/10377 [==============================] - 6s 576us/step - loss: 1.1289 - mean_absolute_error: 0.8247\n",
      "Epoch 48/50\n",
      "10377/10377 [==============================] - 6s 581us/step - loss: 1.1074 - mean_absolute_error: 0.8122\n",
      "Epoch 49/50\n",
      "10377/10377 [==============================] - 6s 583us/step - loss: 1.0965 - mean_absolute_error: 0.8066\n",
      "Epoch 50/50\n",
      "10377/10377 [==============================] - 6s 581us/step - loss: 1.0943 - mean_absolute_error: 0.8086\n",
      "QWK:  0.731159293759\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 8s 727us/step - loss: 5.6121 - mean_absolute_error: 1.7776\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 6s 605us/step - loss: 3.4869 - mean_absolute_error: 1.4516\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 6s 588us/step - loss: 3.1874 - mean_absolute_error: 1.3899\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 6s 577us/step - loss: 3.0615 - mean_absolute_error: 1.3649\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 6s 535us/step - loss: 2.8451 - mean_absolute_error: 1.3100\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 6s 538us/step - loss: 2.8133 - mean_absolute_error: 1.3114\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 6s 538us/step - loss: 2.6817 - mean_absolute_error: 1.2754\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 6s 533us/step - loss: 2.6194 - mean_absolute_error: 1.2544\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 6s 537us/step - loss: 2.5662 - mean_absolute_error: 1.2474\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 6s 536us/step - loss: 2.4804 - mean_absolute_error: 1.2236\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 6s 537us/step - loss: 2.4830 - mean_absolute_error: 1.2191\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 6s 536us/step - loss: 2.3915 - mean_absolute_error: 1.1983\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 6s 534us/step - loss: 2.3107 - mean_absolute_error: 1.1805 2s - los\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 6s 541us/step - loss: 2.2913 - mean_absolute_error: 1.1815\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 6s 538us/step - loss: 2.1843 - mean_absolute_error: 1.1433\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 6s 539us/step - loss: 2.1882 - mean_absolute_error: 1.1514\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 6s 540us/step - loss: 2.0972 - mean_absolute_error: 1.1224\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 6s 539us/step - loss: 2.0821 - mean_absolute_error: 1.1133\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 6s 539us/step - loss: 2.0346 - mean_absolute_error: 1.1038\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 6s 537us/step - loss: 1.9646 - mean_absolute_error: 1.0869\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 6s 541us/step - loss: 1.8698 - mean_absolute_error: 1.0599\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 6s 574us/step - loss: 1.8754 - mean_absolute_error: 1.0646\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 6s 580us/step - loss: 1.8630 - mean_absolute_error: 1.0634\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 6s 581us/step - loss: 1.7806 - mean_absolute_error: 1.0380\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 6s 580us/step - loss: 1.7346 - mean_absolute_error: 1.0220\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 6s 581us/step - loss: 1.6895 - mean_absolute_error: 1.0099\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 6s 576us/step - loss: 1.6740 - mean_absolute_error: 1.0079\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 6s 580us/step - loss: 1.6305 - mean_absolute_error: 0.9970\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 6s 581us/step - loss: 1.6067 - mean_absolute_error: 0.9839\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 6s 597us/step - loss: 1.5604 - mean_absolute_error: 0.9760\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 6s 593us/step - loss: 1.5168 - mean_absolute_error: 0.9552\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 6s 584us/step - loss: 1.4780 - mean_absolute_error: 0.9466\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 6s 579us/step - loss: 1.4608 - mean_absolute_error: 0.9349\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 6s 577us/step - loss: 1.4306 - mean_absolute_error: 0.9279\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 6s 577us/step - loss: 1.3850 - mean_absolute_error: 0.9143\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 6s 581us/step - loss: 1.4062 - mean_absolute_error: 0.9210\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 6s 604us/step - loss: 1.3624 - mean_absolute_error: 0.9068\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 6s 590us/step - loss: 1.3123 - mean_absolute_error: 0.8856\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 6s 586us/step - loss: 1.2933 - mean_absolute_error: 0.8847\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 6s 582us/step - loss: 1.3041 - mean_absolute_error: 0.8852\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 6s 577us/step - loss: 1.2840 - mean_absolute_error: 0.8777\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 6s 596us/step - loss: 1.2551 - mean_absolute_error: 0.8689\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 6s 583us/step - loss: 1.2152 - mean_absolute_error: 0.8531\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 6s 587us/step - loss: 1.1889 - mean_absolute_error: 0.8456\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 7s 662us/step - loss: 1.1763 - mean_absolute_error: 0.8391\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 7s 632us/step - loss: 1.1817 - mean_absolute_error: 0.8418\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 6s 622us/step - loss: 1.1507 - mean_absolute_error: 0.8311\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 6s 617us/step - loss: 1.1217 - mean_absolute_error: 0.8214\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 6s 618us/step - loss: 1.0986 - mean_absolute_error: 0.8148\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 6s 618us/step - loss: 1.1007 - mean_absolute_error: 0.8110\n",
      "QWK:  0.746255271625\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10384/10384 [==============================] - 7s 713us/step - loss: 5.8085 - mean_absolute_error: 1.7970\n",
      "Epoch 2/50\n",
      "10384/10384 [==============================] - 6s 574us/step - loss: 3.4842 - mean_absolute_error: 1.4569\n",
      "Epoch 3/50\n",
      "10384/10384 [==============================] - 6s 574us/step - loss: 3.1558 - mean_absolute_error: 1.3867\n",
      "Epoch 4/50\n",
      "10384/10384 [==============================] - 6s 575us/step - loss: 2.9981 - mean_absolute_error: 1.3566\n",
      "Epoch 5/50\n",
      "10384/10384 [==============================] - 6s 575us/step - loss: 2.8971 - mean_absolute_error: 1.3253\n",
      "Epoch 6/50\n",
      "10384/10384 [==============================] - 6s 582us/step - loss: 2.7971 - mean_absolute_error: 1.3043\n",
      "Epoch 7/50\n",
      "10384/10384 [==============================] - 6s 573us/step - loss: 2.6503 - mean_absolute_error: 1.2693\n",
      "Epoch 8/50\n",
      "10384/10384 [==============================] - 6s 579us/step - loss: 2.5763 - mean_absolute_error: 1.2478\n",
      "Epoch 9/50\n",
      "10384/10384 [==============================] - 6s 576us/step - loss: 2.5128 - mean_absolute_error: 1.2326\n",
      "Epoch 10/50\n",
      "10384/10384 [==============================] - 6s 575us/step - loss: 2.4535 - mean_absolute_error: 1.2212\n",
      "Epoch 11/50\n",
      "10384/10384 [==============================] - 6s 576us/step - loss: 2.3759 - mean_absolute_error: 1.1959\n",
      "Epoch 12/50\n",
      "10384/10384 [==============================] - 6s 577us/step - loss: 2.3294 - mean_absolute_error: 1.1922\n",
      "Epoch 13/50\n",
      "10384/10384 [==============================] - 6s 573us/step - loss: 2.2320 - mean_absolute_error: 1.1659\n",
      "Epoch 14/50\n",
      "10384/10384 [==============================] - 6s 578us/step - loss: 2.2357 - mean_absolute_error: 1.1552\n",
      "Epoch 15/50\n",
      "10384/10384 [==============================] - 6s 578us/step - loss: 2.2206 - mean_absolute_error: 1.1588\n",
      "Epoch 16/50\n",
      "10384/10384 [==============================] - 6s 574us/step - loss: 2.1354 - mean_absolute_error: 1.1359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "10384/10384 [==============================] - 6s 535us/step - loss: 2.0171 - mean_absolute_error: 1.1064\n",
      "Epoch 18/50\n",
      "10384/10384 [==============================] - 6s 535us/step - loss: 1.9862 - mean_absolute_error: 1.0972\n",
      "Epoch 19/50\n",
      "10384/10384 [==============================] - 6s 535us/step - loss: 1.9875 - mean_absolute_error: 1.0924\n",
      "Epoch 20/50\n",
      "10384/10384 [==============================] - 6s 540us/step - loss: 1.9496 - mean_absolute_error: 1.0872\n",
      "Epoch 21/50\n",
      "10384/10384 [==============================] - 6s 536us/step - loss: 1.8827 - mean_absolute_error: 1.0646\n",
      "Epoch 22/50\n",
      "10384/10384 [==============================] - 6s 536us/step - loss: 1.8162 - mean_absolute_error: 1.0449\n",
      "Epoch 23/50\n",
      "10384/10384 [==============================] - 6s 535us/step - loss: 1.7871 - mean_absolute_error: 1.0352\n",
      "Epoch 24/50\n",
      "10384/10384 [==============================] - 6s 539us/step - loss: 1.7369 - mean_absolute_error: 1.0255\n",
      "Epoch 25/50\n",
      "10384/10384 [==============================] - 6s 534us/step - loss: 1.6456 - mean_absolute_error: 0.9936\n",
      "Epoch 26/50\n",
      "10384/10384 [==============================] - 6s 537us/step - loss: 1.6504 - mean_absolute_error: 0.9971\n",
      "Epoch 27/50\n",
      "10384/10384 [==============================] - 6s 541us/step - loss: 1.6578 - mean_absolute_error: 1.0012\n",
      "Epoch 28/50\n",
      "10384/10384 [==============================] - 6s 537us/step - loss: 1.5872 - mean_absolute_error: 0.9801\n",
      "Epoch 29/50\n",
      "10384/10384 [==============================] - 6s 536us/step - loss: 1.5572 - mean_absolute_error: 0.9701\n",
      "Epoch 30/50\n",
      "10384/10384 [==============================] - 6s 537us/step - loss: 1.4953 - mean_absolute_error: 0.9510\n",
      "Epoch 31/50\n",
      "10384/10384 [==============================] - 6s 538us/step - loss: 1.5130 - mean_absolute_error: 0.9541\n",
      "Epoch 32/50\n",
      "10384/10384 [==============================] - 6s 534us/step - loss: 1.4318 - mean_absolute_error: 0.9335\n",
      "Epoch 33/50\n",
      "10384/10384 [==============================] - 6s 566us/step - loss: 1.4115 - mean_absolute_error: 0.9282\n",
      "Epoch 34/50\n",
      "10384/10384 [==============================] - 6s 560us/step - loss: 1.4147 - mean_absolute_error: 0.9251\n",
      "Epoch 35/50\n",
      "10384/10384 [==============================] - 6s 556us/step - loss: 1.3787 - mean_absolute_error: 0.9116\n",
      "Epoch 36/50\n",
      "10384/10384 [==============================] - 6s 567us/step - loss: 1.3303 - mean_absolute_error: 0.9025\n",
      "Epoch 37/50\n",
      "10384/10384 [==============================] - 6s 561us/step - loss: 1.3353 - mean_absolute_error: 0.8998\n",
      "Epoch 38/50\n",
      "10384/10384 [==============================] - 6s 560us/step - loss: 1.3226 - mean_absolute_error: 0.8892\n",
      "Epoch 39/50\n",
      "10384/10384 [==============================] - 6s 560us/step - loss: 1.2821 - mean_absolute_error: 0.8744\n",
      "Epoch 40/50\n",
      "10384/10384 [==============================] - 6s 561us/step - loss: 1.2677 - mean_absolute_error: 0.8746\n",
      "Epoch 41/50\n",
      "10384/10384 [==============================] - 6s 562us/step - loss: 1.2126 - mean_absolute_error: 0.8533\n",
      "Epoch 42/50\n",
      "10384/10384 [==============================] - 6s 563us/step - loss: 1.1822 - mean_absolute_error: 0.8444\n",
      "Epoch 43/50\n",
      "10384/10384 [==============================] - 6s 562us/step - loss: 1.1931 - mean_absolute_error: 0.8479\n",
      "Epoch 44/50\n",
      "10384/10384 [==============================] - 6s 562us/step - loss: 1.1869 - mean_absolute_error: 0.8445\n",
      "Epoch 45/50\n",
      "10384/10384 [==============================] - 6s 568us/step - loss: 1.1457 - mean_absolute_error: 0.8347\n",
      "Epoch 46/50\n",
      "10384/10384 [==============================] - 6s 565us/step - loss: 1.1304 - mean_absolute_error: 0.8195\n",
      "Epoch 47/50\n",
      "10384/10384 [==============================] - 6s 588us/step - loss: 1.1314 - mean_absolute_error: 0.8243\n",
      "Epoch 48/50\n",
      "10384/10384 [==============================] - 6s 619us/step - loss: 1.1075 - mean_absolute_error: 0.8144\n",
      "Epoch 49/50\n",
      "10384/10384 [==============================] - 6s 611us/step - loss: 1.0956 - mean_absolute_error: 0.8123\n",
      "Epoch 50/50\n",
      "10384/10384 [==============================] - 6s 612us/step - loss: 1.0701 - mean_absolute_error: 0.8033\n",
      "QWK:  0.733609789383\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10385/10385 [==============================] - 8s 777us/step - loss: 5.7157 - mean_absolute_error: 1.7959\n",
      "Epoch 2/50\n",
      "10385/10385 [==============================] - 6s 617us/step - loss: 3.4133 - mean_absolute_error: 1.4375\n",
      "Epoch 3/50\n",
      "10385/10385 [==============================] - 6s 624us/step - loss: 3.2281 - mean_absolute_error: 1.3971\n",
      "Epoch 4/50\n",
      "10385/10385 [==============================] - 6s 615us/step - loss: 2.9981 - mean_absolute_error: 1.3540\n",
      "Epoch 5/50\n",
      "10385/10385 [==============================] - 6s 621us/step - loss: 2.9011 - mean_absolute_error: 1.3247\n",
      "Epoch 6/50\n",
      "10385/10385 [==============================] - 7s 659us/step - loss: 2.7869 - mean_absolute_error: 1.3039\n",
      "Epoch 7/50\n",
      "10385/10385 [==============================] - 7s 643us/step - loss: 2.6754 - mean_absolute_error: 1.2753\n",
      "Epoch 8/50\n",
      "10385/10385 [==============================] - 6s 621us/step - loss: 2.5938 - mean_absolute_error: 1.2528\n",
      "Epoch 9/50\n",
      "10385/10385 [==============================] - 6s 624us/step - loss: 2.5354 - mean_absolute_error: 1.2343\n",
      "Epoch 10/50\n",
      "10385/10385 [==============================] - 7s 632us/step - loss: 2.5156 - mean_absolute_error: 1.2351\n",
      "Epoch 11/50\n",
      "10385/10385 [==============================] - 7s 627us/step - loss: 2.3792 - mean_absolute_error: 1.2005\n",
      "Epoch 12/50\n",
      "10385/10385 [==============================] - 7s 648us/step - loss: 2.3595 - mean_absolute_error: 1.1977\n",
      "Epoch 13/50\n",
      "10385/10385 [==============================] - 7s 634us/step - loss: 2.3077 - mean_absolute_error: 1.1841\n",
      "Epoch 14/50\n",
      "10385/10385 [==============================] - 7s 650us/step - loss: 2.2031 - mean_absolute_error: 1.1520\n",
      "Epoch 15/50\n",
      "10385/10385 [==============================] - 6s 625us/step - loss: 2.1993 - mean_absolute_error: 1.1516\n",
      "Epoch 16/50\n",
      "10385/10385 [==============================] - 7s 627us/step - loss: 2.1751 - mean_absolute_error: 1.1469\n",
      "Epoch 17/50\n",
      "10385/10385 [==============================] - 7s 630us/step - loss: 2.0722 - mean_absolute_error: 1.1194\n",
      "Epoch 18/50\n",
      "10385/10385 [==============================] - 7s 628us/step - loss: 2.0317 - mean_absolute_error: 1.1089\n",
      "Epoch 19/50\n",
      "10385/10385 [==============================] - 6s 619us/step - loss: 2.0426 - mean_absolute_error: 1.1116\n",
      "Epoch 20/50\n",
      "10385/10385 [==============================] - 6s 588us/step - loss: 1.9702 - mean_absolute_error: 1.0876\n",
      "Epoch 21/50\n",
      "10385/10385 [==============================] - 6s 590us/step - loss: 1.9008 - mean_absolute_error: 1.0709\n",
      "Epoch 22/50\n",
      "10385/10385 [==============================] - 6s 580us/step - loss: 1.8458 - mean_absolute_error: 1.0542\n",
      "Epoch 23/50\n",
      "10385/10385 [==============================] - 6s 582us/step - loss: 1.7787 - mean_absolute_error: 1.0358\n",
      "Epoch 24/50\n",
      "10385/10385 [==============================] - 6s 585us/step - loss: 1.7553 - mean_absolute_error: 1.0279\n",
      "Epoch 25/50\n",
      "10385/10385 [==============================] - 6s 583us/step - loss: 1.7276 - mean_absolute_error: 1.0210\n",
      "Epoch 26/50\n",
      "10385/10385 [==============================] - 6s 582us/step - loss: 1.6632 - mean_absolute_error: 1.0019\n",
      "Epoch 27/50\n",
      "10385/10385 [==============================] - 6s 583us/step - loss: 1.6268 - mean_absolute_error: 0.9953\n",
      "Epoch 28/50\n",
      "10385/10385 [==============================] - 6s 537us/step - loss: 1.6268 - mean_absolute_error: 0.9925 2s - loss: 1.6\n",
      "Epoch 29/50\n",
      "10385/10385 [==============================] - 6s 538us/step - loss: 1.5569 - mean_absolute_error: 0.9704\n",
      "Epoch 30/50\n",
      "10385/10385 [==============================] - 6s 543us/step - loss: 1.5301 - mean_absolute_error: 0.9647\n",
      "Epoch 31/50\n",
      "10385/10385 [==============================] - 6s 539us/step - loss: 1.4817 - mean_absolute_error: 0.9463\n",
      "Epoch 32/50\n",
      "10385/10385 [==============================] - 6s 539us/step - loss: 1.4587 - mean_absolute_error: 0.9396\n",
      "Epoch 33/50\n",
      "10385/10385 [==============================] - 6s 539us/step - loss: 1.4301 - mean_absolute_error: 0.9275\n",
      "Epoch 34/50\n",
      "10385/10385 [==============================] - 6s 539us/step - loss: 1.4177 - mean_absolute_error: 0.9206\n",
      "Epoch 35/50\n",
      "10385/10385 [==============================] - 6s 537us/step - loss: 1.3631 - mean_absolute_error: 0.9102\n",
      "Epoch 36/50\n",
      "10385/10385 [==============================] - 6s 569us/step - loss: 1.3270 - mean_absolute_error: 0.9000\n",
      "Epoch 37/50\n",
      "10385/10385 [==============================] - 6s 563us/step - loss: 1.3338 - mean_absolute_error: 0.8986\n",
      "Epoch 38/50\n",
      "10385/10385 [==============================] - 6s 562us/step - loss: 1.2898 - mean_absolute_error: 0.8821\n",
      "Epoch 39/50\n",
      "10385/10385 [==============================] - 6s 559us/step - loss: 1.2893 - mean_absolute_error: 0.8838\n",
      "Epoch 40/50\n",
      "10385/10385 [==============================] - 6s 556us/step - loss: 1.2654 - mean_absolute_error: 0.8737\n",
      "Epoch 41/50\n",
      "10385/10385 [==============================] - 6s 559us/step - loss: 1.2455 - mean_absolute_error: 0.8677\n",
      "Epoch 42/50\n",
      "10385/10385 [==============================] - 6s 558us/step - loss: 1.2256 - mean_absolute_error: 0.8563\n",
      "Epoch 43/50\n",
      "10385/10385 [==============================] - 6s 557us/step - loss: 1.2084 - mean_absolute_error: 0.8544\n",
      "Epoch 44/50\n",
      "10385/10385 [==============================] - 6s 559us/step - loss: 1.2047 - mean_absolute_error: 0.8492\n",
      "Epoch 45/50\n",
      "10385/10385 [==============================] - 6s 561us/step - loss: 1.1933 - mean_absolute_error: 0.8442\n",
      "Epoch 46/50\n",
      "10385/10385 [==============================] - 6s 561us/step - loss: 1.1460 - mean_absolute_error: 0.8305\n",
      "Epoch 47/50\n",
      "10385/10385 [==============================] - 6s 561us/step - loss: 1.1385 - mean_absolute_error: 0.8276\n",
      "Epoch 48/50\n",
      "10385/10385 [==============================] - 6s 562us/step - loss: 1.1246 - mean_absolute_error: 0.8223\n",
      "Epoch 49/50\n",
      "10385/10385 [==============================] - 6s 562us/step - loss: 1.1270 - mean_absolute_error: 0.8185\n",
      "Epoch 50/50\n",
      "10385/10385 [==============================] - 6s 566us/step - loss: 1.0867 - mean_absolute_error: 0.8112 2s - \n",
      "QWK:  0.752646017326\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10385/10385 [==============================] - 7s 720us/step - loss: 5.6267 - mean_absolute_error: 1.7837\n",
      "Epoch 2/50\n",
      "10385/10385 [==============================] - 6s 566us/step - loss: 3.3938 - mean_absolute_error: 1.4321\n",
      "Epoch 3/50\n",
      "10385/10385 [==============================] - 6s 563us/step - loss: 3.1464 - mean_absolute_error: 1.3864\n",
      "Epoch 4/50\n",
      "10385/10385 [==============================] - 6s 565us/step - loss: 2.9722 - mean_absolute_error: 1.3448\n",
      "Epoch 5/50\n",
      "10385/10385 [==============================] - 6s 564us/step - loss: 2.8837 - mean_absolute_error: 1.3300\n",
      "Epoch 6/50\n",
      "10385/10385 [==============================] - 6s 565us/step - loss: 2.7782 - mean_absolute_error: 1.3016\n",
      "Epoch 7/50\n",
      "10385/10385 [==============================] - 6s 565us/step - loss: 2.6838 - mean_absolute_error: 1.2784\n",
      "Epoch 8/50\n",
      "10385/10385 [==============================] - 6s 565us/step - loss: 2.5696 - mean_absolute_error: 1.2515\n",
      "Epoch 9/50\n",
      "10385/10385 [==============================] - 6s 566us/step - loss: 2.4685 - mean_absolute_error: 1.2225\n",
      "Epoch 10/50\n",
      "10385/10385 [==============================] - 6s 567us/step - loss: 2.5035 - mean_absolute_error: 1.2330\n",
      "Epoch 11/50\n",
      "10385/10385 [==============================] - 6s 566us/step - loss: 2.4074 - mean_absolute_error: 1.2111\n",
      "Epoch 12/50\n",
      "10385/10385 [==============================] - 6s 571us/step - loss: 2.3164 - mean_absolute_error: 1.1897\n",
      "Epoch 13/50\n",
      "10385/10385 [==============================] - 6s 574us/step - loss: 2.3093 - mean_absolute_error: 1.1796\n",
      "Epoch 14/50\n",
      "10385/10385 [==============================] - 6s 587us/step - loss: 2.2628 - mean_absolute_error: 1.1707\n",
      "Epoch 15/50\n",
      "10385/10385 [==============================] - 6s 622us/step - loss: 2.1954 - mean_absolute_error: 1.1521\n",
      "Epoch 16/50\n",
      "10385/10385 [==============================] - 6s 622us/step - loss: 2.1312 - mean_absolute_error: 1.1408\n",
      "Epoch 17/50\n",
      "10385/10385 [==============================] - 6s 620us/step - loss: 2.0684 - mean_absolute_error: 1.1227\n",
      "Epoch 18/50\n",
      "10385/10385 [==============================] - 6s 625us/step - loss: 2.0827 - mean_absolute_error: 1.1260\n",
      "Epoch 19/50\n",
      "10385/10385 [==============================] - 6s 625us/step - loss: 1.9917 - mean_absolute_error: 1.0992\n",
      "Epoch 20/50\n",
      "10385/10385 [==============================] - 7s 639us/step - loss: 1.9124 - mean_absolute_error: 1.0766\n",
      "Epoch 21/50\n",
      "10385/10385 [==============================] - 7s 639us/step - loss: 1.9069 - mean_absolute_error: 1.0746\n",
      "Epoch 22/50\n",
      "10385/10385 [==============================] - 7s 636us/step - loss: 1.8199 - mean_absolute_error: 1.0496\n",
      "Epoch 23/50\n",
      "10385/10385 [==============================] - 7s 630us/step - loss: 1.8057 - mean_absolute_error: 1.0478\n",
      "Epoch 24/50\n",
      "10385/10385 [==============================] - 6s 621us/step - loss: 1.7593 - mean_absolute_error: 1.0324\n",
      "Epoch 25/50\n",
      "10385/10385 [==============================] - 6s 622us/step - loss: 1.7542 - mean_absolute_error: 1.0313\n",
      "Epoch 26/50\n",
      "10385/10385 [==============================] - 6s 623us/step - loss: 1.7119 - mean_absolute_error: 1.0162\n",
      "Epoch 27/50\n",
      "10385/10385 [==============================] - 6s 624us/step - loss: 1.6253 - mean_absolute_error: 0.9966\n",
      "Epoch 28/50\n",
      "10385/10385 [==============================] - 7s 644us/step - loss: 1.5843 - mean_absolute_error: 0.9841\n",
      "Epoch 29/50\n",
      "10385/10385 [==============================] - 749s 72ms/step - loss: 1.5272 - mean_absolute_error: 0.9669\n",
      "Epoch 30/50\n",
      "10385/10385 [==============================] - 8s 790us/step - loss: 1.5367 - mean_absolute_error: 0.9616\n",
      "Epoch 31/50\n",
      "10385/10385 [==============================] - 7s 709us/step - loss: 1.5056 - mean_absolute_error: 0.9504\n",
      "Epoch 32/50\n",
      "10385/10385 [==============================] - 7s 647us/step - loss: 1.4678 - mean_absolute_error: 0.9421\n",
      "Epoch 33/50\n",
      "10385/10385 [==============================] - 7s 627us/step - loss: 1.4324 - mean_absolute_error: 0.9310\n",
      "Epoch 34/50\n",
      "10385/10385 [==============================] - 7s 629us/step - loss: 1.3920 - mean_absolute_error: 0.9169\n",
      "Epoch 35/50\n",
      "10385/10385 [==============================] - 7s 627us/step - loss: 1.3848 - mean_absolute_error: 0.9163\n",
      "Epoch 36/50\n",
      "10385/10385 [==============================] - 7s 648us/step - loss: 1.3491 - mean_absolute_error: 0.9042\n",
      "Epoch 37/50\n",
      "10385/10385 [==============================] - 7s 637us/step - loss: 1.3395 - mean_absolute_error: 0.8991\n",
      "Epoch 38/50\n",
      "10385/10385 [==============================] - 7s 636us/step - loss: 1.2926 - mean_absolute_error: 0.8871\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10385/10385 [==============================] - 6s 581us/step - loss: 1.2540 - mean_absolute_error: 0.8713\n",
      "Epoch 40/50\n",
      "10385/10385 [==============================] - 6s 575us/step - loss: 1.2575 - mean_absolute_error: 0.8745\n",
      "Epoch 41/50\n",
      "10385/10385 [==============================] - 6s 587us/step - loss: 1.2479 - mean_absolute_error: 0.8626\n",
      "Epoch 42/50\n",
      "10385/10385 [==============================] - 6s 604us/step - loss: 1.2161 - mean_absolute_error: 0.8605\n",
      "Epoch 43/50\n",
      "10385/10385 [==============================] - 8s 728us/step - loss: 1.2400 - mean_absolute_error: 0.8640\n",
      "Epoch 44/50\n",
      "10385/10385 [==============================] - 7s 666us/step - loss: 1.1730 - mean_absolute_error: 0.8430\n",
      "Epoch 45/50\n",
      "10385/10385 [==============================] - 6s 555us/step - loss: 1.1432 - mean_absolute_error: 0.8349\n",
      "Epoch 46/50\n",
      "10385/10385 [==============================] - 5s 476us/step - loss: 1.1666 - mean_absolute_error: 0.8353\n",
      "Epoch 47/50\n",
      "10385/10385 [==============================] - 5s 479us/step - loss: 1.1449 - mean_absolute_error: 0.8303\n",
      "Epoch 48/50\n",
      "10385/10385 [==============================] - 5s 436us/step - loss: 1.0929 - mean_absolute_error: 0.8091\n",
      "Epoch 49/50\n",
      "10385/10385 [==============================] - 6s 620us/step - loss: 1.0965 - mean_absolute_error: 0.8138\n",
      "Epoch 50/50\n",
      "10385/10385 [==============================] - 7s 649us/step - loss: 1.0550 - mean_absolute_error: 0.7977\n",
      "QWK:  0.732983032541\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec  0.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "y['domain1_score']=df['domain1_score'].fillna(0.0).astype(int)\n",
    "dataset = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "dataset = dataset.split(X, y)\n",
    "results_min_max = train_model(X, y, dataset, \"min+max\")\n",
    "print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec \",np.around(np.array(results_min_max).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
