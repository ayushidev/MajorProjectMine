{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from textblob import TextBlob\n",
    "import language_check\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>foreign_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>conj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>beauty_score</th>\n",
       "      <th>maturity_score</th>\n",
       "      <th>vocab</th>\n",
       "      <th>sentiment_essay</th>\n",
       "      <th>Grammar_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>338</td>\n",
       "      <td>16</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>22</td>\n",
       "      <td>175.314893</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>183</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>419</td>\n",
       "      <td>20</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>80</td>\n",
       "      <td>21</td>\n",
       "      <td>239.987278</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>217</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "\n",
       "   domain1_score  word_count  sentence_count  avg_word_length  \\\n",
       "0       6.666667         338              16         4.550296   \n",
       "1       7.500000         419              20         4.463007   \n",
       "\n",
       "   num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
       "0                      4                   2            168      ...         \n",
       "1                      1                   1            189      ...         \n",
       "\n",
       "   verb_count  foreign_count  adj_count  conj_count  adv_count  beauty_score  \\\n",
       "0          39              0         13          69         22    175.314893   \n",
       "1          56              0         14          80         21    239.987278   \n",
       "\n",
       "   maturity_score  vocab  sentiment_essay  Grammar_check  \n",
       "0        0.029508    183         0.310471             11  \n",
       "1        0.023986    217         0.274000             19  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('features.xlsx')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      essay_id  essay_set                                              essay  \\\n",
      "0            1          1  Dear local newspaper, I think effects computer...   \n",
      "1            2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2            3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3            4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4            5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "5            6          1  Dear @LOCATION1, I think that computers have a...   \n",
      "6            7          1  Did you know that more and more people these d...   \n",
      "7            8          1  @PERCENT1 of people agree that computers make ...   \n",
      "8            9          1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
      "9           10          1  In the @LOCATION1 we have the technology of a ...   \n",
      "10          11          1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
      "11          12          1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
      "12          13          1  Dear local newspaper I raed ur argument on the...   \n",
      "13          14          1  My three detaileds for this news paper article...   \n",
      "14          15          1  Dear, In this world today we should have every...   \n",
      "15          16          1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
      "16          17          1  Dear Local Newspaper, I belive that computers ...   \n",
      "17          18          1  Dear Local Newspaper, I must admit that the ex...   \n",
      "18          19          1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
      "19          20          1  Well computers can be a good or a bad thing. I...   \n",
      "20          21          1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
      "21          22          1  Dear local Newspaper @CAPS1 a take all your co...   \n",
      "22          23          1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
      "23          24          1  Dear local newspaper, I've heard that not many...   \n",
      "24          25          1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
      "25          26          1  Do you think that computers are useless? Or do...   \n",
      "26          27          1  Computers a good because you can get infermati...   \n",
      "27          28          1  Dear Newspaper, Computers are high tec and hav...   \n",
      "28          29          1  Dear local newspaper, @CAPS1 people throughout...   \n",
      "29          30          1  Dear Newspaper People, I think that computers ...   \n",
      "...        ...        ...                                                ...   \n",
      "1753      1758          1  Dear local newspaper, @CAPS1 on a beautiful su...   \n",
      "1754      1759          1  Dear @CAPS1, I believe that computers have a n...   \n",
      "1755      1760          1  I think we can all agree that computer usage i...   \n",
      "1756      1761          1  Dear @PERSON1, Computers are very helpful in d...   \n",
      "1757      1762          1  Dear Newspaper, @CAPS1 are worried that people...   \n",
      "1758      1763          1  Dear Local Newspaper: @CAPS1 you know that ove...   \n",
      "1759      1764          1  Dear @PERSON1, The advansing technology is sho...   \n",
      "1760      1765          1  Dear local Newspaper I ting that computers are...   \n",
      "1761      1766          1  Man has always been interested in technology. ...   \n",
      "1762      1767          1  Guaranteed, @NUM1 years from now we will still...   \n",
      "1763      1768          1  I think the effects of the computer are bad, t...   \n",
      "1764      1769          1  Dear editor, I think people are using computer...   \n",
      "1765      1770          1  Dear @CAPS1 @CAPS2, @CAPS3, experts have been ...   \n",
      "1766      1771          1  Computers, a @LOCATION1 topic if you ask me. S...   \n",
      "1767      1772          1  Dear Newspaper Readers, @CAPS1 many hours a da...   \n",
      "1768      1773          1  Dear @CAPS1 newspaper, I have resently read th...   \n",
      "1769      1774          1  Dear @ORGANIZATION2 (our local newspaper), @CA...   \n",
      "1770      1775          1  Dear newspaper, In my opinion computers do ben...   \n",
      "1771      1776          1  Technology, such as computers are very big. I ...   \n",
      "1772      1777          1  Dear Newspaper, Computers have advance a lot s...   \n",
      "1773      1778          1  Dear Newspaper, I think that computers have a ...   \n",
      "1774      1779          1  Dear @LOCATION1, *@CAPS1*. Now I hear my favor...   \n",
      "1775      1780          1  Dear Newspaper I think that computers were one...   \n",
      "1776      1781          1  Mom!!! Did you know that the human body has on...   \n",
      "1777      1782          1  Dear @ORGANIZATION1, I believe that computers ...   \n",
      "1778      1783          1  Dear @CAPS1, @CAPS2 several reasons on way I t...   \n",
      "1779      1784          1  Do a adults and kids spend to much time on the...   \n",
      "1780      1785          1  My opinion is that people should have computer...   \n",
      "1781      1786          1  Dear readers, I think that its good and bad to...   \n",
      "1782      1787          1  Dear - Local Newspaper I agree thats computers...   \n",
      "\n",
      "      domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "0          6.666667         338              16         4.550296   \n",
      "1          7.500000         419              20         4.463007   \n",
      "2          5.833333         279              14         4.526882   \n",
      "3          8.333333         524              27         5.041985   \n",
      "4          6.666667         465              30         4.526882   \n",
      "5          6.666667         246              15         4.191057   \n",
      "6          8.333333         499              30         4.629259   \n",
      "7          8.333333         482              39         4.653527   \n",
      "8          7.500000         443              35         4.424379   \n",
      "9          7.500000         502              26         4.245020   \n",
      "10         6.666667         325              22         5.043077   \n",
      "11         6.666667         392              25         4.568878   \n",
      "12         5.833333         204               6         3.941176   \n",
      "13         5.000000         308              25         4.256494   \n",
      "14         5.000000         176              13         4.636364   \n",
      "15        10.000000         528              35         5.001894   \n",
      "16         6.666667         335              18         4.388060   \n",
      "17         6.666667         366              15         4.355191   \n",
      "18         3.333333          66               7         4.500000   \n",
      "19         5.000000         156              11         4.570513   \n",
      "20         6.666667         367              20         4.585831   \n",
      "21         2.500000          56               2         4.678571   \n",
      "22         8.333333         521              30         4.525912   \n",
      "23         9.166667         559              39         4.754919   \n",
      "24         6.666667         293              16         4.713311   \n",
      "25         7.500000         360              22         4.569444   \n",
      "26         3.333333         120               7         4.425000   \n",
      "27         7.500000         360              28         4.686111   \n",
      "28         7.500000         371              23         4.560647   \n",
      "29         6.666667         257              15         4.533074   \n",
      "...             ...         ...             ...              ...   \n",
      "1753       8.333333         533              38         4.615385   \n",
      "1754       8.333333         328              23         4.765244   \n",
      "1755      10.000000         550              45         4.560000   \n",
      "1756       6.666667         367              21         4.291553   \n",
      "1757       6.666667         337              17         4.299703   \n",
      "1758      10.000000         475              33         5.313684   \n",
      "1759       6.666667         405              24         4.488889   \n",
      "1760       4.166667         100               6         4.430000   \n",
      "1761       6.666667         360              30         4.661111   \n",
      "1762       7.500000         413              25         4.629540   \n",
      "1763       6.666667         471              12         4.414013   \n",
      "1764       7.500000         286              20         4.793706   \n",
      "1765       8.333333         344              22         4.997093   \n",
      "1766       6.666667         372              26         4.408602   \n",
      "1767       8.333333         499              33         5.206413   \n",
      "1768       6.666667         321              18         4.358255   \n",
      "1769       8.333333         404              23         5.049505   \n",
      "1770       7.500000         369              18         4.506775   \n",
      "1771       7.500000         390              19         4.392308   \n",
      "1772       9.166667         604              30         4.346026   \n",
      "1773       4.166667         116               4         4.456897   \n",
      "1774       8.333333         438              31         4.525114   \n",
      "1775       7.500000         463              27         5.000000   \n",
      "1776       8.333333         454              24         4.286344   \n",
      "1777       6.666667         331              23         4.595166   \n",
      "1778       6.666667         497              21         4.213280   \n",
      "1779       5.833333         200              18         4.460000   \n",
      "1780       6.666667         291              18         4.646048   \n",
      "1781       1.666667          15               1         3.800000   \n",
      "1782       5.833333         214              18         4.228972   \n",
      "\n",
      "      num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
      "0                         4                   2            168      ...         \n",
      "1                         1                   1            189      ...         \n",
      "2                         0                   0            140      ...         \n",
      "3                         2                   1            222      ...         \n",
      "4                         0                   0            236      ...         \n",
      "5                         1                   2            132      ...         \n",
      "6                         0                   4            237      ...         \n",
      "7                         1                   3            231      ...         \n",
      "8                         1                   6            218      ...         \n",
      "9                         0                   2            268      ...         \n",
      "10                        1                   1            139      ...         \n",
      "11                        0                   0            196      ...         \n",
      "12                        0                   0            105      ...         \n",
      "13                        0                   0            134      ...         \n",
      "14                        0                   0             75      ...         \n",
      "15                        0                   1            215      ...         \n",
      "16                        0                   1            184      ...         \n",
      "17                        0                   0            192      ...         \n",
      "18                        0                   0             23      ...         \n",
      "19                        2                   0             64      ...         \n",
      "20                        0                   1            177      ...         \n",
      "21                        0                   0             25      ...         \n",
      "22                        0                   2            254      ...         \n",
      "23                       10                   1            245      ...         \n",
      "24                        0                   0            143      ...         \n",
      "25                        8                   2            172      ...         \n",
      "26                        0                   0             60      ...         \n",
      "27                        2                   0            153      ...         \n",
      "28                        2                   1            189      ...         \n",
      "29                        0                   0            117      ...         \n",
      "...                     ...                 ...            ...      ...         \n",
      "1753                      1                   2            220      ...         \n",
      "1754                      0                   0            158      ...         \n",
      "1755                      9                   9            261      ...         \n",
      "1756                     15                   0            186      ...         \n",
      "1757                      0                   1            170      ...         \n",
      "1758                      2                   3            166      ...         \n",
      "1759                      0                   0            202      ...         \n",
      "1760                      0                   0             42      ...         \n",
      "1761                      0                   1            153      ...         \n",
      "1762                      0                   0            190      ...         \n",
      "1763                      0                   0            243      ...         \n",
      "1764                      0                   1            127      ...         \n",
      "1765                      1                   0            150      ...         \n",
      "1766                      4                   2            183      ...         \n",
      "1767                      0                   5            201      ...         \n",
      "1768                      3                   3            154      ...         \n",
      "1769                      5                   2            176      ...         \n",
      "1770                      0                   0            181      ...         \n",
      "1771                      1                   0            191      ...         \n",
      "1772                      0                   2            321      ...         \n",
      "1773                      0                   0             57      ...         \n",
      "1774                      7                   2            202      ...         \n",
      "1775                      0                   2            199      ...         \n",
      "1776                      6                   0            225      ...         \n",
      "1777                      4                   1            162      ...         \n",
      "1778                      0                   1            245      ...         \n",
      "1779                      0                   2             93      ...         \n",
      "1780                      0                   0            160      ...         \n",
      "1781                      0                   0              7      ...         \n",
      "1782                      0                   0             99      ...         \n",
      "\n",
      "      verb_count  foreign_count  adj_count  conj_count  adv_count  \\\n",
      "0             39              0         13          69         22   \n",
      "1             56              0         14          80         21   \n",
      "2             33              0         13          50         17   \n",
      "3             57              0         29          84         30   \n",
      "4             60              0         18          63         41   \n",
      "5             35              0          9          38         18   \n",
      "6             50              0         21          84         41   \n",
      "7             66              0         28          77         28   \n",
      "8             56              0         25          67         34   \n",
      "9             74              0         30          88         39   \n",
      "10            48              0         12          42         26   \n",
      "11            52              0         22          59         43   \n",
      "12            26              0          7          32          7   \n",
      "13            36              0         24          53         13   \n",
      "14            23              0          8          30          8   \n",
      "15            52              0         23          83         25   \n",
      "16            47              0         11          65         33   \n",
      "17            57              0         15          63         42   \n",
      "18             7              0          2           4          1   \n",
      "19            12              0         11          25          7   \n",
      "20            52              0         20          59         37   \n",
      "21             5              0          2          11          1   \n",
      "22            60              0         14         107         34   \n",
      "23            62              0         30          89         44   \n",
      "24            34              0         14          49         10   \n",
      "25            47              0         11          62         21   \n",
      "26            16              0          6          20         11   \n",
      "27            59              0         19          70         26   \n",
      "28            50              0         20          67         24   \n",
      "29            40              0         14          36         16   \n",
      "...          ...            ...        ...         ...        ...   \n",
      "1753          90              0         20          77         34   \n",
      "1754          46              0         21          58         27   \n",
      "1755          73              0         32          80         52   \n",
      "1756          54              0         13          61         24   \n",
      "1757          38              0         15          54         35   \n",
      "1758          60              0         23          57         21   \n",
      "1759          50              0         23          73         29   \n",
      "1760          12              0         10          19          3   \n",
      "1761          51              0         20          41         22   \n",
      "1762          72              0         11          63         39   \n",
      "1763          38              0         23          71         35   \n",
      "1764          43              0         17          51         27   \n",
      "1765          35              0         19          54         29   \n",
      "1766          46              0         12          65         31   \n",
      "1767          72              0         23          75         53   \n",
      "1768          36              0         13          51         27   \n",
      "1769          43              0         25          56         23   \n",
      "1770          45              0         19          66         23   \n",
      "1771          46              0         14          57         33   \n",
      "1772          89              0         17         104         49   \n",
      "1773          10              0          1          18         17   \n",
      "1774          49              0         15          71         38   \n",
      "1775          54              0         27          55         27   \n",
      "1776          58              0         18          70         39   \n",
      "1777          53              0         11          56         28   \n",
      "1778          67              0         19          85         41   \n",
      "1779          24              0          9          36         13   \n",
      "1780          24              0          7          51         18   \n",
      "1781           0              0          3           2          0   \n",
      "1782          26              0         13          30         17   \n",
      "\n",
      "      beauty_score  maturity_score  vocab  sentiment_essay  Grammar_check  \n",
      "0       175.314893    2.950820e-02    183         0.310471             11  \n",
      "1       239.987278    2.398615e-02    217         0.274000             19  \n",
      "2       132.356485    6.002539e-02    146         0.340393              9  \n",
      "3       239.253012    1.937297e-02    271         0.266828             35  \n",
      "4       260.246296    2.102782e-02    249         0.199684             17  \n",
      "5        97.915856    3.785714e-04    110         0.172817             15  \n",
      "6       185.683945    2.109366e-02    281         0.113326              5  \n",
      "7       197.738972    1.870849e-02    271         0.262030              5  \n",
      "8       229.183025    3.196429e-02    224         0.247986             11  \n",
      "9       227.253395    1.852870e-02    241        -0.015800             19  \n",
      "10      116.448435    2.409675e-02    195        -0.000435              4  \n",
      "11      156.101318    1.370496e-06    192         0.040025             46  \n",
      "12       86.253533    1.595507e-09     88         0.061364             24  \n",
      "13      115.093454    6.861656e-02    136         0.135383             47  \n",
      "14       81.400428    4.858628e-02     99         0.148691             10  \n",
      "15      203.249939    6.347593e-05    310         0.252718             25  \n",
      "16      137.622189    2.537143e-02    175         0.086160             14  \n",
      "17      210.322452    1.282332e-04    201         0.160165              9  \n",
      "18       20.111906    7.607374e-05      7         0.418182             35  \n",
      "19       69.627743    1.962759e-07     73         0.084753              6  \n",
      "20      163.412948    6.750000e-07    200         0.150041             16  \n",
      "21       19.304819    1.439109e-01     37        -0.062500              1  \n",
      "22      195.029109    1.683096e-02    292         0.245699             12  \n",
      "23      290.093978    1.444135e-02    313         0.213293             20  \n",
      "24      101.217531    2.816184e-02    174         0.212811              4  \n",
      "25      137.651441    3.304030e-02    177         0.242953             16  \n",
      "26       52.471187    3.912107e-10     54         0.022222             10  \n",
      "27      181.268855    2.985982e-02    214         0.091337             20  \n",
      "28      162.785044    5.297632e-07    188         0.387986             14  \n",
      "29       98.347750    5.506571e-02    158         0.139286              1  \n",
      "...            ...             ...    ...              ...            ...  \n",
      "1753    246.920464    1.704895e-02    281         0.062807             31  \n",
      "1754    113.566147    5.105329e-02    190         0.034854             17  \n",
      "1755    246.991337    1.700187e-02    291         0.163256             22  \n",
      "1756    137.791307    5.055297e-09    179         0.056845             30  \n",
      "1757    119.459879    2.772658e-02    172         0.086779             16  \n",
      "1758    188.233521    6.221826e-05    268         0.130765             17  \n",
      "1759    171.658041    1.828903e-02    206         0.087696             10  \n",
      "1760     35.015578    2.021335e-10     55         0.178846              7  \n",
      "1761    135.896479    2.079178e-04    188         0.164766             18  \n",
      "1762    211.386602    3.344548e-02    205         0.179778              9  \n",
      "1763    203.252329    6.822325e-05    268        -0.132795              4  \n",
      "1764    123.278248    2.119907e-02    161         0.192546             18  \n",
      "1765    147.335212    3.299183e-02    224         0.105204              6  \n",
      "1766    150.070783    2.927475e-02    179         0.047040             25  \n",
      "1767    230.720740    2.144400e-02    274         0.158757             10  \n",
      "1768    146.446051    1.872967e-04    165         0.346389             20  \n",
      "1769    142.641254    2.401112e-02    246         0.313973             12  \n",
      "1770    129.577690    2.388017e-02    193         0.114520             16  \n",
      "1771    195.548554    3.090909e-02    198         0.291571             30  \n",
      "1772    250.419878    1.137376e-02    311         0.116471              7  \n",
      "1773    106.616667    9.053595e-04     64        -0.008333             10  \n",
      "1774    174.815013    3.728933e-02    211         0.215404             25  \n",
      "1775    176.203011    2.813372e-09    234         0.311371             29  \n",
      "1776    209.950832    1.719696e-02    239         0.222872             14  \n",
      "1777    149.507417    1.220533e-04    177         0.193016             13  \n",
      "1778    211.485230    2.921875e-02    256         0.176816             30  \n",
      "1779     80.866086    8.949413e-02    101         0.307500             15  \n",
      "1780    112.843197    5.574713e-02    174         0.229487              9  \n",
      "1781      4.453531    6.927788e-01      7         0.066667              0  \n",
      "1782    105.035204    1.065995e-01     91         0.241520             13  \n",
      "\n",
      "[1783 rows x 21 columns],       essay_id  essay_set                                              essay  \\\n",
      "1783      2978          2  Certain materials being removed from libraries...   \n",
      "1784      2979          2  Write a persuasive essay to a newspaper reflec...   \n",
      "1785      2980          2  Do you think that libraries should remove cert...   \n",
      "1786      2981          2  In @DATE1's world, there are many things found...   \n",
      "1787      2982          2  In life you have the 'offensive things'. The l...   \n",
      "1788      2983          2  A lot of @CAPS3 today are censored because of ...   \n",
      "1789      2984          2  How @CAPS4 you feel if your favorite book was ...   \n",
      "1790      2985          2  Do you think that you should be able to take c...   \n",
      "1791      2986          2  The idea of the removal of 'offensive' materia...   \n",
      "1792      2987          2  All authors write for a purpose, whether it be...   \n",
      "1793      2988          2  Have you seen a magazine, book, movies, etc., ...   \n",
      "1794      2989          2  Personally I don't think libraries, movie stor...   \n",
      "1795      2990          2  A library is a place you can go to read, write...   \n",
      "1796      2991          2  Yes and no, some materials such as books, musi...   \n",
      "1797      2992          2  When talking about censorship in libraries, I ...   \n",
      "1798      2993          2  Dear Newspaper,     @CAPS1 would you feel if s...   \n",
      "1799      2994          2  A lot of people that are in school have probab...   \n",
      "1800      2995          2  Do you think that if certain books, music, mov...   \n",
      "1801      2996          2  If the people that are publishing and writing ...   \n",
      "1802      2997          2  I think that all the bad books should be taken...   \n",
      "1803      2998          2  Dear news people,     I am qriting to you beca...   \n",
      "1804      2999          2  I think that books, magazines, papers etc shou...   \n",
      "1805      3000          2  Hello, my name is @CAPS1, I feel that there sh...   \n",
      "1806      3001          2  Books Removed From Shelves?      'All of us ca...   \n",
      "1807      3002          2  I think almost every movie might offend someon...   \n",
      "1808      3003          2  The world in full of offensive material, and t...   \n",
      "1809      3004          2  People are entitile to their own oppinion abou...   \n",
      "1810      3005          2  I think if people find books offensive they sh...   \n",
      "1811      3006          2  A book is more than words or happenings; they'...   \n",
      "1812      3007          2  I don't believe that certain books, music, mag...   \n",
      "...        ...        ...                                                ...   \n",
      "3553      4748          2  No, materials should not be removed from the l...   \n",
      "3554      4749          2  When you think of a book do you think happy ki...   \n",
      "3555      4750          2  There should not be sensorship on the books in...   \n",
      "3556      4751          2  I do not believe that certain materials, such ...   \n",
      "3557      4752          2  Censorship in libraries is when a librarian ha...   \n",
      "3558      4753          2  I know that some people find some books offens...   \n",
      "3559      4754          2  Do citizens have the right to remove books, ma...   \n",
      "3560      4755          2  Why do we have books out there that are viewed...   \n",
      "3561      4756          2  Libraries @CAPS1 Or @CAPS2?         This world...   \n",
      "3562      4757          2  I believe that if that book, music, or magazin...   \n",
      "3563      4758          2  Do you believe that certain materials, such as...   \n",
      "3564      4759          2  Today in the media there are many things that ...   \n",
      "3565      4760          2  Certain books, magazines, music and movies sho...   \n",
      "3566      4761          2  Offensive things should be taken off for many ...   \n",
      "3567      4762          2  What if some one found your favorite book offe...   \n",
      "3568      4763          2  I blieve that any book or movie that is in the...   \n",
      "3569      4764          2  Debates are a main part of our society today. ...   \n",
      "3570      4765          2  Books are very important to our society. They ...   \n",
      "3571      4766          2  In todays world, children are exposed to a lot...   \n",
      "3572      4767          2  This is going to be an essay about if people f...   \n",
      "3573      4768          2  Books, movies, magazines, music, etc., make up...   \n",
      "3574      4769          2  I believe that certain materials should not be...   \n",
      "3575      4770          2  I personally think certain materials, such as ...   \n",
      "3576      4771          2  We all hope that one day our children will not...   \n",
      "3577      4772          2  Libraries have a variety of material from book...   \n",
      "3578      4773          2  The author is writting about taking books off ...   \n",
      "3579      4774          2  I do not think that materials, such as books, ...   \n",
      "3580      4775          2  Yes we should keep the books,music,movies,an m...   \n",
      "3581      4776          2  I do believe that  book, magazines, music, mov...   \n",
      "3582      4777          2  Different Then Everyone Else     @CAPS1 do peo...   \n",
      "\n",
      "      domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "1783            8.0         488              19         4.391393   \n",
      "1784            2.0         187               3         3.561497   \n",
      "1785            5.0         229              15         4.061135   \n",
      "1786            8.0         507              31         4.297830   \n",
      "1787            8.0         472              35         4.029661   \n",
      "1788            7.0         342              25         4.152047   \n",
      "1789            9.0         543              25         4.627993   \n",
      "1790            4.0         820              36         3.915854   \n",
      "1791            8.0         350              19         4.505714   \n",
      "1792            8.0         388              24         4.237113   \n",
      "1793            6.0         354              17         3.870056   \n",
      "1794            6.0         325              18         4.233846   \n",
      "1795            9.0         534              29         4.127341   \n",
      "1796            7.0         347              12         3.974063   \n",
      "1797            5.0         110               5         4.163636   \n",
      "1798            6.0         372              17         4.250000   \n",
      "1799            8.0         689              39         4.046444   \n",
      "1800            6.0         341              13         4.428152   \n",
      "1801            3.0         148               5         5.202703   \n",
      "1802            6.0         354              21         3.994350   \n",
      "1803            5.0         331              11         3.616314   \n",
      "1804            6.0         308              19         4.022727   \n",
      "1805            6.0         210               9         4.276190   \n",
      "1806            8.0         555              23         3.884685   \n",
      "1807            6.0         531              22         4.222222   \n",
      "1808            7.0         414              18         4.350242   \n",
      "1809            8.0         256              11         3.695312   \n",
      "1810            4.0          87               5         4.344828   \n",
      "1811            6.0         289              15         4.221453   \n",
      "1812            6.0         297              20         4.175084   \n",
      "...             ...         ...             ...              ...   \n",
      "3553            4.0         396               5         3.351010   \n",
      "3554            6.0         272              14         3.772059   \n",
      "3555            4.0          77               4         4.506494   \n",
      "3556            8.0         632              25         4.425633   \n",
      "3557            7.0         357              14         4.224090   \n",
      "3558            7.0         779              42         4.007702   \n",
      "3559            9.0         715              35         4.588811   \n",
      "3560            8.0         530              27         4.379245   \n",
      "3561            7.0         604              31         3.819536   \n",
      "3562            8.0         503              32         3.813121   \n",
      "3563            7.0         449              22         3.817372   \n",
      "3564            6.0         303              16         4.049505   \n",
      "3565            6.0         423              24         4.390071   \n",
      "3566            6.0         294              17         4.224490   \n",
      "3567            7.0         406              17         4.307882   \n",
      "3568            6.0         267              11         4.000000   \n",
      "3569            8.0         427              25         4.416862   \n",
      "3570            8.0         544              33         4.479779   \n",
      "3571            9.0         669              40         4.442451   \n",
      "3572            6.0         341              14         3.850440   \n",
      "3573            7.0         564              22         4.843972   \n",
      "3574            6.0         618              26         4.438511   \n",
      "3575            9.0         417              20         4.393285   \n",
      "3576            8.0         361              23         4.099723   \n",
      "3577            6.0         261              16         4.544061   \n",
      "3578            6.0         258              12         4.085271   \n",
      "3579            6.0         251              16         4.350598   \n",
      "3580            4.0         117               5         4.059829   \n",
      "3581            7.0         529              23         4.136106   \n",
      "3582            5.0         495              15         3.961616   \n",
      "\n",
      "      num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
      "1783                      0                   0            228      ...         \n",
      "1784                      0                   0             95      ...         \n",
      "1785                      0                   4            113      ...         \n",
      "1786                      0                   0            229      ...         \n",
      "1787                      0                   1            190      ...         \n",
      "1788                      0                   0            150      ...         \n",
      "1789                      0                   5            255      ...         \n",
      "1790                      0                   1            457      ...         \n",
      "1791                      0                   0            172      ...         \n",
      "1792                      1                   3            201      ...         \n",
      "1793                      0                   2            191      ...         \n",
      "1794                      0                   0            164      ...         \n",
      "1795                      1                   0            253      ...         \n",
      "1796                      0                   0            183      ...         \n",
      "1797                      0                   0             49      ...         \n",
      "1798                      0                   3            169      ...         \n",
      "1799                      0                   0            341      ...         \n",
      "1800                      0                   2            169      ...         \n",
      "1801                      0                   0             66      ...         \n",
      "1802                      0                   0            173      ...         \n",
      "1803                      0                   0            189      ...         \n",
      "1804                      0                   0            152      ...         \n",
      "1805                      0                   0            103      ...         \n",
      "1806                      0                   4            282      ...         \n",
      "1807                      0                   0            233      ...         \n",
      "1808                      0                   0            212      ...         \n",
      "1809                      0                   0             91      ...         \n",
      "1810                      0                   0             36      ...         \n",
      "1811                      0                   2            137      ...         \n",
      "1812                      0                   1            157      ...         \n",
      "...                     ...                 ...            ...      ...         \n",
      "3553                      0                   0            132      ...         \n",
      "3554                      0                   5            145      ...         \n",
      "3555                      0                   1             40      ...         \n",
      "3556                      0                   1            316      ...         \n",
      "3557                      0                   0            183      ...         \n",
      "3558                      0                   0            409      ...         \n",
      "3559                      0                   2            356      ...         \n",
      "3560                      3                   2            268      ...         \n",
      "3561                      0                   2            293      ...         \n",
      "3562                      0                   2            285      ...         \n",
      "3563                      0                   1            177      ...         \n",
      "3564                      0                   0            159      ...         \n",
      "3565                      0                   0            199      ...         \n",
      "3566                      0                   0            140      ...         \n",
      "3567                      0                   1            179      ...         \n",
      "3568                      0                   0            131      ...         \n",
      "3569                      0                   1            216      ...         \n",
      "3570                      1                   4            286      ...         \n",
      "3571                      0                   1            324      ...         \n",
      "3572                      0                   0            175      ...         \n",
      "3573                      0                   0            266      ...         \n",
      "3574                      0                   2            310      ...         \n",
      "3575                      0                   5            217      ...         \n",
      "3576                      0                   0            190      ...         \n",
      "3577                      0                   0            117      ...         \n",
      "3578                      0                   0            124      ...         \n",
      "3579                      0                   0            125      ...         \n",
      "3580                      0                   0             55      ...         \n",
      "3581                      0                   0            256      ...         \n",
      "3582                      0                   2            240      ...         \n",
      "\n",
      "      verb_count  foreign_count  adj_count  conj_count  adv_count  \\\n",
      "1783          59              0         26          84         35   \n",
      "1784          24              0          5          20          4   \n",
      "1785          30              0         13          30         15   \n",
      "1786          90              0         21          55         35   \n",
      "1787          55              0         22          55         34   \n",
      "1788          32              0         17          50         17   \n",
      "1789          84              0         28          81         28   \n",
      "1790         116              0         29         155         80   \n",
      "1791          52              0         27          50         25   \n",
      "1792          41              0         14          73         34   \n",
      "1793          61              0         14          72         21   \n",
      "1794          49              0         14          52         19   \n",
      "1795          67              0         24          83         39   \n",
      "1796          43              0         16          67         22   \n",
      "1797          20              0          4          14          6   \n",
      "1798          44              0         26          61         21   \n",
      "1799          94              0         31         132         50   \n",
      "1800          49              0         17          65         26   \n",
      "1801          19              0          5          31          8   \n",
      "1802          48              0         20          56         26   \n",
      "1803          39              0          6          55         22   \n",
      "1804          29              0          9          49         13   \n",
      "1805          30              0          9          30         16   \n",
      "1806          68              0         16          98         31   \n",
      "1807          68              0         22          73         46   \n",
      "1808          54              0         27          79         24   \n",
      "1809          31              0         10          35         13   \n",
      "1810           9              0          7          15          4   \n",
      "1811          37              0          6          51         16   \n",
      "1812          39              0         18          51         27   \n",
      "...          ...            ...        ...         ...        ...   \n",
      "3553          42              0         17          36         22   \n",
      "3554          27              0         15          48         17   \n",
      "3555           8              0          4          12          7   \n",
      "3556          81              0         36         115         33   \n",
      "3557          44              0         23          56         34   \n",
      "3558         108              0         28         130         61   \n",
      "3559          92              0         29         112         39   \n",
      "3560          64              0         18          95         26   \n",
      "3561          62              0         21          93         48   \n",
      "3562          71              0         13          99         67   \n",
      "3563          36              0         16          63         23   \n",
      "3564          43              0         13          43         27   \n",
      "3565          62              0         16          67         23   \n",
      "3566          53              0          8          46         19   \n",
      "3567          52              0         25          69         20   \n",
      "3568          40              0         10          51         11   \n",
      "3569          49              0         32          72         17   \n",
      "3570          77              0         24          96         45   \n",
      "3571          85              0         29         123         44   \n",
      "3572          45              0         22          63         17   \n",
      "3573          70              0         45         114         38   \n",
      "3574          65              0         44          93         38   \n",
      "3575          50              0         27          58         35   \n",
      "3576          54              0         12          56         32   \n",
      "3577          26              0         16          40         25   \n",
      "3578          33              0          8          40         23   \n",
      "3579          36              0         20          44         19   \n",
      "3580          12              0          3          12          6   \n",
      "3581          73              0         23          83         29   \n",
      "3582          60              0         40          65         39   \n",
      "\n",
      "      beauty_score  maturity_score  vocab  sentiment_essay  Grammar_check  \n",
      "1783    188.265599    6.546754e-05    276         0.127085              8  \n",
      "1784     62.604905    9.707412e-04     78         0.098140             25  \n",
      "1785    139.861115    2.627727e-08    112         0.022381             12  \n",
      "1786    151.589229    2.101842e-02    275         0.120114              5  \n",
      "1787    210.854582    2.981688e-02    242         0.029199             32  \n",
      "1788    132.695634    4.211765e-02    170         0.069600             13  \n",
      "1789    233.651442    1.838129e-02    278         0.170586             11  \n",
      "1790    560.715724    2.000421e-02    420         0.076310             33  \n",
      "1791    136.819399    1.543333e-04    191         0.047959              4  \n",
      "1792    146.815124    2.381336e-02    214         0.027008              5  \n",
      "1793    121.532249    3.948949e-02    176         0.113846             10  \n",
      "1794    187.057269    5.545813e-02    175         0.031848              9  \n",
      "1795    198.719830    1.541859e-02    267         0.171764             14  \n",
      "1796    133.040536    3.893679e-02    188         0.005505             10  \n",
      "1797     34.208124    1.031439e-01     55         0.271429              3  \n",
      "1798    211.955575    2.682927e-02    205        -0.126848             25  \n",
      "1799    400.146069    2.179807e-07    354         0.217630             16  \n",
      "1800    125.685980    4.839921e-02    194         0.199297             11  \n",
      "1801     66.417225    3.644041e-33     69         0.142857              2  \n",
      "1802    128.112590    3.149812e-02    187        -0.187531             14  \n",
      "1803    204.586969    2.004445e-04    150         0.092824             18  \n",
      "1804    115.537637    3.270224e-04    149         0.031250             19  \n",
      "1805    108.159647    3.671641e-02     96         0.053333              4  \n",
      "1806    193.888626    1.830700e-02    274         0.065043             17  \n",
      "1807    183.213751    1.701418e-02    288         0.182753             17  \n",
      "1808    169.863592    3.275362e-02    207        -0.011537             27  \n",
      "1809     86.198893    3.696941e-02    122        -0.074167             14  \n",
      "1810     32.432698    1.282939e-01     50         0.450000              3  \n",
      "1811     99.052536    3.796684e-02    156         0.201563              3  \n",
      "1812    114.489252    2.833143e-02    158         0.008258             13  \n",
      "...            ...             ...    ...              ...            ...  \n",
      "3553    170.946931    5.521809e-11    170         0.348016             46  \n",
      "3554     95.379717    6.254160e-02    130         0.329375              9  \n",
      "3555     44.487112    1.693564e-01     42         0.340816              7  \n",
      "3556    317.212165    3.609537e-02    315        -0.013680             12  \n",
      "3557    179.664511    2.647114e-02    187         0.180645             12  \n",
      "3558    455.631389    9.223132e-03    399         0.026807             55  \n",
      "3559    268.505970    5.941996e-13    392         0.082253              9  \n",
      "3560    210.115200    1.453601e-11    223         0.085065             23  \n",
      "3561    247.901071    1.733577e-02    274        -0.052613             51  \n",
      "3562    253.040544    4.114894e-02    235         0.141798             43  \n",
      "3563    170.430814    1.150977e-04    216         0.072793             14  \n",
      "3564    157.776329    3.352369e-02    159         0.054613             13  \n",
      "3565    150.649017    1.785185e-04    237         0.010374             15  \n",
      "3566    163.861620    4.779955e-02    145         0.210897             16  \n",
      "3567    209.765251    2.380151e-02    219        -0.066454             25  \n",
      "3568     91.278773    2.715330e-02    137         0.049444             19  \n",
      "3569    148.635933    3.653231e-02    222         0.052024              7  \n",
      "3570    234.551817    2.421429e-02    280         0.227026              9  \n",
      "3571    229.800874    8.243435e-08    390         0.190277             11  \n",
      "3572    115.432533    3.660247e-02    152         0.092917             35  \n",
      "3573    255.204501    2.092954e-02    325         0.196779             18  \n",
      "3574    226.794993    1.607162e-02    308         0.179267             90  \n",
      "3575    142.075454    4.082984e-02    205         0.045101             17  \n",
      "3576    212.459271    2.647059e-02    187         0.095857             24  \n",
      "3577    118.313282    2.300877e-02    153         0.192365             12  \n",
      "3578    120.688916    1.680922e-10    129         0.144318             21  \n",
      "3579     87.425778    4.344000e-02    125         0.100357             14  \n",
      "3580     50.997620    1.729070e-05     59         0.434091             19  \n",
      "3581    261.813135    1.540950e-02    281         0.093370             31  \n",
      "3582    228.551333    1.289854e-04    229         0.099947             50  \n",
      "\n",
      "[1800 rows x 21 columns],       essay_id  essay_set                                              essay  \\\n",
      "3583      5978          3  The features of the setting affect the cyclist...   \n",
      "3584      5979          3  The features of the setting affected the cycli...   \n",
      "3585      5980          3  Everyone travels to unfamiliar places. Sometim...   \n",
      "3586      5981          3  I believe the features of the cyclist affected...   \n",
      "3587      5982          3  The setting effects the cyclist because of the...   \n",
      "3588      5983          3  There were many features of the setting that a...   \n",
      "3589      5984          3  The cyclist was riding through a tower when he...   \n",
      "3590      5985          3  The affects of the cyclist is if it does not c...   \n",
      "3591      5986          3  The essay â€œRough Road Ahead: Do Not Exceed Pos...   \n",
      "3592      5987          3  In the story, â€œRough Road Ahead: Do Not Exceed...   \n",
      "3593      5988          3  The setting of the authors â€œ shortcutâ€ effecte...   \n",
      "3594      5989          3  The features of the setting affect the cyclist...   \n",
      "3595      5990          3  There was many features that affected the cycl...   \n",
      "3596      5991          3  The features in the setting affected the cycli...   \n",
      "3597      5992          3  The features of the setting in â€œRough Road Ahe...   \n",
      "3598      5993          3  In the essay Do Not Exceed Posted Speed Limit ...   \n",
      "3599      5994          3  Many different characteristics in a setting an...   \n",
      "3600      5995          3  The setting of the story affects the cyclist i...   \n",
      "3601      5996          3  In the story â€œRough Road Aheadâ€ by @PERSON1, t...   \n",
      "3602      5997          3  Some features of the setting affect the cyclis...   \n",
      "3603      5998          3  The features of the setting that affect the cy...   \n",
      "3604      5999          3  The cyclist has a couple things if he racing a...   \n",
      "3605      6000          3  â€œDo Not Exceed Posted Speed Limitâ€ had a  roug...   \n",
      "3606      6001          3  An example of the setting affecting the cyclis...   \n",
      "3607      6002          3  The cyclist had a very hard time geting to Yos...   \n",
      "3608      6003          3  The cyclist in the story is at first confident...   \n",
      "3609      6004          3  Well I would put it like this is that the weat...   \n",
      "3610      6005          3  The features of the setting affect the cyclist...   \n",
      "3611      6006          3  The settings of the story were grveling. While...   \n",
      "3612      6007          3  As the cyclist is travelling to Yosemite Natio...   \n",
      "...        ...        ...                                                ...   \n",
      "5279      7679          3  In the story â€œDo not exceed posted speedâ€ by @...   \n",
      "5280      7680          3  Every body experiences physical and emotional ...   \n",
      "5281      7681          3  The features of the setting would have affecte...   \n",
      "5282      7682          3  There are many features of the setting that af...   \n",
      "5283      7683          3  In  rough raod ahead by Joe Kurmaskie the feat...   \n",
      "5284      7684          3  In the essay, â€œRough Road A head Do Not Exceed...   \n",
      "5285      7685          3  The cyclist is alone traveling on a flat land ...   \n",
      "5286      7686          3  In the story â€œDo not exceed posted speed Limit...   \n",
      "5287      7687          3  The setting effected the performence of the cy...   \n",
      "5288      7688          3  â€œDo not exceed posted speed limitâ€, is a short...   \n",
      "5289      7689          3  The young cycolist began his day confident in ...   \n",
      "5290      7690          3  In the essay the features of the setting affec...   \n",
      "5291      7691          3  Weather, places, the will to continue â€“ all ar...   \n",
      "5292      7692          3  The story â€œRough road ahead: do not exceed pos...   \n",
      "5293      7693          3  In the story, â€œRough Road Ahead: Do not exceed...   \n",
      "5294      7694          3  The features of the setting have a huge effect...   \n",
      "5295      7695          3  The features of the setting effect the cyclist...   \n",
      "5296      7696          3  In the essay â€œRough Road Ahead: Do Not Exceed ...   \n",
      "5297      7697          3  The features of the setting affect the cyclist...   \n",
      "5298      7698          3  The setting of the story, are very dry and hum...   \n",
      "5299      7699          3  The features of the setting affect the cyclist...   \n",
      "5300      7700          3  The cyclist in this essay is trying to go to Y...   \n",
      "5301      7701          3  It give him a hard time so he learn a lot of t...   \n",
      "5302      7702          3  The features of the setting very much affect t...   \n",
      "5303      7703          3  The features of the setting in this story make...   \n",
      "5304      7704          3  In the story, the setting affected the cyclist...   \n",
      "5305      7705          3  The features of the setting affect the cyclist...   \n",
      "5306      7706          3  The setting greatly affects the cyclist trying...   \n",
      "5307      7707          3  The features of the setting affected the cycli...   \n",
      "5308      7708          3  The features of the setting in â€œRough Road Ahe...   \n",
      "\n",
      "      domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "3583       3.333333          53               3         4.000000   \n",
      "3584       6.666667         178              12         4.713483   \n",
      "3585       3.333333          96               8         4.343750   \n",
      "3586       3.333333          89               3         3.876404   \n",
      "3587       6.666667         133               3         4.278195   \n",
      "3588       3.333333          74               3         4.216216   \n",
      "3589       3.333333          43               4         3.883721   \n",
      "3590       0.000000         111               8         3.864865   \n",
      "3591       6.666667          92               6         4.695652   \n",
      "3592      10.000000         151               8         4.920530   \n",
      "3593      10.000000         176               7         4.471591   \n",
      "3594       3.333333          96               6         4.052083   \n",
      "3595      10.000000         137               7         4.656934   \n",
      "3596       6.666667          84               5         4.583333   \n",
      "3597       3.333333          69               1         4.898551   \n",
      "3598       6.666667         126               7         4.293651   \n",
      "3599       6.666667         105               6         4.647619   \n",
      "3600       6.666667         105               9         4.580952   \n",
      "3601       3.333333          91               5         4.494505   \n",
      "3602       6.666667          90               7         4.711111   \n",
      "3603       6.666667          52               2         4.307692   \n",
      "3604       3.333333          34               1         3.823529   \n",
      "3605       6.666667         127               7         4.866142   \n",
      "3606       3.333333          62               3         4.048387   \n",
      "3607       6.666667         115               6         4.139130   \n",
      "3608      10.000000         184               9         4.456522   \n",
      "3609       3.333333          40               1         3.800000   \n",
      "3610      10.000000         170              12         4.576471   \n",
      "3611       6.666667          89               6         4.539326   \n",
      "3612      10.000000         138               8         4.615942   \n",
      "...             ...         ...             ...              ...   \n",
      "5279       3.333333         127               6         4.259843   \n",
      "5280      10.000000         157               8         4.885350   \n",
      "5281      10.000000         174              11         4.557471   \n",
      "5282       3.333333         116               4         4.318966   \n",
      "5283       6.666667          93               5         4.204301   \n",
      "5284      10.000000         140               9         4.607143   \n",
      "5285      10.000000         120               4         4.458333   \n",
      "5286       3.333333         147               7         4.517007   \n",
      "5287      10.000000         152               7         4.296053   \n",
      "5288      10.000000         185               5         4.491892   \n",
      "5289      10.000000         163               8         4.325153   \n",
      "5290       6.666667          86               6         4.302326   \n",
      "5291       6.666667          90               4         4.311111   \n",
      "5292       6.666667         153               7         4.254902   \n",
      "5293      10.000000         193              10         4.917098   \n",
      "5294      10.000000         179              10         4.134078   \n",
      "5295       3.333333          63               5         4.174603   \n",
      "5296       6.666667         189               8         4.539683   \n",
      "5297       6.666667          93               6         4.623656   \n",
      "5298       6.666667          84               6         4.630952   \n",
      "5299       3.333333         113               3         3.973451   \n",
      "5300      10.000000         189              13         4.291005   \n",
      "5301       0.000000          52               2         3.980769   \n",
      "5302      10.000000         122               7         4.368852   \n",
      "5303       6.666667         118               7         4.372881   \n",
      "5304       6.666667          68               6         4.220588   \n",
      "5305       3.333333          54               3         4.462963   \n",
      "5306       6.666667         113               5         4.309735   \n",
      "5307       6.666667         150               7         4.526667   \n",
      "5308      10.000000         119               5         4.403361   \n",
      "\n",
      "      num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
      "3583                      0                   0             30      ...         \n",
      "3584                      0                   0             81      ...         \n",
      "3585                      1                   0             44      ...         \n",
      "3586                      0                   2             44      ...         \n",
      "3587                      0                   0             66      ...         \n",
      "3588                      0                   0             33      ...         \n",
      "3589                      0                   0             24      ...         \n",
      "3590                      0                   0             61      ...         \n",
      "3591                      0                   0             44      ...         \n",
      "3592                      0                   0             55      ...         \n",
      "3593                      0                   0             83      ...         \n",
      "3594                      1                   0             49      ...         \n",
      "3595                      0                   0             59      ...         \n",
      "3596                      0                   0             43      ...         \n",
      "3597                      0                   0             31      ...         \n",
      "3598                      0                   0             65      ...         \n",
      "3599                      0                   0             51      ...         \n",
      "3600                      0                   0             52      ...         \n",
      "3601                      0                   0             44      ...         \n",
      "3602                      0                   0             38      ...         \n",
      "3603                      0                   0             26      ...         \n",
      "3604                      0                   0             19      ...         \n",
      "3605                      0                   0             58      ...         \n",
      "3606                      0                   0             33      ...         \n",
      "3607                      0                   0             61      ...         \n",
      "3608                      1                   0             78      ...         \n",
      "3609                      0                   0             25      ...         \n",
      "3610                      0                   0             82      ...         \n",
      "3611                      0                   0             43      ...         \n",
      "3612                      0                   0             67      ...         \n",
      "...                     ...                 ...            ...      ...         \n",
      "5279                      0                   0             70      ...         \n",
      "5280                      0                   0             61      ...         \n",
      "5281                      0                   0             80      ...         \n",
      "5282                      0                   0             61      ...         \n",
      "5283                      0                   0             46      ...         \n",
      "5284                      0                   0             69      ...         \n",
      "5285                      0                   0             55      ...         \n",
      "5286                      0                   0             63      ...         \n",
      "5287                      0                   0             81      ...         \n",
      "5288                      0                   0             89      ...         \n",
      "5289                      0                   0             71      ...         \n",
      "5290                      0                   0             52      ...         \n",
      "5291                      0                   0             41      ...         \n",
      "5292                      0                   0             58      ...         \n",
      "5293                      0                   0             86      ...         \n",
      "5294                      1                   0             92      ...         \n",
      "5295                      0                   0             35      ...         \n",
      "5296                      0                   0             86      ...         \n",
      "5297                      0                   0             44      ...         \n",
      "5298                      0                   0             39      ...         \n",
      "5299                      0                   0             62      ...         \n",
      "5300                      0                   0             92      ...         \n",
      "5301                      0                   0             27      ...         \n",
      "5302                      0                   0             63      ...         \n",
      "5303                      0                   0             43      ...         \n",
      "5304                      0                   0             29      ...         \n",
      "5305                      0                   0             22      ...         \n",
      "5306                      1                   0             53      ...         \n",
      "5307                      0                   0             72      ...         \n",
      "5308                      0                   0             56      ...         \n",
      "\n",
      "      verb_count  foreign_count  adj_count  conj_count  adv_count  \\\n",
      "3583           9              0          1          12          0   \n",
      "3584          23              0         10          35          9   \n",
      "3585          14              0          8          11          7   \n",
      "3586           8              0          8          13          3   \n",
      "3587          21              0          7          24          7   \n",
      "3588          14              0          6          10          4   \n",
      "3589           9              0          3           7          0   \n",
      "3590          16              0          2          16          8   \n",
      "3591          16              0          5           8          9   \n",
      "3592          20              0         10          13         14   \n",
      "3593          37              0          7          20         12   \n",
      "3594          16              0          6          11         10   \n",
      "3595          17              0          7          24         10   \n",
      "3596          10              0          4          16          7   \n",
      "3597          13              0          5          13          4   \n",
      "3598          18              0          6          21          4   \n",
      "3599          12              0          6          18          6   \n",
      "3600          17              0          9          17          7   \n",
      "3601          10              0          3          20          4   \n",
      "3602          14              0          5           9          6   \n",
      "3603           7              0          2          12          5   \n",
      "3604           5              0          0           6          1   \n",
      "3605          25              0          3          18          6   \n",
      "3606           8              0          0          13          1   \n",
      "3607          17              0          8          20          8   \n",
      "3608          26              0         10          23         15   \n",
      "3609           6              0          1           8          5   \n",
      "3610          23              0          7          28          7   \n",
      "3611          14              0          3          12          8   \n",
      "3612          21              0          8          20          8   \n",
      "...          ...            ...        ...         ...        ...   \n",
      "5279          34              0          2          23          6   \n",
      "5280          25              0          8          20         13   \n",
      "5281          26              0          6          23         13   \n",
      "5282          13              0         10          21          5   \n",
      "5283          14              0          3          18          2   \n",
      "5284          23              0          4          19         15   \n",
      "5285          11              0          5          19          8   \n",
      "5286          26              0          4          27          9   \n",
      "5287          28              0          7          24         13   \n",
      "5288          24              0          7          38          7   \n",
      "5289          21              0          5          28         16   \n",
      "5290          19              0          4          14          1   \n",
      "5291           7              0          6          12          2   \n",
      "5292          15              0          5          24          9   \n",
      "5293          27              0          9          30          7   \n",
      "5294          27              0         11          27         12   \n",
      "5295           9              0          5          11          3   \n",
      "5296          28              0          6          36          9   \n",
      "5297          10              0          1          13          6   \n",
      "5298          13              0          5          18          3   \n",
      "5299          17              0          3          20          5   \n",
      "5300          35              0          8          33         15   \n",
      "5301           8              0          4           8          7   \n",
      "5302          25              0         10          18          8   \n",
      "5303          15              0          9          13          8   \n",
      "5304          14              0          5          10          0   \n",
      "5305           8              0          3          10          6   \n",
      "5306          14              0          6          20         10   \n",
      "5307          20              0          8          17         21   \n",
      "5308          19              0          4          21          7   \n",
      "\n",
      "      beauty_score  maturity_score  vocab  sentiment_essay  Grammar_check  \n",
      "3583     15.782550    4.211245e-04     23         0.500000              1  \n",
      "3584     88.153469    5.079419e-09     73         0.157980             22  \n",
      "3585     35.227528    6.453344e-05     47         0.298810              2  \n",
      "3586     26.585285    1.808810e-01     29         0.148295             13  \n",
      "3587     47.416841    1.054718e-01     60         0.101667             10  \n",
      "3588     23.153779    1.579234e-01     34         0.156667              8  \n",
      "3589     12.086772    1.707108e-03     17         0.116667              1  \n",
      "3590     37.178487    1.783353e-01     48         0.011111              3  \n",
      "3591     48.190702    9.567068e-02     47         0.055093              2  \n",
      "3592     94.994850    1.031497e-01     83        -0.061620              3  \n",
      "3593     85.032021    8.440319e-02     91         0.037302             14  \n",
      "3594     35.320558    2.217965e-03     46         0.065368              1  \n",
      "3595     50.907213    1.205586e-01     83         0.042500              3  \n",
      "3596     29.093033    1.139057e-01     41         0.233333              1  \n",
      "3597     38.193740    1.000375e-01     36         0.067007              7  \n",
      "3598     59.272629    1.214973e-01     62        -0.255556              6  \n",
      "3599     55.338440    1.030286e-01     58         0.121944              7  \n",
      "3600     47.890594    1.111123e-01     54         0.149688              2  \n",
      "3601     46.143020    4.617131e-03     46         0.257500              1  \n",
      "3602     31.414440    1.946652e-01     44         0.113591              8  \n",
      "3603     14.929940    1.334660e-01     28        -0.025000              0  \n",
      "3604      8.787934    3.251653e-01     14         0.285714              1  \n",
      "3605     44.594949    7.945611e-02     74        -0.233333              4  \n",
      "3606     34.393776    1.625153e-01     28         0.033333              4  \n",
      "3607     52.735616    8.425966e-02     54        -0.046417              4  \n",
      "3608     78.322038    8.114595e-02     96         0.060625              4  \n",
      "3609     12.977450    5.775859e-01     18         0.016667              3  \n",
      "3610     57.406992    9.631813e-02     90        -0.205556             11  \n",
      "3611     62.537478    1.192645e-01     39         0.069213              2  \n",
      "3612     45.992788    8.456705e-02     79         0.160065              4  \n",
      "...            ...             ...    ...              ...            ...  \n",
      "5279     49.499180    5.689604e-02     61         0.206250              3  \n",
      "5280     85.765633    9.662059e-08     83         0.120000             18  \n",
      "5281     88.855694    5.809439e-02     88         0.055556              7  \n",
      "5282     65.841875    1.281547e-01     50         0.053125              5  \n",
      "5283     55.725638    1.396637e-01     43        -0.150000              5  \n",
      "5284     48.062803    2.306695e-05     74         0.126111              2  \n",
      "5285     66.173284    3.572531e-07     60         0.113889              2  \n",
      "5286     60.822239    6.458780e-02     79         0.254040              4  \n",
      "5287     83.446592    5.732989e-04     71         0.267460              3  \n",
      "5288     93.282406    2.846944e-02     98         0.066239              4  \n",
      "5289     68.913477    9.691076e-02     74        -0.088241             10  \n",
      "5290     30.655138    1.243085e-01     34         0.082143              2  \n",
      "5291     31.372727    1.133716e-01     49         0.032449              0  \n",
      "5292     67.423302    1.083759e-01     79        -0.137395             14  \n",
      "5293    110.114470    8.185191e-02    109         0.017560             12  \n",
      "5294     66.011195    1.791096e-07     85         0.001808              5  \n",
      "5295     35.495235    7.911481e-02     30         0.283333              0  \n",
      "5296    110.019031    1.227976e-03     93        -0.043157              7  \n",
      "5297     44.542213    3.116975e-03     44         0.178912              6  \n",
      "5298     53.977081    6.651969e-05     38        -0.126296              4  \n",
      "5299     93.469596    1.116364e-01     53         0.113542              2  \n",
      "5300     90.943981    8.715690e-02    102         0.173535              6  \n",
      "5301     20.646406    1.842425e-01     32        -0.052778              5  \n",
      "5302     52.856962    2.688643e-03     60        -0.031068              1  \n",
      "5303     36.452769    1.585818e-01     54        -0.032500             18  \n",
      "5304     24.403740    1.806083e-01     37        -0.095000              0  \n",
      "5305     17.590276    8.368226e-02     35        -0.100000              2  \n",
      "5306     67.957764    1.861900e-01     46         0.292208              2  \n",
      "5307     61.693953    2.693848e-02     88         0.226042              1  \n",
      "5308     39.478951    9.677486e-02     62         0.167312              2  \n",
      "\n",
      "[1726 rows x 21 columns],       essay_id  essay_set                                              essay  \\\n",
      "5309      8863          4  The author concludes the story with this becau...   \n",
      "5310      8864          4  The narrater has that in with Paragraph becuse...   \n",
      "5311      8865          4  The author concludes the story with that passa...   \n",
      "5312      8866          4  The author ended the story with this paragraph...   \n",
      "5313      8867          4  The author concludes the story with this parag...   \n",
      "5314      8868          4  The reason the author concludes the story with...   \n",
      "5315      8869          4  The story ended with the author saying, \"when ...   \n",
      "5316      8870          4  I believe that the author concludes the story ...   \n",
      "5317      8871          4  The author would conclude the story with that ...   \n",
      "5318      8872          4  In the story, \"Winter Hibiscus,\" by Minfong Ho...   \n",
      "5319      8873          4  The author concludes the story with this parag...   \n",
      "5320      8874          4  I think he concluded the story with paragraph ...   \n",
      "5321      8875          4  The author concludes the story with this parag...   \n",
      "5322      8876          4  The author concludes the story with this parag...   \n",
      "5323      8877          4  Why the author concludes the story with this p...   \n",
      "5324      8878          4  He states this as the conclusion because it sh...   \n",
      "5325      8879          4  I believe the author concludes the story with ...   \n",
      "5326      8880          4  The arthur uses it because Saeng got confidenc...   \n",
      "5327      8881          4  the author concludes the story this way becaus...   \n",
      "5328      8882          4  I think the author ends the story with this pa...   \n",
      "5329      8883          4  I think that the author concludes the story wi...   \n",
      "5330      8884          4  They probably ended it like that to build susp...   \n",
      "5331      8885          4   Inâ€ Winter Hibiscus,â€ by Minfong Ho, the main...   \n",
      "5332      8886          4  The author concludes the story with this parag...   \n",
      "5333      8887          4  The author concludes this story with this para...   \n",
      "5334      8888          4  In the story, â€œWinter Hibiscusâ€, the author en...   \n",
      "5335      8889          4  I think that she ended the story like that bec...   \n",
      "5336      8890          4  The author concludes the story with this parag...   \n",
      "5337      8891          4  By ending the story with the idea of retaking ...   \n",
      "5338      8892          4  The author concluded the story with that speci...   \n",
      "...        ...        ...                                                ...   \n",
      "7051     10613          4  Coming back from failing her driving test, Sae...   \n",
      "7052     10614          4  The story of â€œWinter Hibiscusâ€ is a prolonged ...   \n",
      "7053     10615          4  In â€œWinter Hibiscusâ€ by Minfong Ho, Saeng has ...   \n",
      "7054     10616          4  Since it is getting close to winter the geese ...   \n",
      "7055     10617          4  The auther concludes the story with this parag...   \n",
      "7056     10618          4  The author concludes her story with that last ...   \n",
      "7057     10619          4  The author is relating to that when the geese ...   \n",
      "7058     10620          4  I think the Author ends this as the concluding...   \n",
      "7059     10621          4  the author concludes the story with that parag...   \n",
      "7060     10622          4  I think the author concludes the story with th...   \n",
      "7061     10623          4  In the short story â€œWinter Hibiscusâ€ Saeng dec...   \n",
      "7062     10624          4  The author concludes the story with that parag...   \n",
      "7063     10625          4  I think that the author concludes the story li...   \n",
      "7064     10626          4  The last paragraph of the story was: â€œWhen the...   \n",
      "7065     10627          4  Saeng told her mother that she failed the test...   \n",
      "7066     10628          4  The Author concludes that, when the geese come...   \n",
      "7067     10629          4  The reason the author chose this paragraph is ...   \n",
      "7068     10630          4  The author concludes the story with the paragr...   \n",
      "7069     10631          4  The author concludes the story with this sente...   \n",
      "7070     10632          4  The author concludes the story with that parag...   \n",
      "7071     10633          4  The author concludes the story with this parag...   \n",
      "7072     10634          4  She ends it with that statement to show that s...   \n",
      "7073     10635          4  She realized that things come and go but also ...   \n",
      "7074     10636          4  The author ended the story with that paragraph...   \n",
      "7075     10637          4  I think the author concludes this story like t...   \n",
      "7076     10638          4  To me it seam like the whoever was saying that...   \n",
      "7077     10639          4  The author concludes the story with this becau...   \n",
      "7078     10640          4  The author uses this conclusion for a reason. ...   \n",
      "7079     10641          4  The author concludes the story with this parag...   \n",
      "7080     10642          4  There was a specific reason as to why the auth...   \n",
      "\n",
      "      domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "5309       0.000000          61               3         4.065574   \n",
      "5310       0.000000          42               2         4.023810   \n",
      "5311      10.000000         136               5         4.830882   \n",
      "5312       6.666667          65               6         4.846154   \n",
      "5313       6.666667         128               9         4.515625   \n",
      "5314       3.333333          72               5         4.541667   \n",
      "5315       0.000000          92               3         4.423913   \n",
      "5316       6.666667          82               4         4.548780   \n",
      "5317       3.333333          32               2         4.156250   \n",
      "5318       6.666667         117               8         4.094017   \n",
      "5319       6.666667         179               8         4.167598   \n",
      "5320       0.000000          42               1         3.904762   \n",
      "5321       3.333333          31               2         4.870968   \n",
      "5322       6.666667         177               5         4.711864   \n",
      "5323       0.000000          71               3         4.507042   \n",
      "5324       3.333333          48               2         3.958333   \n",
      "5325       6.666667          87               5         4.390805   \n",
      "5326       3.333333          34               2         4.823529   \n",
      "5327      10.000000         164               5         4.597561   \n",
      "5328       3.333333          50               3         4.220000   \n",
      "5329       3.333333          81               2         4.222222   \n",
      "5330       3.333333          40               2         4.500000   \n",
      "5331      10.000000         145               2         4.337931   \n",
      "5332       3.333333          59               2         3.423729   \n",
      "5333       6.666667          88               4         4.136364   \n",
      "5334       6.666667         176              12         4.744318   \n",
      "5335       3.333333         115               4         3.947826   \n",
      "5336       6.666667         105               6         4.666667   \n",
      "5337      10.000000         139               6         4.330935   \n",
      "5338       3.333333          79               5         4.873418   \n",
      "...             ...         ...             ...              ...   \n",
      "7051       6.666667         117               6         4.717949   \n",
      "7052      10.000000         146               7         4.876712   \n",
      "7053       6.666667         134               6         4.582090   \n",
      "7054       6.666667          85               6         4.317647   \n",
      "7055       0.000000          21               1         4.952381   \n",
      "7056       3.333333          60               2         4.400000   \n",
      "7057       6.666667          96               5         3.843750   \n",
      "7058       6.666667         152               6         4.342105   \n",
      "7059       0.000000          24               1         5.125000   \n",
      "7060       3.333333          47               3         4.170213   \n",
      "7061       3.333333          89               5         4.280899   \n",
      "7062       3.333333          40               2         4.925000   \n",
      "7063       3.333333          94               7         4.351064   \n",
      "7064       6.666667         147               5         4.571429   \n",
      "7065       3.333333          32               4         3.937500   \n",
      "7066       6.666667         116               5         4.060345   \n",
      "7067       0.000000          34               2         4.588235   \n",
      "7068       6.666667         121               7         4.256198   \n",
      "7069       6.666667          96               7         4.427083   \n",
      "7070       3.333333          41               2         4.926829   \n",
      "7071       3.333333          48               2         4.291667   \n",
      "7072       6.666667         126               3         3.984127   \n",
      "7073       6.666667         112              10         4.062500   \n",
      "7074       3.333333          62               3         4.177419   \n",
      "7075       3.333333          29               1         4.620690   \n",
      "7076       0.000000          82               2         4.304878   \n",
      "7077       6.666667          78               5         4.102564   \n",
      "7078       3.333333          57               3         4.631579   \n",
      "7079       0.000000          37               2         4.675676   \n",
      "7080       6.666667          75               5         4.266667   \n",
      "\n",
      "      num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
      "5309                      0                   0             29      ...         \n",
      "5310                      0                   0             21      ...         \n",
      "5311                      0                   0             68      ...         \n",
      "5312                      0                   0             29      ...         \n",
      "5313                      0                   0             62      ...         \n",
      "5314                      0                   0             39      ...         \n",
      "5315                      0                   0             48      ...         \n",
      "5316                      0                   0             43      ...         \n",
      "5317                      0                   0             18      ...         \n",
      "5318                      0                   0             59      ...         \n",
      "5319                      0                   0             97      ...         \n",
      "5320                      0                   0             25      ...         \n",
      "5321                      0                   0             16      ...         \n",
      "5322                      0                   0             85      ...         \n",
      "5323                      0                   0             40      ...         \n",
      "5324                      0                   0             32      ...         \n",
      "5325                      0                   0             46      ...         \n",
      "5326                      0                   0             15      ...         \n",
      "5327                      0                   0             73      ...         \n",
      "5328                      0                   0             27      ...         \n",
      "5329                      0                   0             45      ...         \n",
      "5330                      0                   0             17      ...         \n",
      "5331                      0                   0             58      ...         \n",
      "5332                      0                   0             27      ...         \n",
      "5333                      0                   0             48      ...         \n",
      "5334                      0                   3             79      ...         \n",
      "5335                      0                   0             70      ...         \n",
      "5336                      0                   0             50      ...         \n",
      "5337                      0                   0             66      ...         \n",
      "5338                      0                   0             40      ...         \n",
      "...                     ...                 ...            ...      ...         \n",
      "7051                      0                   0             51      ...         \n",
      "7052                      0                   0             75      ...         \n",
      "7053                      0                   0             68      ...         \n",
      "7054                      0                   0             44      ...         \n",
      "7055                      0                   0             12      ...         \n",
      "7056                      0                   0             33      ...         \n",
      "7057                      0                   0             61      ...         \n",
      "7058                      0                   0             82      ...         \n",
      "7059                      0                   0             13      ...         \n",
      "7060                      0                   0             21      ...         \n",
      "7061                      0                   0             43      ...         \n",
      "7062                      0                   0             21      ...         \n",
      "7063                      0                   0             46      ...         \n",
      "7064                      0                   0             72      ...         \n",
      "7065                      0                   0             18      ...         \n",
      "7066                      0                   0             60      ...         \n",
      "7067                      0                   0             17      ...         \n",
      "7068                      0                   0             62      ...         \n",
      "7069                      0                   3             43      ...         \n",
      "7070                      0                   0             20      ...         \n",
      "7071                      0                   0             23      ...         \n",
      "7072                      0                   0             64      ...         \n",
      "7073                      0                   0             54      ...         \n",
      "7074                      0                   0             34      ...         \n",
      "7075                      0                   0             14      ...         \n",
      "7076                      0                   0             44      ...         \n",
      "7077                      0                   0             43      ...         \n",
      "7078                      0                   0             25      ...         \n",
      "7079                      0                   0             18      ...         \n",
      "7080                      0                   0             40      ...         \n",
      "\n",
      "      verb_count  foreign_count  adj_count  conj_count  adv_count  \\\n",
      "5309           3              0          3           7          5   \n",
      "5310           7              0          1           8          2   \n",
      "5311          13              0          2          30          3   \n",
      "5312          13              0          4          10          5   \n",
      "5313          17              0          6          21          8   \n",
      "5314           8              0          1          11          1   \n",
      "5315          13              0          3          16          6   \n",
      "5316          11              0          6          12          5   \n",
      "5317           3              0          0           7          1   \n",
      "5318          18              0          6          21          4   \n",
      "5319          19              0          5          25         11   \n",
      "5320           5              0          1           5          6   \n",
      "5321           3              0          0           3          1   \n",
      "5322          18              0         10          29         10   \n",
      "5323           6              0          4          16          2   \n",
      "5324           4              0          1          11          1   \n",
      "5325          14              0          0          11          3   \n",
      "5326           4              0          1           8          3   \n",
      "5327          18              0         11          20          8   \n",
      "5328           9              0          1           9          3   \n",
      "5329           8              0          0          14          4   \n",
      "5330           7              0          1           7          3   \n",
      "5331          10              0          8          18          7   \n",
      "5332           6              0          2           7          3   \n",
      "5333           8              0          3          12          3   \n",
      "5334          24              0         11          19          8   \n",
      "5335          17              0          3          17         11   \n",
      "5336          12              0          3          14          5   \n",
      "5337          25              0          6          24          5   \n",
      "5338          13              0          5          10          4   \n",
      "...          ...            ...        ...         ...        ...   \n",
      "7051          17              0          4          16          9   \n",
      "7052          15              0          3          25          8   \n",
      "7053          10              0          2          28          6   \n",
      "7054          11              0          3          10          6   \n",
      "7055           1              0          0           3          2   \n",
      "7056           2              0          4          14          3   \n",
      "7057          19              0          7          13          5   \n",
      "7058          16              0          5          21          5   \n",
      "7059           4              0          0           6          0   \n",
      "7060           7              0          2           8          5   \n",
      "7061          10              0          5          12          5   \n",
      "7062           4              0          1           8          1   \n",
      "7063          14              0          3          11          7   \n",
      "7064          22              0          6          25          7   \n",
      "7065           5              0          0           7          1   \n",
      "7066          17              0          9          15         11   \n",
      "7067           3              0          1           4          0   \n",
      "7068          15              0          3          17         12   \n",
      "7069          14              0          5          13          5   \n",
      "7070           4              0          1           7          2   \n",
      "7071           8              0          3           4          3   \n",
      "7072          18              0          9          17          4   \n",
      "7073          21              0          6          21          4   \n",
      "7074          13              0          1          11          5   \n",
      "7075           3              0          1           4          0   \n",
      "7076          21              0          0          18          5   \n",
      "7077           8              0          5          12          5   \n",
      "7078           7              0          2           7          2   \n",
      "7079           3              0          1           4          3   \n",
      "7080          12              0          3          10          5   \n",
      "\n",
      "      beauty_score  maturity_score  vocab  sentiment_essay  Grammar_check  \n",
      "5309     22.100683    2.479557e-01     29         0.160000              7  \n",
      "5310     14.267585    3.945685e-01     14         0.300000             10  \n",
      "5311     47.355104    6.308392e-02     66         0.224747              4  \n",
      "5312     41.181767    1.330543e-01     42         0.190303              2  \n",
      "5313     46.948023    1.642766e-05     69         0.135227              6  \n",
      "5314     22.322860    9.488079e-02     41         0.100000              4  \n",
      "5315     33.781850    4.759958e-03     54         0.025000              6  \n",
      "5316     47.631959    7.404965e-02     42         0.180000              0  \n",
      "5317     13.113857    7.017118e-05     17        -0.150000              0  \n",
      "5318     41.878912    7.298189e-02     54         0.208750              7  \n",
      "5319    115.201137    4.471274e-02     87         0.054383              2  \n",
      "5320     12.958404    3.281403e-01     26         0.000000              0  \n",
      "5321     11.859936    1.064207e-02     22         0.000000              1  \n",
      "5322     89.865878    8.402083e-02     97         0.059785             11  \n",
      "5323     23.952928    3.341103e-03     35         0.225758              5  \n",
      "5324     16.984374    1.508246e-03     19         0.375000              0  \n",
      "5325     36.256842    1.491040e-01     48        -0.057143              2  \n",
      "5326     12.278472    2.329826e-02     18         0.100000              4  \n",
      "5327     65.770205    6.280505e-02     82         0.043452             15  \n",
      "5328     32.941119    1.401687e-01     29         0.100000              2  \n",
      "5329     30.011018    1.224328e-01     43        -0.005556              3  \n",
      "5330     14.543841    2.523352e-01     21         0.566667              1  \n",
      "5331     47.100907    1.381495e-01     77         0.121717             22  \n",
      "5332     35.962846    2.411018e-01     26         0.000000             13  \n",
      "5333     29.734848    9.046515e-02     43         0.190909              5  \n",
      "5334     82.852091    6.485908e-02    110         0.054762              9  \n",
      "5335     38.388241    1.862070e-05     65         0.029167              1  \n",
      "5336     39.943220    3.937540e-05     57        -0.162727              5  \n",
      "5337     49.941467    8.992165e-04     69         0.157851             19  \n",
      "5338     31.360285    9.688415e-02     55         0.280000              4  \n",
      "...            ...             ...    ...              ...            ...  \n",
      "7051     46.983797    3.817066e-07     65         0.042045             10  \n",
      "7052     54.761632    5.606508e-02     76         0.195833             10  \n",
      "7053     47.622727    8.424588e-02     64         0.094444              4  \n",
      "7054     28.550074    8.266404e-05     45         0.062500              3  \n",
      "7055      9.302966    4.669411e-01     14         0.200000              2  \n",
      "7056     35.061922    1.784483e-01     30         0.133333              7  \n",
      "7057     51.120881    1.641090e-01     49         0.160000              2  \n",
      "7058     51.664879    1.145214e-03     80         0.562500              2  \n",
      "7059      8.588644    2.443589e-01     16         0.000000              3  \n",
      "7060     33.653657    2.203241e-01     25         0.068182              2  \n",
      "7061     34.744965    9.439266e-02     41         0.033333              4  \n",
      "7062     15.093446    2.905513e-01     25         0.250000              1  \n",
      "7063     34.874030    1.118176e-01     56         0.138961              1  \n",
      "7064     51.030765    1.343530e-01     85         0.040404              4  \n",
      "7065     12.132815    3.371551e-01     13        -0.500000              3  \n",
      "7066     57.990609    8.381489e-02     66         0.178704              0  \n",
      "7067     10.858146    1.932099e-02     18         0.000000              1  \n",
      "7068     71.013658    3.866648e-07     67         0.038657              2  \n",
      "7069     54.921364    6.350999e-02     63         0.073214              0  \n",
      "7070     14.794679    3.935878e-01     26         0.400000              3  \n",
      "7071     16.272911    3.190198e-01     25         0.450000              1  \n",
      "7072     42.090090    8.350846e-02     63         0.093939              3  \n",
      "7073     40.761309    8.323215e-02     50        -0.094733              5  \n",
      "7074     25.632752    5.459198e-03     28         0.200000              1  \n",
      "7075     11.602537    2.931603e-01     18         0.775000              2  \n",
      "7076     29.489816    1.157890e-01     51         0.175000              7  \n",
      "7077     27.781646    1.456525e-01     43         0.186195              2  \n",
      "7078     30.680615    4.832824e-03     36         0.250000              5  \n",
      "7079     13.403006    2.262151e-01     21         0.200000              3  \n",
      "7080     40.268722    1.766399e-01     34        -0.006061              2  \n",
      "\n",
      "[1772 rows x 21 columns],       essay_id  essay_set                                              essay  \\\n",
      "7081     11827          5  In this memoir of Narciso Rodriguez, @PERSON3'...   \n",
      "7082     11828          5  Throughout the excerpt from Home the Blueprint...   \n",
      "7083     11829          5  The mood the author created in the memoir is l...   \n",
      "7084     11830          5  The mood created by the author is showing how ...   \n",
      "7085     11831          5  The mood created in the memoir is happiness an...   \n",
      "7086     11832          5  The mood definitely helps you feel and compreh...   \n",
      "7087     11833          5  In Narciso Rodriguez's memoir from \"Home: The ...   \n",
      "7088     11834          5  In the excerpt the mood created by the author ...   \n",
      "7089     11835          5  The mood created by the author in the memoir i...   \n",
      "7090     11836          5  The author generally stayed in one mood. The m...   \n",
      "7091     11837          5  The mood that the author creates in this story...   \n",
      "7092     11838          5  The mood in the memoir that the author creates...   \n",
      "7093     11839          5  The mood of the memoir is very upbeat, happy m...   \n",
      "7094     11840          5  Describe the mood by the author in the memoir....   \n",
      "7095     11841          5  The mood created by the author in the memoir w...   \n",
      "7096     11842          5   The mood created by the author in this memoir...   \n",
      "7097     11843          5  In this memoir, the author creates a mood. The...   \n",
      "7098     11844          5  The author of this memoir created a mood about...   \n",
      "7099     11845          5  The mood created by this author is clearly gre...   \n",
      "7100     11846          5  The mood set by the author in the memoir is a ...   \n",
      "7101     11847          5  The mood created by the author in the memoir i...   \n",
      "7102     11848          5  In the memoir, the author created a very upbea...   \n",
      "7103     11849          5  The mood created by the author of the memoir i...   \n",
      "7104     11850          5  The author, Narciso Rodriguez, creates a very ...   \n",
      "7105     11851          5  Based on this memoir, the mood created by the ...   \n",
      "7106     11852          5  The mood created by Narciso Rodriguez was happ...   \n",
      "7107     11853          5  In the memoir Narciso Rodriguez the author cre...   \n",
      "7108     11854          5  @PERSON1's mood was happy and greatful. @PERSO...   \n",
      "7109     11855          5  In Narciso Rodriguez's memoir, the mood and fe...   \n",
      "7110     11856          5  In the memoir @PERSON1 it talks about making a...   \n",
      "...        ...        ...                                                ...   \n",
      "8856     13602          5  The mood created in Narciso Rodriguez from Hom...   \n",
      "8857     13603          5  It creates a mood of happiness \"but Cuban musi...   \n",
      "8858     13604          5  The mood created by the author in this memoir ...   \n",
      "8859     13605          5  In the memoir \"Narciso Rodriguez\" from \"Home: ...   \n",
      "8860     13606          5  In the memoir \"Narciso Rodriguez\" by Narciso R...   \n",
      "8861     13607          5  In this memoir I think the mood is a warm and ...   \n",
      "8862     13608          5  The mood in this memoir is very relaxed and pr...   \n",
      "8863     13609          5  There are many different ways on how the mood ...   \n",
      "8864     13610          5  The mood created by the memoir is greatfulness...   \n",
      "8865     13611          5  The mood that the author in the memoir created...   \n",
      "8866     13612          5  In the memoir many moods were expressed. I thi...   \n",
      "8867     13613          5  In the story memoir the author created a coupl...   \n",
      "8868     13614          5  In the memoir, Narciso Rodriguez, from Home: T...   \n",
      "8869     13615          5  In this memoir the mood created by the author ...   \n",
      "8870     13616          5  The mood is @CAPS1 in many ways. First, the mo...   \n",
      "8871     13617          5  In the memoir, \"Narciso Rodriguez\" from Home: ...   \n",
      "8872     13618          5  Warmth                                        ...   \n",
      "8873     13619          5  The @CAPS1 sets a good mood in this memoir abo...   \n",
      "8874     13620          5  The mood set in this memoir by the author, was...   \n",
      "8875     13621          5  The mood I think the author created in the mem...   \n",
      "8876     13622          5  The mood created by the author is loving and h...   \n",
      "8877     13623          5  The mood created by the author in the memoir i...   \n",
      "8878     13624          5  The mood created in this memoir was manily for...   \n",
      "8879     13625          5  The mood in the author's story is happy and gr...   \n",
      "8880     13626          5  The mood created by the author in the memoir i...   \n",
      "8881     13627          5  The mood of this memoir is nonfiction. The moo...   \n",
      "8882     13628          5  The mood was created by the author in the memo...   \n",
      "8883     13629          5  In the memoir \"Narciso Rodriguez\", the mood cr...   \n",
      "8884     13630          5  The mood created @CAPS3 the author, Narciso Ro...   \n",
      "8885     13631          5  The author created such a specific mood for th...   \n",
      "\n",
      "      domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "7081            5.0         139               8         4.431655   \n",
      "7082            5.0         167               7         4.425150   \n",
      "7083            7.5         109               6         4.990826   \n",
      "7084            2.5          71               3         4.591549   \n",
      "7085            7.5         127               8         4.401575   \n",
      "7086            7.5         150               6         4.453333   \n",
      "7087            7.5         171               9         4.888889   \n",
      "7088           10.0         212              10         4.834906   \n",
      "7089            7.5         158               9         4.753165   \n",
      "7090            7.5         146               7         4.315068   \n",
      "7091            2.5          38               1         3.815789   \n",
      "7092            2.5         200              11         4.310000   \n",
      "7093            2.5          79               4         4.189873   \n",
      "7094            2.5          92               6         4.108696   \n",
      "7095            5.0         103               7         4.524272   \n",
      "7096            5.0         144               3         4.138889   \n",
      "7097           10.0         156              11         4.737179   \n",
      "7098            7.5         158               6         4.525316   \n",
      "7099            5.0         131               8         4.480916   \n",
      "7100            5.0         113               5         4.451327   \n",
      "7101           10.0         186               9         4.607527   \n",
      "7102            5.0         177               8         4.310734   \n",
      "7103            5.0         134               7         4.410448   \n",
      "7104           10.0         190              13         4.763158   \n",
      "7105            5.0         106               6         4.358491   \n",
      "7106            5.0         122               8         4.540984   \n",
      "7107            5.0          84               5         4.904762   \n",
      "7108            2.5          49               4         4.510204   \n",
      "7109            7.5         149               7         4.563758   \n",
      "7110           10.0         162               6         4.765432   \n",
      "...             ...         ...             ...              ...   \n",
      "8856            5.0         134               9         4.447761   \n",
      "8857            2.5          28               2         4.071429   \n",
      "8858            5.0          93               4         4.838710   \n",
      "8859            7.5         126               8         4.841270   \n",
      "8860            7.5         140               4         4.621429   \n",
      "8861           10.0         211               8         4.251185   \n",
      "8862            5.0          81               7         4.259259   \n",
      "8863            5.0         129               8         4.100775   \n",
      "8864            5.0          80               6         4.512500   \n",
      "8865            5.0         134               8         4.216418   \n",
      "8866            5.0          72               5         4.763889   \n",
      "8867            5.0          97               5         4.701031   \n",
      "8868            5.0         133              10         4.789474   \n",
      "8869            2.5          73               5         4.301370   \n",
      "8870            5.0         102               7         4.549020   \n",
      "8871            7.5         123               7         4.658537   \n",
      "8872            5.0         265               4         2.339623   \n",
      "8873            7.5         149               7         4.510067   \n",
      "8874           10.0         167              10         4.742515   \n",
      "8875            5.0         113               5         4.141593   \n",
      "8876            5.0         101               5         4.455446   \n",
      "8877            5.0          47               2         4.468085   \n",
      "8878            5.0         124               8         4.185484   \n",
      "8879            7.5         149               7         4.429530   \n",
      "8880            7.5         143               9         4.342657   \n",
      "8881            5.0         129               7         4.705426   \n",
      "8882            0.0          29               1         4.310345   \n",
      "8883           10.0         166               9         4.590361   \n",
      "8884            7.5         132               6         4.704545   \n",
      "8885            5.0          94               6         4.478723   \n",
      "\n",
      "      num_exclamation_marks  num_question_marks  num_stopwords      ...        \\\n",
      "7081                      0                   0             64      ...         \n",
      "7082                      0                   0             85      ...         \n",
      "7083                      0                   0             50      ...         \n",
      "7084                      0                   0             41      ...         \n",
      "7085                      0                   0             70      ...         \n",
      "7086                      0                   0             75      ...         \n",
      "7087                      0                   0             80      ...         \n",
      "7088                      0                   0             99      ...         \n",
      "7089                      0                   0             79      ...         \n",
      "7090                      0                   0             79      ...         \n",
      "7091                      0                   0             18      ...         \n",
      "7092                      0                   0            106      ...         \n",
      "7093                      0                   0             45      ...         \n",
      "7094                      0                   0             51      ...         \n",
      "7095                      0                   0             53      ...         \n",
      "7096                      0                   0             77      ...         \n",
      "7097                      0                   0             75      ...         \n",
      "7098                      0                   0             75      ...         \n",
      "7099                      0                   0             64      ...         \n",
      "7100                      0                   0             57      ...         \n",
      "7101                      0                   0             88      ...         \n",
      "7102                      0                   0            100      ...         \n",
      "7103                      0                   0             61      ...         \n",
      "7104                      0                   0             85      ...         \n",
      "7105                      0                   0             58      ...         \n",
      "7106                      0                   0             54      ...         \n",
      "7107                      0                   0             39      ...         \n",
      "7108                      0                   0             27      ...         \n",
      "7109                      0                   0             77      ...         \n",
      "7110                      0                   0             80      ...         \n",
      "...                     ...                 ...            ...      ...         \n",
      "8856                      0                   0             70      ...         \n",
      "8857                      0                   0             14      ...         \n",
      "8858                      0                   0             46      ...         \n",
      "8859                      0                   0             64      ...         \n",
      "8860                      0                   0             67      ...         \n",
      "8861                      0                   0            113      ...         \n",
      "8862                      0                   0             49      ...         \n",
      "8863                      0                   0             75      ...         \n",
      "8864                      0                   0             44      ...         \n",
      "8865                      0                   0             67      ...         \n",
      "8866                      0                   0             35      ...         \n",
      "8867                      0                   0             43      ...         \n",
      "8868                      0                   0             66      ...         \n",
      "8869                      0                   0             35      ...         \n",
      "8870                      0                   0             48      ...         \n",
      "8871                      0                   0             54      ...         \n",
      "8872                      0                   0             54      ...         \n",
      "8873                      0                   0             72      ...         \n",
      "8874                      0                   0             75      ...         \n",
      "8875                      0                   0             61      ...         \n",
      "8876                      0                   0             54      ...         \n",
      "8877                      0                   0             27      ...         \n",
      "8878                      0                   0             69      ...         \n",
      "8879                      1                   0             77      ...         \n",
      "8880                      0                   0             68      ...         \n",
      "8881                      0                   0             66      ...         \n",
      "8882                      0                   0             15      ...         \n",
      "8883                      0                   0             77      ...         \n",
      "8884                      0                   0             63      ...         \n",
      "8885                      0                   0             52      ...         \n",
      "\n",
      "      verb_count  foreign_count  adj_count  conj_count  adv_count  \\\n",
      "7081          23              0          3          21          3   \n",
      "7082          19              0          9          25         13   \n",
      "7083          15              0          4          19          5   \n",
      "7084          14              0          2          15          2   \n",
      "7085          22              0          3          24          9   \n",
      "7086          13              0         12          27          9   \n",
      "7087          12              0          6          34          8   \n",
      "7088          26              0          8          41         19   \n",
      "7089          24              0          7          29          7   \n",
      "7090          24              0          2          22         10   \n",
      "7091           4              0          2           5          1   \n",
      "7092          16              0          7          35          8   \n",
      "7093           5              0          1          18          8   \n",
      "7094          14              0          4          12          5   \n",
      "7095          13              0          8          20          5   \n",
      "7096          20              0          6          33          8   \n",
      "7097          17              0          6          21          8   \n",
      "7098          22              0          4          22         13   \n",
      "7099          18              0          4          19          7   \n",
      "7100          13              0          5          22          2   \n",
      "7101          26              0          6          32         12   \n",
      "7102          27              0         11          34         13   \n",
      "7103          16              0          4          20          8   \n",
      "7104          26              0         11          33         11   \n",
      "7105          16              0          6          17         10   \n",
      "7106          13              0          7          24          7   \n",
      "7107           9              0         12          12          4   \n",
      "7108          10              0          3           8          2   \n",
      "7109          21              0         12          33          5   \n",
      "7110          19              0          8          30          9   \n",
      "...          ...            ...        ...         ...        ...   \n",
      "8856          24              0          6          21          6   \n",
      "8857           2              0          1           6          0   \n",
      "8858           9              0          2          18          2   \n",
      "8859          27              0          2          22          3   \n",
      "8860          19              0          4          25         10   \n",
      "8861          28              0         13          37          8   \n",
      "8862          12              0          0          15          9   \n",
      "8863          26              0          8          19          5   \n",
      "8864          11              0          0          13          4   \n",
      "8865          11              0          5          24          5   \n",
      "8866          10              0          8          12          2   \n",
      "8867          12              0          2           8          2   \n",
      "8868          18              0          5          23          2   \n",
      "8869          10              0          8          11          9   \n",
      "8870          12              0          3          17          6   \n",
      "8871          19              0          7          14          9   \n",
      "8872          13              0          5          18          5   \n",
      "8873          10              0          9          29          8   \n",
      "8874          25              0         10          23          7   \n",
      "8875          13              0          2          20          5   \n",
      "8876          18              0          7          18          6   \n",
      "8877           5              0          2          12          0   \n",
      "8878          21              0          5          23          6   \n",
      "8879          17              0          7          29         11   \n",
      "8880          26              0          1          24          9   \n",
      "8881          23              0          6          26          6   \n",
      "8882           3              0          0           7          0   \n",
      "8883          30              0         11          21          4   \n",
      "8884          22              0          3          24          7   \n",
      "8885          12              0          3          16          6   \n",
      "\n",
      "      beauty_score  maturity_score  vocab  sentiment_essay  Grammar_check  \n",
      "7081     64.234836    8.069277e-02     62         0.240000              6  \n",
      "7082     84.100454    7.113686e-04     86         0.216387              5  \n",
      "7083     37.214336    2.896558e-05     61         0.312500             14  \n",
      "7084     26.655637    4.489796e-03     35        -0.062500             11  \n",
      "7085     49.998353    1.550228e-01     62         0.414444              7  \n",
      "7086     72.328368    3.765885e-04     85         0.170614              9  \n",
      "7087     56.707889    1.185784e-01     91         0.379167             17  \n",
      "7088     73.250266    3.977227e-02    130         0.356731             14  \n",
      "7089     57.074210    1.565823e-01     79         0.536538              9  \n",
      "7090     51.179384    9.629383e-04     71         0.290000              8  \n",
      "7091     12.148877    2.444691e-03     12        -0.075758              0  \n",
      "7092     82.179895    2.585048e-06    114         0.333137              1  \n",
      "7093     51.819778    8.450215e-02     40         0.392045              2  \n",
      "7094     34.768458    3.647634e-03     37         0.111174              9  \n",
      "7095     34.214604    1.019351e-01     61         0.361979              3  \n",
      "7096     67.300722    1.105454e-03     73         0.070909              7  \n",
      "7097     53.921909    5.681329e-02     91         0.289286              9  \n",
      "7098     88.024384    1.212746e-01    102         0.258609             10  \n",
      "7099     46.056968    7.403509e-02     57         0.173750             17  \n",
      "7100     40.550547    6.068967e-02     58         0.330556              5  \n",
      "7101    105.956893    1.019289e-01    111         0.311818             11  \n",
      "7102     86.113842    5.731727e-02     93         0.139962              3  \n",
      "7103     57.698073    7.103515e-04     69         0.212121              6  \n",
      "7104     60.845717    6.060192e-02    109         0.478409             10  \n",
      "7105     35.583928    1.203396e-01     59         0.292857              5  \n",
      "7106     44.125579    5.231005e-02     74         0.332273             10  \n",
      "7107     26.248443    1.726087e-01     46         0.179079              4  \n",
      "7108     20.264526    2.990922e-01     22         0.525000              2  \n",
      "7109     62.976496    1.268493e-01     73         0.405195              9  \n",
      "7110     52.059884    7.475894e-08     82         0.390769             11  \n",
      "...            ...             ...    ...              ...            ...  \n",
      "8856     43.780891    1.627632e-01     76         0.558333             10  \n",
      "8857     11.157552    2.652202e-01     14         0.366667              2  \n",
      "8858     44.466660    6.383214e-02     53         0.316667              7  \n",
      "8859     45.141666    6.803972e-02     76         0.488889             15  \n",
      "8860     56.331168    1.940006e-05     68         0.407851              9  \n",
      "8861     70.129839    3.657784e-02    108         0.451136             18  \n",
      "8862     41.091198    1.055968e-01     40         0.640000              2  \n",
      "8863     44.134601    2.096616e-01     59         0.209091             11  \n",
      "8864     27.810985    2.291950e-01     36         0.500000              8  \n",
      "8865     52.796725    1.220498e-01     69         0.208571             13  \n",
      "8866     24.625920    1.341722e-01     46         0.494444              1  \n",
      "8867     29.991153    1.731722e-01     41         0.290000             16  \n",
      "8868     47.089761    2.326071e-05     55         0.183766             11  \n",
      "8869     28.859051    1.226867e-01     50         0.074495              0  \n",
      "8870     34.945319    2.139758e-03     48         0.295833              8  \n",
      "8871     49.020998    1.029604e-01     67         0.423333             16  \n",
      "8872     49.850403    5.742785e-02     69         0.243750             15  \n",
      "8873     66.667671    1.131886e-01     69         0.401948             12  \n",
      "8874     63.188441    5.561056e-02     91         0.330000             11  \n",
      "8875     34.251659    2.096616e-01     59         0.457143              9  \n",
      "8876     35.173461    4.438214e-03     53         0.500000             10  \n",
      "8877     15.679154    5.154410e-01     24         0.350000              0  \n",
      "8878     46.776969    3.470664e-05     52         0.477273             17  \n",
      "8879     70.591846    6.305973e-02     82         0.317054             12  \n",
      "8880    101.178397    1.176473e-03     68         0.645000              7  \n",
      "8881     79.237785    7.453125e-02     64         0.328636              9  \n",
      "8882     12.085912    6.642595e-01     10         0.000000              2  \n",
      "8883     72.602827    7.171725e-02     99         0.364314             21  \n",
      "8884     56.541018    2.690952e-05     65         0.325000             10  \n",
      "8885     47.286833    9.218766e-02     42         0.279167              6  \n",
      "\n",
      "[1805 rows x 21 columns],        essay_id  essay_set                                              essay  \\\n",
      "8886      14834          6  There were many obstacles that the builders fa...   \n",
      "8887      14835          6  Him from the start, there would have been many...   \n",
      "8888      14836          6  The builders of the Empire State Building face...   \n",
      "8889      14837          6  In the passage The Mooring Mast by Marcia Amid...   \n",
      "8890      14838          6  The builders of the Empire State Building face...   \n",
      "8891      14839          6  soon after it's conception, The Empire State B...   \n",
      "8892      14840          6  The builders of the Empire @CAPS1 Building wer...   \n",
      "8893      14841          6  In the excerpt \"The Mooring Mast\" by Marcia Am...   \n",
      "8894      14842          6  The obstacles the builders face were great. Th...   \n",
      "8895      14843          6  The builders of the Empire State Building a gr...   \n",
      "8896      14844          6  The Empire State Building's docking zone was a...   \n",
      "8897      14845          6  During the construction of the Empire State Bu...   \n",
      "8898      14846          6  The builders of the Empire State Building face...   \n",
      "8899      14847          6  The builders of the Empire State Building face...   \n",
      "8900      14848          6  The obstacles the builders of the Empire state...   \n",
      "8901      14849          6  The @CAPS1 The Mooring Mast by Marcia Amidon L...   \n",
      "8902      14850          6  In Marcia Amidon LÃ¼sted's excerpt, The Mooring...   \n",
      "8903      14851          6  The builders of the empire state buildings had...   \n",
      "8904      14852          6  The first obstacle that it faced with having t...   \n",
      "8905      14853          6  The obstacles the builders of the Empire State...   \n",
      "8906      14854          6  The builders of the Empire State Building face...   \n",
      "8907      14855          6  The builders of the @ORGANIZATION1 many obstac...   \n",
      "8908      14856          6  In the reading The Mooring Mast written by Mar...   \n",
      "8909      14857          6  The builders of the Empire State Building face...   \n",
      "8910      14858          6  In building the Empire State Building, the bui...   \n",
      "8911      14859          6  The idea of mooring dirigibles to the top of t...   \n",
      "8912      14860          6  In the excerpt \"The Mooring @CAPS1\", by: Marci...   \n",
      "8913      14861          6  The builders of the Empire State Building face...   \n",
      "8914      14862          6  Based on the excerpt \"The Mooring Mast\" by Mar...   \n",
      "8915      14863          6  The builders of the Empire State Building face...   \n",
      "...         ...        ...                                                ...   \n",
      "10656     16604          6  The builders of the Empire State Building face...   \n",
      "10657     16605          6  In the attempt to allow dirigibles to dock at ...   \n",
      "10658     16606          6  Based on the excerpt \"The Mooring mast\" by Mar...   \n",
      "10659     16607          6  In The \"Mooring Mast\" by @ORGANIZATION1 many p...   \n",
      "10660     16608          6  When Designing the dock for dirigibles on the ...   \n",
      "10661     16609          6  The @CAPS1 says the greatest obstacle would be...   \n",
      "10662     16610          6  The builders faced many problems that should h...   \n",
      "10663     16611          6  After finishing the @ORGANIZATION1, architects...   \n",
      "10664     16612          6  There were many obstacles the builder of the E...   \n",
      "10665     16613          6  While constructing the Empire State Building, ...   \n",
      "10666     16614          6  To design a mast for the dirigibles to land wa...   \n",
      "10667     16615          6  There were many obstacles which the builders o...   \n",
      "10668     16616          6  In the excerpt, \"The Mooring Mast\", by Marcia ...   \n",
      "10669     16617          6  The builders of the Empire State Building face...   \n",
      "10670     16618          6  In the @CAPS1, the docking of dirigibles had m...   \n",
      "10671     16619          6  The builders of the empire state building face...   \n",
      "10672     16620          6  In The Mooring Mast by Marcia Amidon LÃ¼sted yo...   \n",
      "10673     16621          6  The builders and architects of the Empire Stat...   \n",
      "10674     16622          6  An obstacle the builders of the Empire State b...   \n",
      "10675     16623          6  The architects had to go through many obstacle...   \n",
      "10676     16624          6  The @CAPS1 of the Empire State Building faced ...   \n",
      "10677     16625          6  The builders of the Empire State Building were...   \n",
      "10678     16626          6  Dirigibles are large steel-frame balloons encl...   \n",
      "10679     16627          6  The builders of the Empire State building had ...   \n",
      "10680     16628          6  There were many obstacles the builders of the ...   \n",
      "10681     16629          6  The one obstacle the builders had when trying ...   \n",
      "10682     16630          6  Some of the problems with the constructing of ...   \n",
      "10683     16631          6  The builders of the Empire State building face...   \n",
      "10684     16632          6  The obstacles the builders of the Empire State...   \n",
      "10685     16633          6  You want me to tell you what they had to go th...   \n",
      "\n",
      "       domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "8886             5.0         122               6         4.696721   \n",
      "8887             7.5         180               9         4.850000   \n",
      "8888            10.0         166               8         4.656627   \n",
      "8889             2.5         193               7         4.694301   \n",
      "8890             7.5         162              10         4.740741   \n",
      "8891            10.0         168              10         5.184524   \n",
      "8892             7.5         148               6         4.851351   \n",
      "8893             7.5         128               5         5.304688   \n",
      "8894             7.5         114               9         4.842105   \n",
      "8895             7.5         130              12         4.800000   \n",
      "8896             7.5         245              15         4.706122   \n",
      "8897            10.0         312              18         4.993590   \n",
      "8898            10.0         213               8         4.723005   \n",
      "8899             7.5         195              13         4.851282   \n",
      "8900             5.0         135               5         4.577778   \n",
      "8901             7.5         152               7         5.019737   \n",
      "8902             2.5         141               7         4.702128   \n",
      "8903             7.5         130               8         4.538462   \n",
      "8904             7.5          87               3         4.735632   \n",
      "8905             5.0          88               4         5.090909   \n",
      "8906             7.5         114               9         5.078947   \n",
      "8907            10.0         244              11         4.778689   \n",
      "8908             5.0         178              12         4.455056   \n",
      "8909            10.0         233              13         5.038627   \n",
      "8910             7.5         116               6         4.646552   \n",
      "8911             5.0         167              11         4.736527   \n",
      "8912             7.5         212              10         4.863208   \n",
      "8913             7.5         198              11         4.797980   \n",
      "8914             7.5         159               8         4.559748   \n",
      "8915             7.5         109               5         4.788991   \n",
      "...              ...         ...             ...              ...   \n",
      "10656           10.0         177               6         4.378531   \n",
      "10657           10.0         200               9         5.025000   \n",
      "10658            2.5          92               6         4.695652   \n",
      "10659            7.5         173               9         4.745665   \n",
      "10660            7.5         167               6         4.652695   \n",
      "10661            2.5          24               2         4.666667   \n",
      "10662            7.5         118              10         4.881356   \n",
      "10663            7.5         156               9         5.243590   \n",
      "10664            7.5         178              11         4.820225   \n",
      "10665            7.5         221               8         4.909502   \n",
      "10666            2.5         160               6         4.787500   \n",
      "10667            5.0         219              10         4.757991   \n",
      "10668           10.0         183               9         4.754098   \n",
      "10669            7.5         149               8         5.013423   \n",
      "10670            2.5          41               3         4.926829   \n",
      "10671            5.0         151               6         4.940397   \n",
      "10672           10.0         227              12         4.519824   \n",
      "10673            7.5         195               8         4.815385   \n",
      "10674            7.5         128               7         4.726562   \n",
      "10675           10.0         171               9         4.812865   \n",
      "10676            7.5         235              12         4.693617   \n",
      "10677            5.0         139               7         4.446043   \n",
      "10678            5.0         160               9         4.918750   \n",
      "10679            7.5         184               8         4.657609   \n",
      "10680            7.5         130               7         4.438462   \n",
      "10681            0.0         154               8         4.597403   \n",
      "10682            5.0          67               3         4.850746   \n",
      "10683            7.5         105               5         4.790476   \n",
      "10684            5.0          68               2         4.529412   \n",
      "10685            5.0         158               9         4.310127   \n",
      "\n",
      "       num_exclamation_marks  num_question_marks  num_stopwords  \\\n",
      "8886                       0                   0             56   \n",
      "8887                       0                   0             87   \n",
      "8888                       0                   0             73   \n",
      "8889                       0                   0             81   \n",
      "8890                       0                   0             65   \n",
      "8891                       0                   0             71   \n",
      "8892                       0                   0             71   \n",
      "8893                       0                   0             53   \n",
      "8894                       0                   0             48   \n",
      "8895                       1                   0             54   \n",
      "8896                       0                   0            111   \n",
      "8897                       0                   0            131   \n",
      "8898                       0                   0            101   \n",
      "8899                       0                   0             87   \n",
      "8900                       0                   0             64   \n",
      "8901                       0                   0             58   \n",
      "8902                       0                   0             65   \n",
      "8903                       0                   0             55   \n",
      "8904                       0                   0             43   \n",
      "8905                       0                   0             41   \n",
      "8906                       0                   0             48   \n",
      "8907                       0                   0            116   \n",
      "8908                       0                   0             81   \n",
      "8909                       1                   0            100   \n",
      "8910                       0                   0             56   \n",
      "8911                       0                   0             78   \n",
      "8912                       0                   0             84   \n",
      "8913                       0                   0             79   \n",
      "8914                       0                   0             74   \n",
      "8915                       0                   0             48   \n",
      "...                      ...                 ...            ...   \n",
      "10656                      0                   0             86   \n",
      "10657                      0                   0             84   \n",
      "10658                      0                   0             39   \n",
      "10659                      0                   0             76   \n",
      "10660                      0                   0             90   \n",
      "10661                      0                   0             10   \n",
      "10662                      0                   0             48   \n",
      "10663                      0                   0             67   \n",
      "10664                      0                   0             85   \n",
      "10665                      0                   0            101   \n",
      "10666                      0                   0             74   \n",
      "10667                      0                   0             98   \n",
      "10668                      0                   0             79   \n",
      "10669                      0                   0             60   \n",
      "10670                      0                   0             17   \n",
      "10671                      0                   0             62   \n",
      "10672                      0                   0             98   \n",
      "10673                      0                   0             86   \n",
      "10674                      0                   0             53   \n",
      "10675                      0                   0             70   \n",
      "10676                      0                   0            102   \n",
      "10677                      0                   0             80   \n",
      "10678                      0                   0             62   \n",
      "10679                      0                   0             87   \n",
      "10680                      0                   0             58   \n",
      "10681                      0                   0             78   \n",
      "10682                      0                   0             25   \n",
      "10683                      0                   0             49   \n",
      "10684                      0                   0             31   \n",
      "10685                      3                   1             77   \n",
      "\n",
      "           ...        verb_count  foreign_count  adj_count  conj_count  \\\n",
      "8886       ...                16              0          7          21   \n",
      "8887       ...                26              0         11          34   \n",
      "8888       ...                24              0         10          19   \n",
      "8889       ...                28              0          6          32   \n",
      "8890       ...                27              0         16          20   \n",
      "8891       ...                23              0          9          25   \n",
      "8892       ...                22              0          4          27   \n",
      "8893       ...                23              0          6          20   \n",
      "8894       ...                22              0         10          14   \n",
      "8895       ...                17              0         10          14   \n",
      "8896       ...                46              0         15          26   \n",
      "8897       ...                41              0         21          42   \n",
      "8898       ...                33              0         13          33   \n",
      "8899       ...                31              0          9          29   \n",
      "8900       ...                18              0          7          28   \n",
      "8901       ...                22              0          5          19   \n",
      "8902       ...                18              0          8          17   \n",
      "8903       ...                17              0         13          16   \n",
      "8904       ...                13              0          8          15   \n",
      "8905       ...                16              0          2           9   \n",
      "8906       ...                16              0          6          15   \n",
      "8907       ...                33              0         17          30   \n",
      "8908       ...                25              0          7          23   \n",
      "8909       ...                36              0         17          29   \n",
      "8910       ...                13              0          7          21   \n",
      "8911       ...                27              0         12          24   \n",
      "8912       ...                28              0         20          31   \n",
      "8913       ...                29              0         16          27   \n",
      "8914       ...                26              0          8          20   \n",
      "8915       ...                22              0          5          16   \n",
      "...        ...               ...            ...        ...         ...   \n",
      "10656      ...                25              0         17          31   \n",
      "10657      ...                27              0         18          29   \n",
      "10658      ...                16              0          6          13   \n",
      "10659      ...                21              0         14          29   \n",
      "10660      ...                27              0          5          33   \n",
      "10661      ...                 2              0          2           2   \n",
      "10662      ...                18              0         12          17   \n",
      "10663      ...                23              0         12          20   \n",
      "10664      ...                26              0         10          30   \n",
      "10665      ...                34              0         16          36   \n",
      "10666      ...                21              0          7          24   \n",
      "10667      ...                39              0         11          30   \n",
      "10668      ...                26              0         10          25   \n",
      "10669      ...                21              0         10          23   \n",
      "10670      ...                 6              0          5           6   \n",
      "10671      ...                19              0         12          24   \n",
      "10672      ...                39              0          9          31   \n",
      "10673      ...                21              0         17          30   \n",
      "10674      ...                18              0          6          16   \n",
      "10675      ...                24              0         13          27   \n",
      "10676      ...                32              0         18          33   \n",
      "10677      ...                22              0         11          25   \n",
      "10678      ...                22              0         12          25   \n",
      "10679      ...                24              0         12          29   \n",
      "10680      ...                13              0         11          22   \n",
      "10681      ...                23              0          4          25   \n",
      "10682      ...                14              0          6          11   \n",
      "10683      ...                13              0          6          16   \n",
      "10684      ...                 8              0          3           8   \n",
      "10685      ...                27              0          7          16   \n",
      "\n",
      "       adv_count  beauty_score  maturity_score  vocab  sentiment_essay  \\\n",
      "8886          12     48.680765    1.034712e-01     65         0.002350   \n",
      "8887           9     64.126150    6.356227e-02    105         0.042924   \n",
      "8888          14     60.482435    7.001046e-02     94         0.013798   \n",
      "8889           4    101.741032    5.529307e-04     92         0.097727   \n",
      "8890          11     55.101360    5.869619e-02     91         0.081068   \n",
      "8891          10     63.356230    7.698933e-02     93         0.101042   \n",
      "8892           5     55.904830    7.809537e-02     84         0.244215   \n",
      "8893          10     59.787501    6.917949e-02     70         0.031250   \n",
      "8894           3     47.568914    7.869597e-02     68         0.143571   \n",
      "8895          13     52.019444    1.177383e-01     77         0.174359   \n",
      "8896          22    100.788080    6.656005e-02    125         0.075758   \n",
      "8897          27    139.582512    4.211886e-02    170        -0.075835   \n",
      "8898           8     91.698710    2.918908e-12    118         0.211691   \n",
      "8899           6     82.577581    6.380607e-02    100         0.225758   \n",
      "8900           5     55.892294    1.337313e-01     67         0.185101   \n",
      "8901           7     67.497140    1.008434e-01     83         0.277037   \n",
      "8902           6     61.545622    9.625000e-02     64         0.175649   \n",
      "8903          17     44.461125    1.167862e-01     69         0.038095   \n",
      "8904           6     28.552060    7.627601e-02     51        -0.033167   \n",
      "8905           3     34.609443    1.766513e-01     50         0.121591   \n",
      "8906           3     40.636749    1.009231e-01     65         0.106108   \n",
      "8907          14     96.327113    1.984303e-08    133         0.106725   \n",
      "8908          21     74.468679    6.961404e-02     96         0.188102   \n",
      "8909          12     87.705971    4.608836e-02    144         0.160414   \n",
      "8910           9     43.315829    3.690550e-05     60        -0.010606   \n",
      "8911          13     73.524734    7.976481e-02     85         0.235191   \n",
      "8912          12     71.746188    6.911593e-02    114         0.056605   \n",
      "8913          14     71.347612    7.278744e-02    115        -0.004210   \n",
      "8914          14     63.112946    9.903628e-02     83         0.140879   \n",
      "8915           2     39.799932    1.830634e-01     59         0.219048   \n",
      "...          ...           ...             ...    ...              ...   \n",
      "10656         17     93.849072    9.545455e-02     99        -0.058529   \n",
      "10657         16     86.383223    6.136474e-02    108         0.064414   \n",
      "10658          2     36.048005    8.254111e-02     47         0.102500   \n",
      "10659          4     69.205498    4.592392e-02     98         0.190762   \n",
      "10660          9     66.035862    7.609989e-02     82         0.075000   \n",
      "10661          0     10.440304    4.697530e-01     14         0.250000   \n",
      "10662          7     43.939043    6.810411e-02     71         0.276643   \n",
      "10663         17     54.268206    7.337979e-02     85         0.134715   \n",
      "10664          9     60.206882    6.487717e-02     96         0.296154   \n",
      "10665         16     82.343117    5.789762e-02    136         0.085144   \n",
      "10666          5     73.207516    5.322581e-02     93         0.151042   \n",
      "10667         14    122.238455    4.171385e-02    117         0.158471   \n",
      "10668         11     64.269709    7.084746e-02     88        -0.019948   \n",
      "10669          8     52.440822    7.426462e-02     84        -0.081765   \n",
      "10670          4     28.124935    1.766756e-02     21         0.073889   \n",
      "10671         13     51.863905    1.395833e-01     72         0.134846   \n",
      "10672         18     82.595492    4.357502e-02    112         0.039412   \n",
      "10673         11     78.273852    9.208696e-02    115         0.117459   \n",
      "10674          4     47.207523    1.658609e-01     65         0.185357   \n",
      "10675          6     73.198171    5.431043e-02     96         0.067311   \n",
      "10676         20     93.422963    5.654765e-02    134         0.017288   \n",
      "10677         16     50.563674    1.855113e-07     66         0.194823   \n",
      "10678          7     60.479094    8.111116e-02     77         0.148985   \n",
      "10679         10     79.766896    1.204211e-01     95         0.055942   \n",
      "10680          8     46.809779    9.071025e-02     65         0.141667   \n",
      "10681         15     62.464780    7.870152e-02     77         0.087786   \n",
      "10682          4     24.053274    1.050821e-01     41         0.144375   \n",
      "10683          8     37.487847    2.076963e-01     55        -0.025385   \n",
      "10684          6     21.562408    9.069529e-02     36        -0.133333   \n",
      "10685          8     51.691081    6.446077e-02     83         0.143519   \n",
      "\n",
      "       Grammar_check  \n",
      "8886               2  \n",
      "8887               0  \n",
      "8888               1  \n",
      "8889              12  \n",
      "8890               3  \n",
      "8891               4  \n",
      "8892               3  \n",
      "8893              13  \n",
      "8894               5  \n",
      "8895               1  \n",
      "8896               3  \n",
      "8897              13  \n",
      "8898               5  \n",
      "8899              13  \n",
      "8900               1  \n",
      "8901              16  \n",
      "8902               2  \n",
      "8903               8  \n",
      "8904               4  \n",
      "8905               1  \n",
      "8906               4  \n",
      "8907               8  \n",
      "8908               8  \n",
      "8909               8  \n",
      "8910               5  \n",
      "8911              10  \n",
      "8912               6  \n",
      "8913               8  \n",
      "8914              11  \n",
      "8915               1  \n",
      "...              ...  \n",
      "10656              5  \n",
      "10657             12  \n",
      "10658             12  \n",
      "10659              5  \n",
      "10660             10  \n",
      "10661              0  \n",
      "10662              4  \n",
      "10663              1  \n",
      "10664             11  \n",
      "10665              5  \n",
      "10666              6  \n",
      "10667             14  \n",
      "10668             12  \n",
      "10669              5  \n",
      "10670              0  \n",
      "10671              9  \n",
      "10672             12  \n",
      "10673              0  \n",
      "10674              5  \n",
      "10675              7  \n",
      "10676              6  \n",
      "10677              1  \n",
      "10678              9  \n",
      "10679              3  \n",
      "10680              7  \n",
      "10681             13  \n",
      "10682              2  \n",
      "10683              7  \n",
      "10684              1  \n",
      "10685             14  \n",
      "\n",
      "[1800 rows x 21 columns],        essay_id  essay_set                                              essay  \\\n",
      "10686     17834          7  Patience is when your waiting .I was patience ...   \n",
      "10687     17836          7  I am not a patience person, like I canâ€™t sit i...   \n",
      "10688     17837          7  One day I was at basketball practice and I was...   \n",
      "10689     17838          7  I going to write about a time when I went to t...   \n",
      "10690     17839          7  It can be very hard for somebody to be patient...   \n",
      "10691     17840          7  There was a girl name @PERSON1. She loved spen...   \n",
      "10692     17841          7  Un Patience @CAPS1.   My name is @CAPS2 and I ...   \n",
      "10693     17842          7  A time when I was patient was when I preordere...   \n",
      "10694     17843          7  One time I was patience it was when I wanted a...   \n",
      "10695     17844          7  I think patience is a time when you have to be...   \n",
      "10696     17845          7  You know that life is so much harder when you ...   \n",
      "10697     17846          7  One nice sunny day I was traped in a doctors o...   \n",
      "10698     17847          7  A time I was patient was @DATE1, when I was in...   \n",
      "10699     17849          7  One day, my soccer team was in the chamionchip...   \n",
      "10700     17850          7  This is about a story I was patient I was on a...   \n",
      "10701     17851          7  Tick, tock, tick, tock. Being patient is hard ...   \n",
      "10702     17852          7  One day @CAPS1 went to school pretty earlie in...   \n",
      "10703     17853          7  I recall once a famous madican named @PERSON1 ...   \n",
      "10704     17854          7  One day, a few years ago, I woke up and my mom...   \n",
      "10705     17856          7  The time that I was patient was not long ago. ...   \n",
      "10706     17857          7  One day I was patient was when I tried out for...   \n",
      "10707     17858          7  A time that I was patient was last year at che...   \n",
      "10708     17859          7  Im writing about the time I was patient at a @...   \n",
      "10709     17860          7  Being patient? Being patient is very hard for ...   \n",
      "10710     17861          7  One when I was patient was when we were going ...   \n",
      "10711     17862          7  Patients is verey important. But I m not verey...   \n",
      "10712     17863          7  Patience is when you can take your time at som...   \n",
      "10713     17864          7  When I was patient. Was when I go down the riv...   \n",
      "10714     17865          7  Patience? I am not a patient type of person. I...   \n",
      "10715     17867          7  I am not a patient person at all. But sometime...   \n",
      "...         ...        ...                                                ...   \n",
      "12225     19529          7  â€œ@CAPS1!â€ @CAPS2 whined, her voice escalating ...   \n",
      "12226     19530          7  I was patient about getting my phone. I had to...   \n",
      "12227     19533          7  A time when I was patient I was patient on @CA...   \n",
      "12228     19534          7  One time me and my family were at @CAPS1 point...   \n",
      "12229     19535          7  I was patient once when my dad had said he was...   \n",
      "12230     19536          7  I was patient one time, really patient but kin...   \n",
      "12231     19537          7  Once my friend @PERSON1 was very patient,he wa...   \n",
      "12232     19538          7  The patient player. Once when I was four and m...   \n",
      "12233     19539          7  When I had to go get my check up with my asma ...   \n",
      "12234     19540          7  When my mom, @PERSON1 and I went to @ORGANIZAT...   \n",
      "12235     19541          7  A time when I was patient was when @DATE1 we d...   \n",
      "12236     19542          7  Not long ago I went to the pumpkin patch with ...   \n",
      "12237     19543          7  Will I was patiently waiting for my little sis...   \n",
      "12238     19544          7  At the @CAPS1â€™s office. I had an @CAPS2 appoin...   \n",
      "12239     19545          7  Patient people are the people that tolerate li...   \n",
      "12240     19546          7  In the @DATE1 time my mom had planned of trip ...   \n",
      "12241     19547          7  A time I was patient was when I was at a denti...   \n",
      "12242     19548          7  On a fine @DATE1 day I was heading toward the ...   \n",
      "12243     19549          7  When I was and the @NUM1 grade and I had to pu...   \n",
      "12244     19550          7  My friend an I were wateng to go no a new ride...   \n",
      "12245     19553          7  One early @TIME1 in @LOCATION2, @LOCATION1 for...   \n",
      "12246     19554          7  @CAPS1, @CAPS1 is a huge past of life to be ??...   \n",
      "12247     19555          7  To say really Iâ€™m not a very patient person, b...   \n",
      "12248     19556          7  One day, my mom was really patient. We were ou...   \n",
      "12249     19557          7  Awhile back my friends and I made a @CAPS2 gam...   \n",
      "12250     19558          7  One time I was getting a cool @CAPS1 game it w...   \n",
      "12251     19559          7  A patent person in my life is my mom. Aicason ...   \n",
      "12252     19561          7  A time when someone else I know was patient wa...   \n",
      "12253     19562          7  I hate weddings. I love when people get marrie...   \n",
      "12254     19563          7  A few weeks ago, we had a garage sale and a mo...   \n",
      "\n",
      "       domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "10686            6.0          91               3         4.373626   \n",
      "10687            5.2          95               3         3.842105   \n",
      "10688            6.0         159               1         3.849057   \n",
      "10689            6.8         217              13         4.184332   \n",
      "10690            5.2         151              12         4.364238   \n",
      "10691            9.2         337              33         4.670623   \n",
      "10692            6.4         172              11         3.738372   \n",
      "10693            7.2         126               7         4.325397   \n",
      "10694            4.8         365               6         3.778082   \n",
      "10695            4.0          88               9         3.897727   \n",
      "10696            6.4         176               9         3.721591   \n",
      "10697            7.6         116               9         4.215517   \n",
      "10698            6.8         188              11         3.856383   \n",
      "10699            5.6         165              10         3.769697   \n",
      "10700            4.8         166               4         4.048193   \n",
      "10701            6.4         170              13         3.876471   \n",
      "10702            6.8         227              17         4.132159   \n",
      "10703            6.4          93               8         4.537634   \n",
      "10704            8.4         184              12         3.804348   \n",
      "10705            5.6         139              12         3.661871   \n",
      "10706            7.2         154               9         3.844156   \n",
      "10707            9.6         196              16         3.943878   \n",
      "10708            6.8         186              11         3.930108   \n",
      "10709            6.4         188              13         4.276596   \n",
      "10710            6.4         141               5         3.666667   \n",
      "10711            3.6         103               9         4.048544   \n",
      "10712            2.4          52               4         4.326923   \n",
      "10713            4.0         105               4         3.628571   \n",
      "10714            6.8         270              15         4.411111   \n",
      "10715            6.4         174              12         3.666667   \n",
      "...              ...         ...             ...              ...   \n",
      "12225            9.6         366              27         4.844262   \n",
      "12226            9.6         398              19         3.648241   \n",
      "12227            5.6         115              10         3.817391   \n",
      "12228            7.2         187              10         3.791444   \n",
      "12229            6.4         137               5         3.781022   \n",
      "12230            6.4         134               5         3.656716   \n",
      "12231            5.2          34               2         4.588235   \n",
      "12232            5.6         120               4         4.016667   \n",
      "12233            5.6         243               7         3.765432   \n",
      "12234            6.4         204              14         3.906863   \n",
      "12235            5.2         166              11         3.674699   \n",
      "12236            6.0          86               5         3.860465   \n",
      "12237            6.4         122               6         3.729508   \n",
      "12238            3.6         101              10         4.217822   \n",
      "12239            6.0         110               7         4.363636   \n",
      "12240            6.4         101               6         3.693069   \n",
      "12241            7.2         299              15         3.528428   \n",
      "12242            7.2         130              12         3.915385   \n",
      "12243            3.2         122               2         3.262295   \n",
      "12244            3.2          50               6         3.560000   \n",
      "12245            7.6         157              13         3.936306   \n",
      "12246            5.6         176              11         3.909091   \n",
      "12247            7.2         154              14         4.058442   \n",
      "12248            7.6         346              34         3.832370   \n",
      "12249            9.6         299              25         4.123746   \n",
      "12250            4.8          72               6         3.444444   \n",
      "12251            6.4         223               9         4.260090   \n",
      "12252            7.6         166              13         4.222892   \n",
      "12253            8.8         297              30         4.360269   \n",
      "12254            6.0         167               9         3.431138   \n",
      "\n",
      "       num_exclamation_marks  num_question_marks  num_stopwords  \\\n",
      "10686                      0                   0             42   \n",
      "10687                      0                   0             48   \n",
      "10688                      2                   0             77   \n",
      "10689                      0                   0            109   \n",
      "10690                      0                   0             81   \n",
      "10691                      0                   2            130   \n",
      "10692                      0                   0             97   \n",
      "10693                      0                   0             66   \n",
      "10694                      0                   0            189   \n",
      "10695                      1                   0             49   \n",
      "10696                      1                   1            101   \n",
      "10697                      0                   0             53   \n",
      "10698                      0                   0            102   \n",
      "10699                      0                   0             89   \n",
      "10700                      0                   0             85   \n",
      "10701                      0                   3             81   \n",
      "10702                      0                   2            113   \n",
      "10703                      1                   0             40   \n",
      "10704                      0                   0             93   \n",
      "10705                      0                   0             83   \n",
      "10706                      1                   0             90   \n",
      "10707                      1                   0            108   \n",
      "10708                      0                   0            100   \n",
      "10709                     12                   2            103   \n",
      "10710                      0                   0             85   \n",
      "10711                      0                   0             56   \n",
      "10712                      3                   0             27   \n",
      "10713                      0                   0             60   \n",
      "10714                      0                   1            117   \n",
      "10715                      2                   0             99   \n",
      "...                      ...                 ...            ...   \n",
      "12225                      9                   4            136   \n",
      "12226                      0                   0            221   \n",
      "12227                      0                   0             56   \n",
      "12228                      0                   0             97   \n",
      "12229                      0                   0             82   \n",
      "12230                      0                   0             70   \n",
      "12231                      0                   0             12   \n",
      "12232                      0                   0             71   \n",
      "12233                      0                   0            141   \n",
      "12234                      2                   0             98   \n",
      "12235                      2                   0            101   \n",
      "12236                      0                   0             42   \n",
      "12237                      0                   0             65   \n",
      "12238                      0                   0             46   \n",
      "12239                      4                   0             48   \n",
      "12240                      0                   2             57   \n",
      "12241                      0                   0            170   \n",
      "12242                      0                   0             66   \n",
      "12243                      0                   0             75   \n",
      "12244                      0                   0             27   \n",
      "12245                      0                   0             85   \n",
      "12246                      1                   8             96   \n",
      "12247                      2                   0             80   \n",
      "12248                      0                   1            182   \n",
      "12249                      2                   1            147   \n",
      "12250                      0                   0             35   \n",
      "12251                      0                   0            106   \n",
      "12252                      0                   0             87   \n",
      "12253                      1                   8            131   \n",
      "12254                      3                   0             96   \n",
      "\n",
      "           ...        verb_count  foreign_count  adj_count  conj_count  \\\n",
      "10686      ...                14              0          0          14   \n",
      "10687      ...                11              0          0          17   \n",
      "10688      ...                33              0          2          23   \n",
      "10689      ...                38              0          4          34   \n",
      "10690      ...                26              0          2          18   \n",
      "10691      ...                60              0         10          45   \n",
      "10692      ...                26              0          3          20   \n",
      "10693      ...                21              0          2          18   \n",
      "10694      ...                57              0         10          44   \n",
      "10695      ...                19              0          3          13   \n",
      "10696      ...                28              0          7          24   \n",
      "10697      ...                20              0          6          19   \n",
      "10698      ...                27              0         10          35   \n",
      "10699      ...                27              0          3          29   \n",
      "10700      ...                22              0          5          31   \n",
      "10701      ...                30              0          2          29   \n",
      "10702      ...                41              0          5          40   \n",
      "10703      ...                14              0          8          17   \n",
      "10704      ...                42              0          5          20   \n",
      "10705      ...                27              0          1          19   \n",
      "10706      ...                27              0          5          26   \n",
      "10707      ...                39              0          7          26   \n",
      "10708      ...                30              0          7          28   \n",
      "10709      ...                32              0         10          29   \n",
      "10710      ...                38              0          2          17   \n",
      "10711      ...                14              0          2          10   \n",
      "10712      ...                10              0          2           5   \n",
      "10713      ...                13              0          1          15   \n",
      "10714      ...                44              0          7          27   \n",
      "10715      ...                21              0          5          25   \n",
      "...        ...               ...            ...        ...         ...   \n",
      "12225      ...                58              0          9          40   \n",
      "12226      ...                74              0         16          48   \n",
      "12227      ...                18              0          2          11   \n",
      "12228      ...                24              0          4          32   \n",
      "12229      ...                28              0          1          16   \n",
      "12230      ...                22              0          1          19   \n",
      "12231      ...                 4              0          0           4   \n",
      "12232      ...                22              0          0          16   \n",
      "12233      ...                48              0         10          41   \n",
      "12234      ...                27              0         10          21   \n",
      "12235      ...                34              0          1          25   \n",
      "12236      ...                 8              0          1           9   \n",
      "12237      ...                21              0          7          11   \n",
      "12238      ...                15              0          2          10   \n",
      "12239      ...                16              0          3          16   \n",
      "12240      ...                19              0          0          13   \n",
      "12241      ...                58              0          4          41   \n",
      "12242      ...                28              0          2          23   \n",
      "12243      ...                17              0          3          27   \n",
      "12244      ...                 8              0          1           7   \n",
      "12245      ...                33              0          2          23   \n",
      "12246      ...                28              0          6          26   \n",
      "12247      ...                23              0          1          18   \n",
      "12248      ...                71              0          5          46   \n",
      "12249      ...                57              0         12          41   \n",
      "12250      ...                15              0          2           9   \n",
      "12251      ...                26              0         10          37   \n",
      "12252      ...                29              0          5          32   \n",
      "12253      ...                43              0         13          32   \n",
      "12254      ...                24              0          3          26   \n",
      "\n",
      "       adv_count  beauty_score  maturity_score  vocab  sentiment_essay  \\\n",
      "10686          3     50.579343    1.583935e-01     49         0.000000   \n",
      "10687          8     62.965046    1.294595e-01     37         0.197143   \n",
      "10688         10     64.505477    6.136390e-02     66         0.056528   \n",
      "10689         18    139.812677    4.324539e-02    108         0.019286   \n",
      "10690         21     93.040094    1.096232e-05     75        -0.162917   \n",
      "10691         29    152.511583    3.585999e-02    162        -0.006200   \n",
      "10692         20     87.045305    1.749530e-05     76         0.090000   \n",
      "10693          5     46.608007    8.581176e-02     51         0.002083   \n",
      "10694         22    174.315319    2.357705e-02    165         0.155844   \n",
      "10695          4     33.877945    2.064202e-01     35         0.033838   \n",
      "10696         18     63.821251    7.846151e-02     64         0.155714   \n",
      "10697          4     45.737023    1.424421e-03     49        -0.078788   \n",
      "10698         12     84.605792    2.768602e-09     70         0.443031   \n",
      "10699          9    118.427847    9.508368e-04     63        -0.288021   \n",
      "10700         17     74.121089    3.319157e-11     75         0.047608   \n",
      "10701          5     57.714929    1.021742e-01     69        -0.008690   \n",
      "10702         10     95.685533    7.837302e-04     84        -0.063889   \n",
      "10703          5     29.901725    1.420626e-01     36         0.276944   \n",
      "10704         17     93.144282    9.299858e-04     63         0.018750   \n",
      "10705          6     45.889845    1.075613e-01     48        -0.047222   \n",
      "10706         10     51.758597    1.298316e-01     55         0.027778   \n",
      "10707         13     86.057030    8.408269e-02     94         0.187930   \n",
      "10708         14     90.359028    7.271230e-02     80         0.150000   \n",
      "10709         11     76.225241    1.088089e-03     98        -0.336081   \n",
      "10710          9     52.402587    1.175005e-01     60         0.225000   \n",
      "10711          9     60.999811    1.668216e-01     43         0.360000   \n",
      "10712          2     19.073175    1.725906e-01     30         0.285313   \n",
      "10713          7     69.085228    3.901049e-03     36        -0.152778   \n",
      "10714         15    108.814793    5.321895e-14     97         0.156296   \n",
      "10715         13     82.371828    2.104892e-07     78         0.240972   \n",
      "...          ...           ...             ...    ...              ...   \n",
      "12225         19    180.785511    1.166461e-06    154         0.129269   \n",
      "12226         23    166.562887    5.294943e-13    145         0.140857   \n",
      "12227          3     40.785157    3.742404e-03     36         0.141667   \n",
      "12228         16     83.403432    4.658228e-02     79         0.064583   \n",
      "12229         12     63.760777    1.599228e-03     57        -0.072917   \n",
      "12230          9     74.128450    9.075000e-02     40         0.227273   \n",
      "12231          3     40.769762    4.839078e-01     13        -0.066667   \n",
      "12232         21    113.190326    1.307908e-03     56        -0.062500   \n",
      "12233         19    160.117198    6.591512e-02    117         0.328986   \n",
      "12234         16    112.420895    4.980748e-02     74        -0.058890   \n",
      "12235         15     71.214585    1.152935e-01     62         0.030324   \n",
      "12236          7     62.885490    2.764905e-03     40        -0.010417   \n",
      "12237         12     65.218493    7.695228e-02     52        -0.161574   \n",
      "12238          9     62.908075    2.368423e-03     38         0.171429   \n",
      "12239         13     58.232091    1.287476e-01     45        -0.035565   \n",
      "12240         12     51.891473    3.502117e-06     34         0.024074   \n",
      "12241         15    108.771832    4.148636e-04    120        -0.141667   \n",
      "12242         10     63.166397    1.131335e-01     52         0.166567   \n",
      "12243          8     37.730298    8.550494e-02     37         0.000000   \n",
      "12244          0     16.966930    5.437883e-08     13        -0.122727   \n",
      "12245          8     57.226232    1.302270e-03     63         0.162500   \n",
      "12246         12     73.922342    8.407391e-06     76         0.199510   \n",
      "12247         22     85.726701    7.468386e-04     81         0.156389   \n",
      "12248         28    132.563591    4.895833e-02    144        -0.001923   \n",
      "12249         27    169.795368    1.228207e-08    140         0.049862   \n",
      "12250          1     29.430869    3.917322e-01     18        -0.210417   \n",
      "12251         17    102.941198    3.879544e-02    116         0.075383   \n",
      "12252         15    152.998467    7.666685e-02     75         0.147837   \n",
      "12253         31    231.414607    4.689655e-02    116        -0.016916   \n",
      "12254          9     79.032976    2.497254e-07     62         0.317188   \n",
      "\n",
      "       Grammar_check  \n",
      "10686             21  \n",
      "10687             11  \n",
      "10688             17  \n",
      "10689              6  \n",
      "10690              3  \n",
      "10691              2  \n",
      "10692              7  \n",
      "10693              7  \n",
      "10694             43  \n",
      "10695              2  \n",
      "10696              4  \n",
      "10697              8  \n",
      "10698             12  \n",
      "10699              2  \n",
      "10700              8  \n",
      "10701             11  \n",
      "10702             15  \n",
      "10703             13  \n",
      "10704              4  \n",
      "10705              5  \n",
      "10706              2  \n",
      "10707              5  \n",
      "10708              7  \n",
      "10709              8  \n",
      "10710              3  \n",
      "10711             12  \n",
      "10712              8  \n",
      "10713              9  \n",
      "10714             16  \n",
      "10715              3  \n",
      "...              ...  \n",
      "12225              4  \n",
      "12226              6  \n",
      "12227              7  \n",
      "12228              2  \n",
      "12229              3  \n",
      "12230              7  \n",
      "12231              4  \n",
      "12232              3  \n",
      "12233             12  \n",
      "12234              2  \n",
      "12235              7  \n",
      "12236              3  \n",
      "12237              4  \n",
      "12238              7  \n",
      "12239              4  \n",
      "12240              2  \n",
      "12241             25  \n",
      "12242              2  \n",
      "12243             15  \n",
      "12244              7  \n",
      "12245              4  \n",
      "12246             11  \n",
      "12247              3  \n",
      "12248              4  \n",
      "12249             11  \n",
      "12250              8  \n",
      "12251              8  \n",
      "12252              5  \n",
      "12253              9  \n",
      "12254              2  \n",
      "\n",
      "[1569 rows x 21 columns],        essay_id  essay_set                                              essay  \\\n",
      "12255     20716          8   A long time ago when I was in third grade I h...   \n",
      "12256     20717          8   Softball has to be one of the single most gre...   \n",
      "12257     20718          8   Some people like making people laugh, I love ...   \n",
      "12258     20719          8   \"LAUGHTER\"  @CAPS1 I hang out with my friends...   \n",
      "12259     20721          8  Well ima tell a story about the time i got @CA...   \n",
      "12260     20722          8  @CAPS3 I'm going to tell @CAPS18 a little stor...   \n",
      "12261     20723          8   My best friend, @PERSON1 and I have been clos...   \n",
      "12262     20724          8    Laughter is a really good thing to have in a...   \n",
      "12263     20725          8   I believe that with all people laughter, and ...   \n",
      "12264     20726          8   Laughter, one of life's greatest gifts. Have ...   \n",
      "12265     20727          8   Laugher Laughter is to express delight, fun, ...   \n",
      "12266     20728          8   Starting a story out with two @NUM1 @DATE1 ol...   \n",
      "12267     20732          8  When i meet new people its like alright cause ...   \n",
      "12268     20733          8   People always say laughter is a big part in a...   \n",
      "12269     20734          8      \"Laughter is the best medicine.\" The phras...   \n",
      "12270     20735          8   one time when i was skateboarding with my fri...   \n",
      "12271     20736          8                 \"Laughter\" A good relationship ...   \n",
      "12272     20737          8   Laughter. A story about laughter is what you ...   \n",
      "12273     20739          8   Laughter is a big part in my relationship. I ...   \n",
      "12274     20741          8   Everyone enjoys laughter. Everywhere you look...   \n",
      "12275     20742          8   Why is it that laughter can bring people so c...   \n",
      "12276     20743          8   \"@CAPS1 figure something out to do! I am real...   \n",
      "12277     20745          8  Interesting fly.It was the @DATE1 of this @DAT...   \n",
      "12278     20746          8   In many relationships I have had, there has a...   \n",
      "12279     20747          8   These days you don't really need a reason to ...   \n",
      "12280     20748          8  Friends are'nt always what we want them to be ...   \n",
      "12281     20751          8    It was a normal bright and warm @DATE1 day, ...   \n",
      "12282     20754          8  Making someone laugh is the best feeling to th...   \n",
      "12283     20755          8   \"Laughter is the key to the soul.\" @CAPS1 you...   \n",
      "12284     20756          8   Their are many joys in life, some are more re...   \n",
      "...         ...        ...                                                ...   \n",
      "12948     21592          8   We all understand the benefits of laughter. L...   \n",
      "12949     21594          8        It was midsummer, and i could feel the c...   \n",
      "12950     21595          8   Have you ever experienced a time with your fr...   \n",
      "12951     21596          8   I woke up just like any other day happy yet l...   \n",
      "12952     21598          8   Laughter is an important part of my life, eit...   \n",
      "12953     21599          8   I sat at the table, speechless, as they told ...   \n",
      "12954     21601          8   As I remember back, it was @DATE1. It was a h...   \n",
      "12955     21603          8   Those eyes, it was like I was looking out int...   \n",
      "12956     21604          8  Some say that laugh is the common language bet...   \n",
      "12957     21605          8   Laughter is an integral element to many situa...   \n",
      "12958     21606          8  One time I was at my friend @PERSON1's house, ...   \n",
      "12959     21607          8   LAUGHTER @CAPS1 knows that laughter is a heal...   \n",
      "12960     21608          8  One thing that people in the world love to do ...   \n",
      "12961     21609          8   Laughter, to me, is an important aspect of my...   \n",
      "12962     21610          8   People always say that the worst parts of lif...   \n",
      "12963     21611          8   Why is it that people can look back at someth...   \n",
      "12964     21613          8   Before my best friend moved away, we would st...   \n",
      "12965     21615          8                                @ORGANIZATION1  ...   \n",
      "12966     21617          8   Morose and somnolent, I woke up. I woke up to...   \n",
      "12967     21618          8   A while back my mom had decided to send me to...   \n",
      "12968     21619          8                              I dont like computers   \n",
      "12969     21620          8   Everyone knows how important a laugh can be. ...   \n",
      "12970     21621          8   Laughter is an important part of my family. W...   \n",
      "12971     21623          8   laughter is an important part of any kind of ...   \n",
      "12972     21624          8  Sometime ago on a hot @DATE1 day my @NUM1 ,@PE...   \n",
      "12973     21626          8   In most stories mothers and daughters are eit...   \n",
      "12974     21628          8   I never understood the meaning laughter is th...   \n",
      "12975     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
      "12976     21630          8                                 Trippin' on fen...   \n",
      "12977     21633          8   Many people believe that laughter can improve...   \n",
      "\n",
      "       domain1_score  word_count  sentence_count  avg_word_length  \\\n",
      "12255       5.666667         681              39         4.024963   \n",
      "12256       7.666667         784              30         4.056122   \n",
      "12257       6.666667         851              39         4.168038   \n",
      "12258       5.000000         724              30         3.718232   \n",
      "12259       4.333333         645              26         3.564341   \n",
      "12260       6.666667         706              72         4.739377   \n",
      "12261       6.666667         825              82         4.369697   \n",
      "12262       5.666667         293              17         3.935154   \n",
      "12263       6.833333         737              48         4.438263   \n",
      "12264       6.666667         537              40         4.601490   \n",
      "12265       5.166667         445              18         4.065169   \n",
      "12266       7.333333         818              37         4.826406   \n",
      "12267       5.166667         815              17         3.673620   \n",
      "12268       6.000000         562              31         4.275801   \n",
      "12269       7.166667         783              54         4.735632   \n",
      "12270       3.333333         135               4         4.407407   \n",
      "12271       7.500000         730              41         4.238356   \n",
      "12272       6.666667         529              38         4.096408   \n",
      "12273       5.000000         737              52         3.932157   \n",
      "12274       5.833333         281              30         4.387900   \n",
      "12275       6.666667         637              28         4.061224   \n",
      "12276       7.000000         853              61         4.406800   \n",
      "12277       5.666667         478               7         4.692469   \n",
      "12278       5.500000         845              32         3.873373   \n",
      "12279       6.333333         357              22         4.204482   \n",
      "12280       6.000000         693              40         4.004329   \n",
      "12281       7.500000         624              37         3.932692   \n",
      "12282       5.166667         829              36         4.031363   \n",
      "12283       5.000000         227              17         4.488987   \n",
      "12284       5.666667         741              18         4.022942   \n",
      "...              ...         ...             ...              ...   \n",
      "12948       6.666667         477              33         4.291405   \n",
      "12949       5.333333         623              40         3.736758   \n",
      "12950       6.000000         388              22         4.100515   \n",
      "12951       5.166667         361              14         3.958449   \n",
      "12952       5.000000         367              18         4.207084   \n",
      "12953       7.833333         852              66         4.157277   \n",
      "12954       6.666667         430              27         4.183721   \n",
      "12955       5.833333         475              27         4.006316   \n",
      "12956       5.500000         494              35         4.516194   \n",
      "12957       6.000000         306              15         4.555556   \n",
      "12958       6.000000         430              25         4.072093   \n",
      "12959       8.000000         823              58         4.442284   \n",
      "12960       6.666667         559              26         4.368515   \n",
      "12961       6.666667         666              37         4.478979   \n",
      "12962       6.666667         852              40         4.142019   \n",
      "12963       7.000000         853              57         4.475967   \n",
      "12964       6.666667         760              38         4.263158   \n",
      "12965       5.333333         833              15         3.807923   \n",
      "12966       6.000000         712              46         4.164326   \n",
      "12967       6.666667         723              40         4.026279   \n",
      "12968       1.666667           4               1         4.500000   \n",
      "12969       5.500000         792              31         4.060606   \n",
      "12970       7.333333         837              45         4.379928   \n",
      "12971       5.833333         769              32         3.924577   \n",
      "12972       5.000000         818               9         3.997555   \n",
      "12973       5.833333         848              27         4.259434   \n",
      "12974       5.333333         548              35         4.140511   \n",
      "12975       6.666667         818              41         4.621027   \n",
      "12976       6.666667         594              39         4.195286   \n",
      "12977       6.666667         468              29         4.399573   \n",
      "\n",
      "       num_exclamation_marks  num_question_marks  num_stopwords  \\\n",
      "12255                      0                   2            320   \n",
      "12256                      0                   1            404   \n",
      "12257                      0                   0            418   \n",
      "12258                      1                   8            397   \n",
      "12259                      1                   5            343   \n",
      "12260                     13                   9            245   \n",
      "12261                      3                   5            383   \n",
      "12262                      0                   0            159   \n",
      "12263                      0                   2            363   \n",
      "12264                      4                   5            234   \n",
      "12265                      0                   0            217   \n",
      "12266                      7                   0            376   \n",
      "12267                      0                   0            443   \n",
      "12268                      0                   3            281   \n",
      "12269                      8                   6            335   \n",
      "12270                      0                   0             68   \n",
      "12271                      0                   6            349   \n",
      "12272                      6                   2            274   \n",
      "12273                      0                   0            397   \n",
      "12274                      0                   1            135   \n",
      "12275                      0                   1            339   \n",
      "12276                     24                   0            396   \n",
      "12277                      0                   0            221   \n",
      "12278                      1                   7            471   \n",
      "12279                      0                   0            191   \n",
      "12280                      1                   1            369   \n",
      "12281                      0                   0            339   \n",
      "12282                      0                   0            443   \n",
      "12283                      3                   1            113   \n",
      "12284                      0                   0            405   \n",
      "...                      ...                 ...            ...   \n",
      "12948                      0                   1            237   \n",
      "12949                      0                   0            317   \n",
      "12950                      0                   1            196   \n",
      "12951                      0                   1            183   \n",
      "12952                      0                   0            180   \n",
      "12953                      0                   0            436   \n",
      "12954                      0                   0            216   \n",
      "12955                      0                   3            247   \n",
      "12956                      8                   1            255   \n",
      "12957                      0                   0            162   \n",
      "12958                      2                   0            243   \n",
      "12959                      0                   0            417   \n",
      "12960                      0                   1            280   \n",
      "12961                      0                   0            338   \n",
      "12962                      2                   1            436   \n",
      "12963                      4                   1            421   \n",
      "12964                      0                   0            382   \n",
      "12965                      1                   1            411   \n",
      "12966                      0                   0            397   \n",
      "12967                      0                   0            378   \n",
      "12968                      0                   0              1   \n",
      "12969                      0                   0            425   \n",
      "12970                      2                   0            384   \n",
      "12971                      3                   0            420   \n",
      "12972                      0                   0            422   \n",
      "12973                      0                   0            403   \n",
      "12974                      0                  10            257   \n",
      "12975                      9                   7            399   \n",
      "12976                      0                   2            280   \n",
      "12977                      0                   0            239   \n",
      "\n",
      "           ...        verb_count  foreign_count  adj_count  conj_count  \\\n",
      "12255      ...               120              0         16         103   \n",
      "12256      ...               120              0         34         125   \n",
      "12257      ...               144              0         39         136   \n",
      "12258      ...               118              0         21         102   \n",
      "12259      ...               132              0          8          82   \n",
      "12260      ...                95              0         14          73   \n",
      "12261      ...               125              0         29         121   \n",
      "12262      ...                32              0         15          49   \n",
      "12263      ...               103              0         44         140   \n",
      "12264      ...                58              0         28          88   \n",
      "12265      ...                58              0         13          68   \n",
      "12266      ...               119              0         40         112   \n",
      "12267      ...                99              0         29          95   \n",
      "12268      ...                85              0         24          81   \n",
      "12269      ...               122              0         42         126   \n",
      "12270      ...                19              0          6          25   \n",
      "12271      ...               111              0         40         117   \n",
      "12272      ...                84              0         24          77   \n",
      "12273      ...                95              0         21         116   \n",
      "12274      ...                49              0         22          37   \n",
      "12275      ...                92              0         19         108   \n",
      "12276      ...               132              0         16         153   \n",
      "12277      ...                81              0         21          71   \n",
      "12278      ...               129              0         30         137   \n",
      "12279      ...                53              0         20          59   \n",
      "12280      ...               108              0         22         109   \n",
      "12281      ...               109              0         19         104   \n",
      "12282      ...               143              0         42         133   \n",
      "12283      ...                36              0         10          31   \n",
      "12284      ...                97              0         47         130   \n",
      "...        ...               ...            ...        ...         ...   \n",
      "12948      ...                73              0         34          76   \n",
      "12949      ...               109              0         19         104   \n",
      "12950      ...                73              0         13          57   \n",
      "12951      ...                63              0          9          62   \n",
      "12952      ...                59              0         18          46   \n",
      "12953      ...               119              0         46         127   \n",
      "12954      ...                92              0         20          69   \n",
      "12955      ...                69              0         12          75   \n",
      "12956      ...                73              0         26          73   \n",
      "12957      ...                56              0         23          56   \n",
      "12958      ...                76              0         11          74   \n",
      "12959      ...               133              0         21         149   \n",
      "12960      ...                67              0         36         114   \n",
      "12961      ...                81              0         41         111   \n",
      "12962      ...               129              0         31         146   \n",
      "12963      ...               113              0         34         142   \n",
      "12964      ...               135              0         39         123   \n",
      "12965      ...               131              0         31         130   \n",
      "12966      ...               110              0         32         122   \n",
      "12967      ...               135              0         23         112   \n",
      "12968      ...                 0              0          0           1   \n",
      "12969      ...               148              0         47         131   \n",
      "12970      ...               142              0         26         127   \n",
      "12971      ...               142              0         20         131   \n",
      "12972      ...               126              0         23         111   \n",
      "12973      ...               113              0         34         152   \n",
      "12974      ...                71              0         28          89   \n",
      "12975      ...               139              0         28         158   \n",
      "12976      ...                87              0         25          96   \n",
      "12977      ...                67              0         31          74   \n",
      "\n",
      "       adv_count  beauty_score  maturity_score  vocab  sentiment_essay  \\\n",
      "12255         40    348.529998    4.169347e-07    268         0.083224   \n",
      "12256         71    367.687308    8.224299e-03    428         0.242626   \n",
      "12257         56    378.977122    8.812556e-03    431         0.175740   \n",
      "12258         62    365.259774    1.383686e-02    331         0.127181   \n",
      "12259         73    400.494793    4.553345e-12    260         0.142754   \n",
      "12260         42    343.232575    2.136691e-02    278         0.090055   \n",
      "12261         74    465.160833    1.102162e-02    372         0.080578   \n",
      "12262         32    117.214738    2.477246e-02    153         0.226287   \n",
      "12263         61    386.631700    1.582069e-02    390         0.110211   \n",
      "12264         58    257.781450    1.292018e-02    274         0.174153   \n",
      "12265         24    204.233012    2.223342e-02    206         0.265257   \n",
      "12266         58    410.903165    1.826586e-10    380         0.095384   \n",
      "12267         77    395.146054    1.682803e-02    372         0.198775   \n",
      "12268         45    269.554411    3.130584e-02    291         0.253947   \n",
      "12269         55    324.572060    9.574773e-03    374         0.158567   \n",
      "12270          9     66.917145    6.622252e-02     82         0.206250   \n",
      "12271         52    407.712993    9.860065e-03    357         0.125334   \n",
      "12272         54    256.610927    2.227437e-02    277         0.169963   \n",
      "12273         59    411.869073    9.740634e-03    347         0.125052   \n",
      "12274         15    158.578649    2.309320e-02    152         0.401677   \n",
      "12275         55    248.878948    2.119703e-02    286        -0.050772   \n",
      "12276         62    429.495976    1.157234e-02    387         0.085442   \n",
      "12277         45    261.587395    2.821430e-07    233         0.202769   \n",
      "12278         99    492.551167    1.447887e-02    373         0.148713   \n",
      "12279         28    197.187162    2.870330e-02    185         0.250606   \n",
      "12280         51    298.474395    1.673077e-02    312         0.202643   \n",
      "12281         48    263.366945    6.186225e-05    280         0.091517   \n",
      "12282         54    348.336273    1.466166e-02    414         0.157231   \n",
      "12283         26     97.037539    4.289417e-02    112         0.173272   \n",
      "12284         61    366.469061    1.760547e-02    380         0.197854   \n",
      "...          ...           ...             ...    ...              ...   \n",
      "12948         49    258.268635    1.966667e-02    240         0.119607   \n",
      "12949         36    262.181193    1.785743e-02    266         0.127025   \n",
      "12950         40    162.428558    3.651747e-02    187         0.021612   \n",
      "12951         24    225.528147    1.546123e-04    179         0.422500   \n",
      "12952         18    153.263083    3.125792e-02    175         0.021180   \n",
      "12953         43    326.977428    1.604017e-02    399         0.271381   \n",
      "12954         35    164.847733    2.576699e-02    196         0.223993   \n",
      "12955         33    246.903157    2.341909e-02    211         0.220427   \n",
      "12956         37    261.173208    2.231061e-02    264         0.105614   \n",
      "12957         28    176.762143    6.723054e-02    166         0.112663   \n",
      "12958         49    219.570462    1.222503e-06    177         0.074601   \n",
      "12959         50    345.622043    9.343926e-03    381         0.037500   \n",
      "12960         33    300.655896    1.677862e-02    318         0.164962   \n",
      "12961         40    258.587384    1.531271e-02    352         0.147406   \n",
      "12962         47    478.272429    1.230824e-02    439         0.131329   \n",
      "12963         70    431.435152    8.850455e-03    399         0.043536   \n",
      "12964         66    470.061861    1.038828e-02    366         0.128636   \n",
      "12965         82    372.140790    3.658352e-05    315         0.106755   \n",
      "12966         63    313.134278    1.608140e-02    336        -0.010284   \n",
      "12967         74    399.522445    1.913899e-02    311         0.027532   \n",
      "12968          0      1.192334    6.695000e+00      2         0.000000   \n",
      "12969         73    383.271637    2.869620e-05    396         0.096956   \n",
      "12970         77    397.177824    1.035962e-02    387         0.078776   \n",
      "12971         62    312.938802    2.558989e-02    356         0.106333   \n",
      "12972         66    388.374857    1.034985e-02    343         0.146174   \n",
      "12973         67    409.472115    1.604612e-02    391         0.146336   \n",
      "12974         53    313.474505    1.607660e-02    266         0.147192   \n",
      "12975         53    364.203687    9.133910e-03    392         0.131967   \n",
      "12976         55    337.015440    1.338433e-02    263        -0.001249   \n",
      "12977         34    242.472318    2.093778e-02    256         0.247257   \n",
      "\n",
      "       Grammar_check  \n",
      "12255             55  \n",
      "12256             13  \n",
      "12257             24  \n",
      "12258             66  \n",
      "12259            108  \n",
      "12260             76  \n",
      "12261             27  \n",
      "12262             10  \n",
      "12263              3  \n",
      "12264             11  \n",
      "12265             30  \n",
      "12266             22  \n",
      "12267             81  \n",
      "12268             32  \n",
      "12269             21  \n",
      "12270             18  \n",
      "12271             21  \n",
      "12272             13  \n",
      "12273             17  \n",
      "12274              3  \n",
      "12275              9  \n",
      "12276             61  \n",
      "12277             73  \n",
      "12278             86  \n",
      "12279              6  \n",
      "12280             47  \n",
      "12281             12  \n",
      "12282             20  \n",
      "12283              7  \n",
      "12284             28  \n",
      "...              ...  \n",
      "12948              4  \n",
      "12949             45  \n",
      "12950              5  \n",
      "12951             10  \n",
      "12952              9  \n",
      "12953              5  \n",
      "12954              2  \n",
      "12955             52  \n",
      "12956             24  \n",
      "12957             10  \n",
      "12958              3  \n",
      "12959              3  \n",
      "12960              9  \n",
      "12961              5  \n",
      "12962             15  \n",
      "12963              9  \n",
      "12964              1  \n",
      "12965             40  \n",
      "12966              9  \n",
      "12967             13  \n",
      "12968              1  \n",
      "12969             14  \n",
      "12970             17  \n",
      "12971             28  \n",
      "12972             37  \n",
      "12973             15  \n",
      "12974             41  \n",
      "12975             23  \n",
      "12976             13  \n",
      "12977              3  \n",
      "\n",
      "[723 rows x 21 columns]]\n"
     ]
    }
   ],
   "source": [
    "Set=[]\n",
    "for i in range(1,9):\n",
    "    Set.append(df[df['essay_set']==i])\n",
    "print (Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy for each set:  [0.49009999999999998, 0.4874, 0.63270000000000004, 0.6119, 0.63970000000000005, 0.59219999999999995, 0.37859999999999999, 0.51060000000000005]\n",
      "Linear Regression MSE for each set:  [1.1500999999999999, 1.3025, 5.8716999999999997, 4.4135999999999997, 2.7067000000000001, 3.5754000000000001, 2.6019000000000001, 0.73760000000000003]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "MSE_LR=[]\n",
    "Log_R=[]\n",
    "for data in Set:\n",
    "    x = data.drop(['domain1_score','essay_id', 'essay','essay_set'], axis=1)\n",
    "\n",
    "#   y = df['domain1_score']\n",
    "#   x = df_normalized.drop(['domain1_score'],axis=1)\n",
    "# # df['A']=df['A'].fillna(0.0).astype(int)\n",
    "    y = data['domain1_score'].fillna(0.0).astype(int)\n",
    "#     print (x)\n",
    "#     print (y)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    where_are_NaNs = np.isnan(x)\n",
    "    x[where_are_NaNs] = 0\n",
    "    where_are_NaNs = np.isnan(y)\n",
    "    y[where_are_NaNs] = 0\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kfold.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    logistic_reg = LogisticRegression()\n",
    "    logistic_reg.fit(X_train, y_train)\n",
    "    y_pred = logistic_reg.predict(X_test)\n",
    "#     print('\\nLogistic regression classifier accuracy:', logistic_reg.score(X_test, y_test))\n",
    "    MSE_LR.append( round(metrics.mean_squared_error(y_test,y_pred),4) )\n",
    "    Log_R.append( round(logistic_reg.score(X_test, y_test),4) )\n",
    "print (\"Logistic Regression Accuracy for each set: \",Log_R)\n",
    "print (\"Linear Regression MSE for each set: \",MSE_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Accuracy for each set:  [0.66879999999999995, 0.53269999999999995, 0.46739999999999998, 0.59519999999999995, 0.68059999999999998, 0.55989999999999995, 0.43480000000000002, 0.40329999999999999]\n",
      "Linear Regression MSE for each set:  [0.61509999999999998, 0.87350000000000005, 4.1479999999999997, 3.7938999999999998, 1.9155, 2.6093999999999999, 1.8859999999999999, 0.44750000000000001]\n"
     ]
    }
   ],
   "source": [
    "MSE_LinR=[]\n",
    "Lin_R=[]\n",
    "for data in Set:\n",
    "    x = data.drop(['domain1_score', 'essay','essay_set'], axis=1)\n",
    "#   y = df['domain1_score']\n",
    "#   x = df_normalized.drop(['domain1_score'],axis=1)\n",
    "# # df['A']=df['A'].fillna(0.0).astype(int)\n",
    "    y = data['domain1_score'].fillna(0.0).astype(int)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    where_are_NaNs = np.isnan(x)\n",
    "    x[where_are_NaNs] = 0\n",
    "    where_are_NaNs = np.isnan(y)\n",
    "    y[where_are_NaNs] = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kfold.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "#     print (X_train)\n",
    "#     print (y_train)\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "#     print('\\nLinear regression classifier accuracy:', linear_reg.score(X_test, y_test))\n",
    "    Lin_R.append( round(lin_reg.score(X_test, y_test),4) )\n",
    "    MSE_LinR.append( round(metrics.mean_squared_error(y_test,y_pred),4 ) )\n",
    "print (\"Linear Regression Accuracy for each set: \",Lin_R)\n",
    "print (\"Linear Regression MSE for each set: \",MSE_LinR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Regressor linear Accuracy for each set:  [0.66739999999999999, 0.52129999999999999, 0.45029999999999998, 0.59179999999999999, 0.68440000000000001, 0.55010000000000003, 0.43409999999999999, 0.38550000000000001]\n",
      "SVM Regressor linear MSE for each set:  [0.61770000000000003, 0.89470000000000005, 4.2808000000000002, 3.8262, 1.8929, 2.6678999999999999, 1.8883000000000001, 0.46089999999999998]\n"
     ]
    }
   ],
   "source": [
    "SVR=[]\n",
    "MSE_SVR=[]\n",
    "for data in Set:\n",
    "    x = data.drop(['domain1_score', 'essay','essay_id','essay_set'], axis=1)\n",
    "#   y = df['domain1_score']\n",
    "#   x = df_normalized.drop(['domain1_score'],axis=1)\n",
    "# # df['A']=df['A'].fillna(0.0).astype(int)\n",
    "    y = data['domain1_score'].fillna(0.0).astype(int)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    where_are_NaNs = np.isnan(x)\n",
    "    x[where_are_NaNs] = 0\n",
    "    where_are_NaNs = np.isnan(y)\n",
    "    y[where_are_NaNs] = 0\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kfold.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = svm.SVR(kernel=\"linear\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    SVR.append(round(clf.score(X_test, y_test),4))\n",
    "    MSE_SVR.append( round(metrics.mean_squared_error(y_test,y_pred),4 ) )\n",
    "print (\"SVM Regressor linear Accuracy for each set: \",SVR)\n",
    "print (\"SVM Regressor linear MSE for each set: \",MSE_SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Regressor RBF Accuracy for each set:  [-0.0030000000000000001, -0.0074000000000000003, 0.065699999999999995, 0.18479999999999999, 0.1115, 0.039100000000000003, 0.011900000000000001, -0.0015]\n",
      "SVM Regressor RBF MSE for each set:  [1.8627, 1.883, 7.2759999999999998, 7.6406000000000001, 5.3295000000000003, 5.6981999999999999, 3.2968000000000002, 0.75119999999999998]\n"
     ]
    }
   ],
   "source": [
    "SVR2=[]\n",
    "MSE_SVR2=[]\n",
    "for data in Set:\n",
    "    x = data.drop(['domain1_score', 'essay','essay_id','essay_set'], axis=1)\n",
    "#   y = df['domain1_score']\n",
    "#   x = df_normalized.drop(['domain1_score'],axis=1)\n",
    "# # df['A']=df['A'].fillna(0.0).astype(int)\n",
    "    y = data['domain1_score'].fillna(0.0).astype(int)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    where_are_NaNs = np.isnan(x)\n",
    "    x[where_are_NaNs] = 0\n",
    "    where_are_NaNs = np.isnan(y)\n",
    "    y[where_are_NaNs] = 0\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kfold.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = svm.SVR(kernel=\"rbf\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    SVR2.append(clf.score(X_test, y_test))\n",
    "    MSE_SVR2.append( round(metrics.mean_squared_error(y_test,y_pred),4 ) )\n",
    "print (\"SVM Regressor RBF Accuracy for each set: \",SVR2)\n",
    "print (\"SVM Regressor RBF MSE for each set: \",MSE_SVR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Accuracy:  [0.66830000000000001, 0.61229999999999996, 0.42799999999999999, 0.61380000000000001, 0.7036, 0.53449999999999998, 0.52529999999999999, 0.39090000000000003]\n",
      "Random Forest Regressor MSE for each set:  [0.61609999999999998, 0.72460000000000002, 4.4547999999999996, 3.6200999999999999, 1.778, 2.7605, 1.5838000000000001, 0.45689999999999997]\n"
     ]
    }
   ],
   "source": [
    "MSE_RF=[]\n",
    "RF=[]\n",
    "for data in Set:\n",
    "    x = data.drop(['domain1_score', 'essay','essay_id','essay_set'], axis=1)\n",
    "#   y = df['domain1_score']\n",
    "#   x = df_normalized.drop(['domain1_score'],axis=1)\n",
    "# # df['A']=df['A'].fillna(0.0).astype(int)\n",
    "    y = data['domain1_score'].fillna(0.0).astype(int)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    where_are_NaNs = np.isnan(x)\n",
    "    x[where_are_NaNs] = 0\n",
    "    where_are_NaNs = np.isnan(y)\n",
    "    y[where_are_NaNs] = 0\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kfold.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    model = RandomForestRegressor(random_state=0,n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)    \n",
    "#     print('Random Forest Regressor Accuracy:', clf.score(X_test, y_test))\n",
    "    RF.append(round(model.score(X_test, y_test),4))\n",
    "    MSE_RF.append( round(metrics.mean_squared_error(y_test,y_pred),4 ) )\n",
    "print (\"Random Forest Regressor Accuracy: \",RF)\n",
    "print (\"Random Forest Regressor MSE for each set: \",MSE_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame(columns=['Model','set1','set2','set3','set4','set5','set6','set7','set8'])\n",
    "results = results.append(pd.Series(MSE_LR, index=['set1','set2','set3','set4','set5','set6','set7','set8']), ignore_index=True)\n",
    "results = results.append(pd.Series(MSE_LinR, index=['set1','set2','set3','set4','set5','set6','set7','set8']), ignore_index=True)\n",
    "results = results.append(pd.Series(MSE_SVR, index=['set1','set2','set3','set4','set5','set6','set7','set8']), ignore_index=True)\n",
    "results = results.append(pd.Series(MSE_SVR2, index=['set1','set2','set3','set4','set5','set6','set7','set8']), ignore_index=True)\n",
    "results = results.append(pd.Series(MSE_RF, index=['set1','set2','set3','set4','set5','set6','set7','set8']), ignore_index=True)\n",
    "results.Model=['Logistic Regression','Linear Regression','SVR(linear)','SVR(rbf)','Random Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>set1</th>\n",
       "      <th>set2</th>\n",
       "      <th>set3</th>\n",
       "      <th>set4</th>\n",
       "      <th>set5</th>\n",
       "      <th>set6</th>\n",
       "      <th>set7</th>\n",
       "      <th>set8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.1501</td>\n",
       "      <td>1.3025</td>\n",
       "      <td>5.8717</td>\n",
       "      <td>4.4136</td>\n",
       "      <td>2.7067</td>\n",
       "      <td>3.5754</td>\n",
       "      <td>2.6019</td>\n",
       "      <td>0.7376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>4.1480</td>\n",
       "      <td>3.7939</td>\n",
       "      <td>1.9155</td>\n",
       "      <td>2.6094</td>\n",
       "      <td>1.8860</td>\n",
       "      <td>0.4475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVR(linear)</td>\n",
       "      <td>0.6177</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>4.2808</td>\n",
       "      <td>3.8262</td>\n",
       "      <td>1.8929</td>\n",
       "      <td>2.6679</td>\n",
       "      <td>1.8883</td>\n",
       "      <td>0.4609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVR(rbf)</td>\n",
       "      <td>1.8627</td>\n",
       "      <td>1.8830</td>\n",
       "      <td>7.2760</td>\n",
       "      <td>7.6406</td>\n",
       "      <td>5.3295</td>\n",
       "      <td>5.6982</td>\n",
       "      <td>3.2968</td>\n",
       "      <td>0.7512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>0.7246</td>\n",
       "      <td>4.4548</td>\n",
       "      <td>3.6201</td>\n",
       "      <td>1.7780</td>\n",
       "      <td>2.7605</td>\n",
       "      <td>1.5838</td>\n",
       "      <td>0.4569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model    set1    set2    set3    set4    set5    set6  \\\n",
       "0  Logistic Regression  1.1501  1.3025  5.8717  4.4136  2.7067  3.5754   \n",
       "1    Linear Regression  0.6151  0.8735  4.1480  3.7939  1.9155  2.6094   \n",
       "2          SVR(linear)  0.6177  0.8947  4.2808  3.8262  1.8929  2.6679   \n",
       "3             SVR(rbf)  1.8627  1.8830  7.2760  7.6406  5.3295  5.6982   \n",
       "4        Random Forest  0.6161  0.7246  4.4548  3.6201  1.7780  2.7605   \n",
       "\n",
       "     set7    set8  \n",
       "0  2.6019  0.7376  \n",
       "1  1.8860  0.4475  \n",
       "2  1.8883  0.4609  \n",
       "3  3.2968  0.7512  \n",
       "4  1.5838  0.4569  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "writer = ExcelWriter('Results_setwise.xlsx')\n",
    "results.to_excel(writer,'Sheet1',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0       6.666667  \n",
       "1       7.500000  \n",
       "2       5.833333  \n",
       "3       8.333333  \n",
       "4       6.666667  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_excel('test2.xlsx')\n",
    "# X = X.drop(X.columns[0:2],axis=1)\n",
    "y = pd.DataFrame(X['domain1_score'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      essay_id  essay_set                                              essay  \\\n",
      "0            1          1  Dear local newspaper, I think effects computer...   \n",
      "1            2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2            3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3            4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4            5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "5            6          1  Dear @LOCATION1, I think that computers have a...   \n",
      "6            7          1  Did you know that more and more people these d...   \n",
      "7            8          1  @PERCENT1 of people agree that computers make ...   \n",
      "8            9          1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
      "9           10          1  In the @LOCATION1 we have the technology of a ...   \n",
      "10          11          1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
      "11          12          1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
      "12          13          1  Dear local newspaper I raed ur argument on the...   \n",
      "13          14          1  My three detaileds for this news paper article...   \n",
      "14          15          1  Dear, In this world today we should have every...   \n",
      "15          16          1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
      "16          17          1  Dear Local Newspaper, I belive that computers ...   \n",
      "17          18          1  Dear Local Newspaper, I must admit that the ex...   \n",
      "18          19          1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
      "19          20          1  Well computers can be a good or a bad thing. I...   \n",
      "20          21          1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
      "21          22          1  Dear local Newspaper @CAPS1 a take all your co...   \n",
      "22          23          1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
      "23          24          1  Dear local newspaper, I've heard that not many...   \n",
      "24          25          1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
      "25          26          1  Do you think that computers are useless? Or do...   \n",
      "26          27          1  Computers a good because you can get infermati...   \n",
      "27          28          1  Dear Newspaper, Computers are high tec and hav...   \n",
      "28          29          1  Dear local newspaper, @CAPS1 people throughout...   \n",
      "29          30          1  Dear Newspaper People, I think that computers ...   \n",
      "...        ...        ...                                                ...   \n",
      "1753      1758          1  Dear local newspaper, @CAPS1 on a beautiful su...   \n",
      "1754      1759          1  Dear @CAPS1, I believe that computers have a n...   \n",
      "1755      1760          1  I think we can all agree that computer usage i...   \n",
      "1756      1761          1  Dear @PERSON1, Computers are very helpful in d...   \n",
      "1757      1762          1  Dear Newspaper, @CAPS1 are worried that people...   \n",
      "1758      1763          1  Dear Local Newspaper: @CAPS1 you know that ove...   \n",
      "1759      1764          1  Dear @PERSON1, The advansing technology is sho...   \n",
      "1760      1765          1  Dear local Newspaper I ting that computers are...   \n",
      "1761      1766          1  Man has always been interested in technology. ...   \n",
      "1762      1767          1  Guaranteed, @NUM1 years from now we will still...   \n",
      "1763      1768          1  I think the effects of the computer are bad, t...   \n",
      "1764      1769          1  Dear editor, I think people are using computer...   \n",
      "1765      1770          1  Dear @CAPS1 @CAPS2, @CAPS3, experts have been ...   \n",
      "1766      1771          1  Computers, a @LOCATION1 topic if you ask me. S...   \n",
      "1767      1772          1  Dear Newspaper Readers, @CAPS1 many hours a da...   \n",
      "1768      1773          1  Dear @CAPS1 newspaper, I have resently read th...   \n",
      "1769      1774          1  Dear @ORGANIZATION2 (our local newspaper), @CA...   \n",
      "1770      1775          1  Dear newspaper, In my opinion computers do ben...   \n",
      "1771      1776          1  Technology, such as computers are very big. I ...   \n",
      "1772      1777          1  Dear Newspaper, Computers have advance a lot s...   \n",
      "1773      1778          1  Dear Newspaper, I think that computers have a ...   \n",
      "1774      1779          1  Dear @LOCATION1, *@CAPS1*. Now I hear my favor...   \n",
      "1775      1780          1  Dear Newspaper I think that computers were one...   \n",
      "1776      1781          1  Mom!!! Did you know that the human body has on...   \n",
      "1777      1782          1  Dear @ORGANIZATION1, I believe that computers ...   \n",
      "1778      1783          1  Dear @CAPS1, @CAPS2 several reasons on way I t...   \n",
      "1779      1784          1  Do a adults and kids spend to much time on the...   \n",
      "1780      1785          1  My opinion is that people should have computer...   \n",
      "1781      1786          1  Dear readers, I think that its good and bad to...   \n",
      "1782      1787          1  Dear - Local Newspaper I agree thats computers...   \n",
      "\n",
      "      domain1_score  \n",
      "0          6.666667  \n",
      "1          7.500000  \n",
      "2          5.833333  \n",
      "3          8.333333  \n",
      "4          6.666667  \n",
      "5          6.666667  \n",
      "6          8.333333  \n",
      "7          8.333333  \n",
      "8          7.500000  \n",
      "9          7.500000  \n",
      "10         6.666667  \n",
      "11         6.666667  \n",
      "12         5.833333  \n",
      "13         5.000000  \n",
      "14         5.000000  \n",
      "15        10.000000  \n",
      "16         6.666667  \n",
      "17         6.666667  \n",
      "18         3.333333  \n",
      "19         5.000000  \n",
      "20         6.666667  \n",
      "21         2.500000  \n",
      "22         8.333333  \n",
      "23         9.166667  \n",
      "24         6.666667  \n",
      "25         7.500000  \n",
      "26         3.333333  \n",
      "27         7.500000  \n",
      "28         7.500000  \n",
      "29         6.666667  \n",
      "...             ...  \n",
      "1753       8.333333  \n",
      "1754       8.333333  \n",
      "1755      10.000000  \n",
      "1756       6.666667  \n",
      "1757       6.666667  \n",
      "1758      10.000000  \n",
      "1759       6.666667  \n",
      "1760       4.166667  \n",
      "1761       6.666667  \n",
      "1762       7.500000  \n",
      "1763       6.666667  \n",
      "1764       7.500000  \n",
      "1765       8.333333  \n",
      "1766       6.666667  \n",
      "1767       8.333333  \n",
      "1768       6.666667  \n",
      "1769       8.333333  \n",
      "1770       7.500000  \n",
      "1771       7.500000  \n",
      "1772       9.166667  \n",
      "1773       4.166667  \n",
      "1774       8.333333  \n",
      "1775       7.500000  \n",
      "1776       8.333333  \n",
      "1777       6.666667  \n",
      "1778       6.666667  \n",
      "1779       5.833333  \n",
      "1780       6.666667  \n",
      "1781       1.666667  \n",
      "1782       5.833333  \n",
      "\n",
      "[1783 rows x 4 columns],       essay_id  essay_set                                              essay  \\\n",
      "1783      2978          2  Certain materials being removed from libraries...   \n",
      "1784      2979          2  Write a persuasive essay to a newspaper reflec...   \n",
      "1785      2980          2  Do you think that libraries should remove cert...   \n",
      "1786      2981          2  In @DATE1's world, there are many things found...   \n",
      "1787      2982          2  In life you have the 'offensive things'. The l...   \n",
      "1788      2983          2  A lot of @CAPS3 today are censored because of ...   \n",
      "1789      2984          2  How @CAPS4 you feel if your favorite book was ...   \n",
      "1790      2985          2  Do you think that you should be able to take c...   \n",
      "1791      2986          2  The idea of the removal of 'offensive' materia...   \n",
      "1792      2987          2  All authors write for a purpose, whether it be...   \n",
      "1793      2988          2  Have you seen a magazine, book, movies, etc., ...   \n",
      "1794      2989          2  Personally I don't think libraries, movie stor...   \n",
      "1795      2990          2  A library is a place you can go to read, write...   \n",
      "1796      2991          2  Yes and no, some materials such as books, musi...   \n",
      "1797      2992          2  When talking about censorship in libraries, I ...   \n",
      "1798      2993          2  Dear Newspaper,     @CAPS1 would you feel if s...   \n",
      "1799      2994          2  A lot of people that are in school have probab...   \n",
      "1800      2995          2  Do you think that if certain books, music, mov...   \n",
      "1801      2996          2  If the people that are publishing and writing ...   \n",
      "1802      2997          2  I think that all the bad books should be taken...   \n",
      "1803      2998          2  Dear news people,     I am qriting to you beca...   \n",
      "1804      2999          2  I think that books, magazines, papers etc shou...   \n",
      "1805      3000          2  Hello, my name is @CAPS1, I feel that there sh...   \n",
      "1806      3001          2  Books Removed From Shelves?      'All of us ca...   \n",
      "1807      3002          2  I think almost every movie might offend someon...   \n",
      "1808      3003          2  The world in full of offensive material, and t...   \n",
      "1809      3004          2  People are entitile to their own oppinion abou...   \n",
      "1810      3005          2  I think if people find books offensive they sh...   \n",
      "1811      3006          2  A book is more than words or happenings; they'...   \n",
      "1812      3007          2  I don't believe that certain books, music, mag...   \n",
      "...        ...        ...                                                ...   \n",
      "3553      4748          2  No, materials should not be removed from the l...   \n",
      "3554      4749          2  When you think of a book do you think happy ki...   \n",
      "3555      4750          2  There should not be sensorship on the books in...   \n",
      "3556      4751          2  I do not believe that certain materials, such ...   \n",
      "3557      4752          2  Censorship in libraries is when a librarian ha...   \n",
      "3558      4753          2  I know that some people find some books offens...   \n",
      "3559      4754          2  Do citizens have the right to remove books, ma...   \n",
      "3560      4755          2  Why do we have books out there that are viewed...   \n",
      "3561      4756          2  Libraries @CAPS1 Or @CAPS2?         This world...   \n",
      "3562      4757          2  I believe that if that book, music, or magazin...   \n",
      "3563      4758          2  Do you believe that certain materials, such as...   \n",
      "3564      4759          2  Today in the media there are many things that ...   \n",
      "3565      4760          2  Certain books, magazines, music and movies sho...   \n",
      "3566      4761          2  Offensive things should be taken off for many ...   \n",
      "3567      4762          2  What if some one found your favorite book offe...   \n",
      "3568      4763          2  I blieve that any book or movie that is in the...   \n",
      "3569      4764          2  Debates are a main part of our society today. ...   \n",
      "3570      4765          2  Books are very important to our society. They ...   \n",
      "3571      4766          2  In todays world, children are exposed to a lot...   \n",
      "3572      4767          2  This is going to be an essay about if people f...   \n",
      "3573      4768          2  Books, movies, magazines, music, etc., make up...   \n",
      "3574      4769          2  I believe that certain materials should not be...   \n",
      "3575      4770          2  I personally think certain materials, such as ...   \n",
      "3576      4771          2  We all hope that one day our children will not...   \n",
      "3577      4772          2  Libraries have a variety of material from book...   \n",
      "3578      4773          2  The author is writting about taking books off ...   \n",
      "3579      4774          2  I do not think that materials, such as books, ...   \n",
      "3580      4775          2  Yes we should keep the books,music,movies,an m...   \n",
      "3581      4776          2  I do believe that  book, magazines, music, mov...   \n",
      "3582      4777          2  Different Then Everyone Else     @CAPS1 do peo...   \n",
      "\n",
      "      domain1_score  \n",
      "1783            8.0  \n",
      "1784            2.0  \n",
      "1785            5.0  \n",
      "1786            8.0  \n",
      "1787            8.0  \n",
      "1788            7.0  \n",
      "1789            9.0  \n",
      "1790            4.0  \n",
      "1791            8.0  \n",
      "1792            8.0  \n",
      "1793            6.0  \n",
      "1794            6.0  \n",
      "1795            9.0  \n",
      "1796            7.0  \n",
      "1797            5.0  \n",
      "1798            6.0  \n",
      "1799            8.0  \n",
      "1800            6.0  \n",
      "1801            3.0  \n",
      "1802            6.0  \n",
      "1803            5.0  \n",
      "1804            6.0  \n",
      "1805            6.0  \n",
      "1806            8.0  \n",
      "1807            6.0  \n",
      "1808            7.0  \n",
      "1809            8.0  \n",
      "1810            4.0  \n",
      "1811            6.0  \n",
      "1812            6.0  \n",
      "...             ...  \n",
      "3553            4.0  \n",
      "3554            6.0  \n",
      "3555            4.0  \n",
      "3556            8.0  \n",
      "3557            7.0  \n",
      "3558            7.0  \n",
      "3559            9.0  \n",
      "3560            8.0  \n",
      "3561            7.0  \n",
      "3562            8.0  \n",
      "3563            7.0  \n",
      "3564            6.0  \n",
      "3565            6.0  \n",
      "3566            6.0  \n",
      "3567            7.0  \n",
      "3568            6.0  \n",
      "3569            8.0  \n",
      "3570            8.0  \n",
      "3571            9.0  \n",
      "3572            6.0  \n",
      "3573            7.0  \n",
      "3574            6.0  \n",
      "3575            9.0  \n",
      "3576            8.0  \n",
      "3577            6.0  \n",
      "3578            6.0  \n",
      "3579            6.0  \n",
      "3580            4.0  \n",
      "3581            7.0  \n",
      "3582            5.0  \n",
      "\n",
      "[1800 rows x 4 columns],       essay_id  essay_set                                              essay  \\\n",
      "3583      5978          3  The features of the setting affect the cyclist...   \n",
      "3584      5979          3  The features of the setting affected the cycli...   \n",
      "3585      5980          3  Everyone travels to unfamiliar places. Sometim...   \n",
      "3586      5981          3  I believe the features of the cyclist affected...   \n",
      "3587      5982          3  The setting effects the cyclist because of the...   \n",
      "3588      5983          3  There were many features of the setting that a...   \n",
      "3589      5984          3  The cyclist was riding through a tower when he...   \n",
      "3590      5985          3  The affects of the cyclist is if it does not c...   \n",
      "3591      5986          3  The essay â€œRough Road Ahead: Do Not Exceed Pos...   \n",
      "3592      5987          3  In the story, â€œRough Road Ahead: Do Not Exceed...   \n",
      "3593      5988          3  The setting of the authors â€œ shortcutâ€ effecte...   \n",
      "3594      5989          3  The features of the setting affect the cyclist...   \n",
      "3595      5990          3  There was many features that affected the cycl...   \n",
      "3596      5991          3  The features in the setting affected the cycli...   \n",
      "3597      5992          3  The features of the setting in â€œRough Road Ahe...   \n",
      "3598      5993          3  In the essay Do Not Exceed Posted Speed Limit ...   \n",
      "3599      5994          3  Many different characteristics in a setting an...   \n",
      "3600      5995          3  The setting of the story affects the cyclist i...   \n",
      "3601      5996          3  In the story â€œRough Road Aheadâ€ by @PERSON1, t...   \n",
      "3602      5997          3  Some features of the setting affect the cyclis...   \n",
      "3603      5998          3  The features of the setting that affect the cy...   \n",
      "3604      5999          3  The cyclist has a couple things if he racing a...   \n",
      "3605      6000          3  â€œDo Not Exceed Posted Speed Limitâ€ had a  roug...   \n",
      "3606      6001          3  An example of the setting affecting the cyclis...   \n",
      "3607      6002          3  The cyclist had a very hard time geting to Yos...   \n",
      "3608      6003          3  The cyclist in the story is at first confident...   \n",
      "3609      6004          3  Well I would put it like this is that the weat...   \n",
      "3610      6005          3  The features of the setting affect the cyclist...   \n",
      "3611      6006          3  The settings of the story were grveling. While...   \n",
      "3612      6007          3  As the cyclist is travelling to Yosemite Natio...   \n",
      "...        ...        ...                                                ...   \n",
      "5279      7679          3  In the story â€œDo not exceed posted speedâ€ by @...   \n",
      "5280      7680          3  Every body experiences physical and emotional ...   \n",
      "5281      7681          3  The features of the setting would have affecte...   \n",
      "5282      7682          3  There are many features of the setting that af...   \n",
      "5283      7683          3  In  rough raod ahead by Joe Kurmaskie the feat...   \n",
      "5284      7684          3  In the essay, â€œRough Road A head Do Not Exceed...   \n",
      "5285      7685          3  The cyclist is alone traveling on a flat land ...   \n",
      "5286      7686          3  In the story â€œDo not exceed posted speed Limit...   \n",
      "5287      7687          3  The setting effected the performence of the cy...   \n",
      "5288      7688          3  â€œDo not exceed posted speed limitâ€, is a short...   \n",
      "5289      7689          3  The young cycolist began his day confident in ...   \n",
      "5290      7690          3  In the essay the features of the setting affec...   \n",
      "5291      7691          3  Weather, places, the will to continue â€“ all ar...   \n",
      "5292      7692          3  The story â€œRough road ahead: do not exceed pos...   \n",
      "5293      7693          3  In the story, â€œRough Road Ahead: Do not exceed...   \n",
      "5294      7694          3  The features of the setting have a huge effect...   \n",
      "5295      7695          3  The features of the setting effect the cyclist...   \n",
      "5296      7696          3  In the essay â€œRough Road Ahead: Do Not Exceed ...   \n",
      "5297      7697          3  The features of the setting affect the cyclist...   \n",
      "5298      7698          3  The setting of the story, are very dry and hum...   \n",
      "5299      7699          3  The features of the setting affect the cyclist...   \n",
      "5300      7700          3  The cyclist in this essay is trying to go to Y...   \n",
      "5301      7701          3  It give him a hard time so he learn a lot of t...   \n",
      "5302      7702          3  The features of the setting very much affect t...   \n",
      "5303      7703          3  The features of the setting in this story make...   \n",
      "5304      7704          3  In the story, the setting affected the cyclist...   \n",
      "5305      7705          3  The features of the setting affect the cyclist...   \n",
      "5306      7706          3  The setting greatly affects the cyclist trying...   \n",
      "5307      7707          3  The features of the setting affected the cycli...   \n",
      "5308      7708          3  The features of the setting in â€œRough Road Ahe...   \n",
      "\n",
      "      domain1_score  \n",
      "3583       3.333333  \n",
      "3584       6.666667  \n",
      "3585       3.333333  \n",
      "3586       3.333333  \n",
      "3587       6.666667  \n",
      "3588       3.333333  \n",
      "3589       3.333333  \n",
      "3590       0.000000  \n",
      "3591       6.666667  \n",
      "3592      10.000000  \n",
      "3593      10.000000  \n",
      "3594       3.333333  \n",
      "3595      10.000000  \n",
      "3596       6.666667  \n",
      "3597       3.333333  \n",
      "3598       6.666667  \n",
      "3599       6.666667  \n",
      "3600       6.666667  \n",
      "3601       3.333333  \n",
      "3602       6.666667  \n",
      "3603       6.666667  \n",
      "3604       3.333333  \n",
      "3605       6.666667  \n",
      "3606       3.333333  \n",
      "3607       6.666667  \n",
      "3608      10.000000  \n",
      "3609       3.333333  \n",
      "3610      10.000000  \n",
      "3611       6.666667  \n",
      "3612      10.000000  \n",
      "...             ...  \n",
      "5279       3.333333  \n",
      "5280      10.000000  \n",
      "5281      10.000000  \n",
      "5282       3.333333  \n",
      "5283       6.666667  \n",
      "5284      10.000000  \n",
      "5285      10.000000  \n",
      "5286       3.333333  \n",
      "5287      10.000000  \n",
      "5288      10.000000  \n",
      "5289      10.000000  \n",
      "5290       6.666667  \n",
      "5291       6.666667  \n",
      "5292       6.666667  \n",
      "5293      10.000000  \n",
      "5294      10.000000  \n",
      "5295       3.333333  \n",
      "5296       6.666667  \n",
      "5297       6.666667  \n",
      "5298       6.666667  \n",
      "5299       3.333333  \n",
      "5300      10.000000  \n",
      "5301       0.000000  \n",
      "5302      10.000000  \n",
      "5303       6.666667  \n",
      "5304       6.666667  \n",
      "5305       3.333333  \n",
      "5306       6.666667  \n",
      "5307       6.666667  \n",
      "5308      10.000000  \n",
      "\n",
      "[1726 rows x 4 columns],       essay_id  essay_set                                              essay  \\\n",
      "5309      8863          4  The author concludes the story with this becau...   \n",
      "5310      8864          4  The narrater has that in with Paragraph becuse...   \n",
      "5311      8865          4  The author concludes the story with that passa...   \n",
      "5312      8866          4  The author ended the story with this paragraph...   \n",
      "5313      8867          4  The author concludes the story with this parag...   \n",
      "5314      8868          4  The reason the author concludes the story with...   \n",
      "5315      8869          4  The story ended with the author saying, \"when ...   \n",
      "5316      8870          4  I believe that the author concludes the story ...   \n",
      "5317      8871          4  The author would conclude the story with that ...   \n",
      "5318      8872          4  In the story, \"Winter Hibiscus,\" by Minfong Ho...   \n",
      "5319      8873          4  The author concludes the story with this parag...   \n",
      "5320      8874          4  I think he concluded the story with paragraph ...   \n",
      "5321      8875          4  The author concludes the story with this parag...   \n",
      "5322      8876          4  The author concludes the story with this parag...   \n",
      "5323      8877          4  Why the author concludes the story with this p...   \n",
      "5324      8878          4  He states this as the conclusion because it sh...   \n",
      "5325      8879          4  I believe the author concludes the story with ...   \n",
      "5326      8880          4  The arthur uses it because Saeng got confidenc...   \n",
      "5327      8881          4  the author concludes the story this way becaus...   \n",
      "5328      8882          4  I think the author ends the story with this pa...   \n",
      "5329      8883          4  I think that the author concludes the story wi...   \n",
      "5330      8884          4  They probably ended it like that to build susp...   \n",
      "5331      8885          4   Inâ€ Winter Hibiscus,â€ by Minfong Ho, the main...   \n",
      "5332      8886          4  The author concludes the story with this parag...   \n",
      "5333      8887          4  The author concludes this story with this para...   \n",
      "5334      8888          4  In the story, â€œWinter Hibiscusâ€, the author en...   \n",
      "5335      8889          4  I think that she ended the story like that bec...   \n",
      "5336      8890          4  The author concludes the story with this parag...   \n",
      "5337      8891          4  By ending the story with the idea of retaking ...   \n",
      "5338      8892          4  The author concluded the story with that speci...   \n",
      "...        ...        ...                                                ...   \n",
      "7051     10613          4  Coming back from failing her driving test, Sae...   \n",
      "7052     10614          4  The story of â€œWinter Hibiscusâ€ is a prolonged ...   \n",
      "7053     10615          4  In â€œWinter Hibiscusâ€ by Minfong Ho, Saeng has ...   \n",
      "7054     10616          4  Since it is getting close to winter the geese ...   \n",
      "7055     10617          4  The auther concludes the story with this parag...   \n",
      "7056     10618          4  The author concludes her story with that last ...   \n",
      "7057     10619          4  The author is relating to that when the geese ...   \n",
      "7058     10620          4  I think the Author ends this as the concluding...   \n",
      "7059     10621          4  the author concludes the story with that parag...   \n",
      "7060     10622          4  I think the author concludes the story with th...   \n",
      "7061     10623          4  In the short story â€œWinter Hibiscusâ€ Saeng dec...   \n",
      "7062     10624          4  The author concludes the story with that parag...   \n",
      "7063     10625          4  I think that the author concludes the story li...   \n",
      "7064     10626          4  The last paragraph of the story was: â€œWhen the...   \n",
      "7065     10627          4  Saeng told her mother that she failed the test...   \n",
      "7066     10628          4  The Author concludes that, when the geese come...   \n",
      "7067     10629          4  The reason the author chose this paragraph is ...   \n",
      "7068     10630          4  The author concludes the story with the paragr...   \n",
      "7069     10631          4  The author concludes the story with this sente...   \n",
      "7070     10632          4  The author concludes the story with that parag...   \n",
      "7071     10633          4  The author concludes the story with this parag...   \n",
      "7072     10634          4  She ends it with that statement to show that s...   \n",
      "7073     10635          4  She realized that things come and go but also ...   \n",
      "7074     10636          4  The author ended the story with that paragraph...   \n",
      "7075     10637          4  I think the author concludes this story like t...   \n",
      "7076     10638          4  To me it seam like the whoever was saying that...   \n",
      "7077     10639          4  The author concludes the story with this becau...   \n",
      "7078     10640          4  The author uses this conclusion for a reason. ...   \n",
      "7079     10641          4  The author concludes the story with this parag...   \n",
      "7080     10642          4  There was a specific reason as to why the auth...   \n",
      "\n",
      "      domain1_score  \n",
      "5309       0.000000  \n",
      "5310       0.000000  \n",
      "5311      10.000000  \n",
      "5312       6.666667  \n",
      "5313       6.666667  \n",
      "5314       3.333333  \n",
      "5315       0.000000  \n",
      "5316       6.666667  \n",
      "5317       3.333333  \n",
      "5318       6.666667  \n",
      "5319       6.666667  \n",
      "5320       0.000000  \n",
      "5321       3.333333  \n",
      "5322       6.666667  \n",
      "5323       0.000000  \n",
      "5324       3.333333  \n",
      "5325       6.666667  \n",
      "5326       3.333333  \n",
      "5327      10.000000  \n",
      "5328       3.333333  \n",
      "5329       3.333333  \n",
      "5330       3.333333  \n",
      "5331      10.000000  \n",
      "5332       3.333333  \n",
      "5333       6.666667  \n",
      "5334       6.666667  \n",
      "5335       3.333333  \n",
      "5336       6.666667  \n",
      "5337      10.000000  \n",
      "5338       3.333333  \n",
      "...             ...  \n",
      "7051       6.666667  \n",
      "7052      10.000000  \n",
      "7053       6.666667  \n",
      "7054       6.666667  \n",
      "7055       0.000000  \n",
      "7056       3.333333  \n",
      "7057       6.666667  \n",
      "7058       6.666667  \n",
      "7059       0.000000  \n",
      "7060       3.333333  \n",
      "7061       3.333333  \n",
      "7062       3.333333  \n",
      "7063       3.333333  \n",
      "7064       6.666667  \n",
      "7065       3.333333  \n",
      "7066       6.666667  \n",
      "7067       0.000000  \n",
      "7068       6.666667  \n",
      "7069       6.666667  \n",
      "7070       3.333333  \n",
      "7071       3.333333  \n",
      "7072       6.666667  \n",
      "7073       6.666667  \n",
      "7074       3.333333  \n",
      "7075       3.333333  \n",
      "7076       0.000000  \n",
      "7077       6.666667  \n",
      "7078       3.333333  \n",
      "7079       0.000000  \n",
      "7080       6.666667  \n",
      "\n",
      "[1772 rows x 4 columns],       essay_id  essay_set                                              essay  \\\n",
      "7081     11827          5  In this memoir of Narciso Rodriguez, @PERSON3'...   \n",
      "7082     11828          5  Throughout the excerpt from Home the Blueprint...   \n",
      "7083     11829          5  The mood the author created in the memoir is l...   \n",
      "7084     11830          5  The mood created by the author is showing how ...   \n",
      "7085     11831          5  The mood created in the memoir is happiness an...   \n",
      "7086     11832          5  The mood definitely helps you feel and compreh...   \n",
      "7087     11833          5  In Narciso Rodriguez's memoir from \"Home: The ...   \n",
      "7088     11834          5  In the excerpt the mood created by the author ...   \n",
      "7089     11835          5  The mood created by the author in the memoir i...   \n",
      "7090     11836          5  The author generally stayed in one mood. The m...   \n",
      "7091     11837          5  The mood that the author creates in this story...   \n",
      "7092     11838          5  The mood in the memoir that the author creates...   \n",
      "7093     11839          5  The mood of the memoir is very upbeat, happy m...   \n",
      "7094     11840          5  Describe the mood by the author in the memoir....   \n",
      "7095     11841          5  The mood created by the author in the memoir w...   \n",
      "7096     11842          5   The mood created by the author in this memoir...   \n",
      "7097     11843          5  In this memoir, the author creates a mood. The...   \n",
      "7098     11844          5  The author of this memoir created a mood about...   \n",
      "7099     11845          5  The mood created by this author is clearly gre...   \n",
      "7100     11846          5  The mood set by the author in the memoir is a ...   \n",
      "7101     11847          5  The mood created by the author in the memoir i...   \n",
      "7102     11848          5  In the memoir, the author created a very upbea...   \n",
      "7103     11849          5  The mood created by the author of the memoir i...   \n",
      "7104     11850          5  The author, Narciso Rodriguez, creates a very ...   \n",
      "7105     11851          5  Based on this memoir, the mood created by the ...   \n",
      "7106     11852          5  The mood created by Narciso Rodriguez was happ...   \n",
      "7107     11853          5  In the memoir Narciso Rodriguez the author cre...   \n",
      "7108     11854          5  @PERSON1's mood was happy and greatful. @PERSO...   \n",
      "7109     11855          5  In Narciso Rodriguez's memoir, the mood and fe...   \n",
      "7110     11856          5  In the memoir @PERSON1 it talks about making a...   \n",
      "...        ...        ...                                                ...   \n",
      "8856     13602          5  The mood created in Narciso Rodriguez from Hom...   \n",
      "8857     13603          5  It creates a mood of happiness \"but Cuban musi...   \n",
      "8858     13604          5  The mood created by the author in this memoir ...   \n",
      "8859     13605          5  In the memoir \"Narciso Rodriguez\" from \"Home: ...   \n",
      "8860     13606          5  In the memoir \"Narciso Rodriguez\" by Narciso R...   \n",
      "8861     13607          5  In this memoir I think the mood is a warm and ...   \n",
      "8862     13608          5  The mood in this memoir is very relaxed and pr...   \n",
      "8863     13609          5  There are many different ways on how the mood ...   \n",
      "8864     13610          5  The mood created by the memoir is greatfulness...   \n",
      "8865     13611          5  The mood that the author in the memoir created...   \n",
      "8866     13612          5  In the memoir many moods were expressed. I thi...   \n",
      "8867     13613          5  In the story memoir the author created a coupl...   \n",
      "8868     13614          5  In the memoir, Narciso Rodriguez, from Home: T...   \n",
      "8869     13615          5  In this memoir the mood created by the author ...   \n",
      "8870     13616          5  The mood is @CAPS1 in many ways. First, the mo...   \n",
      "8871     13617          5  In the memoir, \"Narciso Rodriguez\" from Home: ...   \n",
      "8872     13618          5  Warmth                                        ...   \n",
      "8873     13619          5  The @CAPS1 sets a good mood in this memoir abo...   \n",
      "8874     13620          5  The mood set in this memoir by the author, was...   \n",
      "8875     13621          5  The mood I think the author created in the mem...   \n",
      "8876     13622          5  The mood created by the author is loving and h...   \n",
      "8877     13623          5  The mood created by the author in the memoir i...   \n",
      "8878     13624          5  The mood created in this memoir was manily for...   \n",
      "8879     13625          5  The mood in the author's story is happy and gr...   \n",
      "8880     13626          5  The mood created by the author in the memoir i...   \n",
      "8881     13627          5  The mood of this memoir is nonfiction. The moo...   \n",
      "8882     13628          5  The mood was created by the author in the memo...   \n",
      "8883     13629          5  In the memoir \"Narciso Rodriguez\", the mood cr...   \n",
      "8884     13630          5  The mood created @CAPS3 the author, Narciso Ro...   \n",
      "8885     13631          5  The author created such a specific mood for th...   \n",
      "\n",
      "      domain1_score  \n",
      "7081            5.0  \n",
      "7082            5.0  \n",
      "7083            7.5  \n",
      "7084            2.5  \n",
      "7085            7.5  \n",
      "7086            7.5  \n",
      "7087            7.5  \n",
      "7088           10.0  \n",
      "7089            7.5  \n",
      "7090            7.5  \n",
      "7091            2.5  \n",
      "7092            2.5  \n",
      "7093            2.5  \n",
      "7094            2.5  \n",
      "7095            5.0  \n",
      "7096            5.0  \n",
      "7097           10.0  \n",
      "7098            7.5  \n",
      "7099            5.0  \n",
      "7100            5.0  \n",
      "7101           10.0  \n",
      "7102            5.0  \n",
      "7103            5.0  \n",
      "7104           10.0  \n",
      "7105            5.0  \n",
      "7106            5.0  \n",
      "7107            5.0  \n",
      "7108            2.5  \n",
      "7109            7.5  \n",
      "7110           10.0  \n",
      "...             ...  \n",
      "8856            5.0  \n",
      "8857            2.5  \n",
      "8858            5.0  \n",
      "8859            7.5  \n",
      "8860            7.5  \n",
      "8861           10.0  \n",
      "8862            5.0  \n",
      "8863            5.0  \n",
      "8864            5.0  \n",
      "8865            5.0  \n",
      "8866            5.0  \n",
      "8867            5.0  \n",
      "8868            5.0  \n",
      "8869            2.5  \n",
      "8870            5.0  \n",
      "8871            7.5  \n",
      "8872            5.0  \n",
      "8873            7.5  \n",
      "8874           10.0  \n",
      "8875            5.0  \n",
      "8876            5.0  \n",
      "8877            5.0  \n",
      "8878            5.0  \n",
      "8879            7.5  \n",
      "8880            7.5  \n",
      "8881            5.0  \n",
      "8882            0.0  \n",
      "8883           10.0  \n",
      "8884            7.5  \n",
      "8885            5.0  \n",
      "\n",
      "[1805 rows x 4 columns],        essay_id  essay_set                                              essay  \\\n",
      "8886      14834          6  There were many obstacles that the builders fa...   \n",
      "8887      14835          6  Him from the start, there would have been many...   \n",
      "8888      14836          6  The builders of the Empire State Building face...   \n",
      "8889      14837          6  In the passage The Mooring Mast by Marcia Amid...   \n",
      "8890      14838          6  The builders of the Empire State Building face...   \n",
      "8891      14839          6  soon after it's conception, The Empire State B...   \n",
      "8892      14840          6  The builders of the Empire @CAPS1 Building wer...   \n",
      "8893      14841          6  In the excerpt \"The Mooring Mast\" by Marcia Am...   \n",
      "8894      14842          6  The obstacles the builders face were great. Th...   \n",
      "8895      14843          6  The builders of the Empire State Building a gr...   \n",
      "8896      14844          6  The Empire State Building's docking zone was a...   \n",
      "8897      14845          6  During the construction of the Empire State Bu...   \n",
      "8898      14846          6  The builders of the Empire State Building face...   \n",
      "8899      14847          6  The builders of the Empire State Building face...   \n",
      "8900      14848          6  The obstacles the builders of the Empire state...   \n",
      "8901      14849          6  The @CAPS1 The Mooring Mast by Marcia Amidon L...   \n",
      "8902      14850          6  In Marcia Amidon LÃ¼sted's excerpt, The Mooring...   \n",
      "8903      14851          6  The builders of the empire state buildings had...   \n",
      "8904      14852          6  The first obstacle that it faced with having t...   \n",
      "8905      14853          6  The obstacles the builders of the Empire State...   \n",
      "8906      14854          6  The builders of the Empire State Building face...   \n",
      "8907      14855          6  The builders of the @ORGANIZATION1 many obstac...   \n",
      "8908      14856          6  In the reading The Mooring Mast written by Mar...   \n",
      "8909      14857          6  The builders of the Empire State Building face...   \n",
      "8910      14858          6  In building the Empire State Building, the bui...   \n",
      "8911      14859          6  The idea of mooring dirigibles to the top of t...   \n",
      "8912      14860          6  In the excerpt \"The Mooring @CAPS1\", by: Marci...   \n",
      "8913      14861          6  The builders of the Empire State Building face...   \n",
      "8914      14862          6  Based on the excerpt \"The Mooring Mast\" by Mar...   \n",
      "8915      14863          6  The builders of the Empire State Building face...   \n",
      "...         ...        ...                                                ...   \n",
      "10656     16604          6  The builders of the Empire State Building face...   \n",
      "10657     16605          6  In the attempt to allow dirigibles to dock at ...   \n",
      "10658     16606          6  Based on the excerpt \"The Mooring mast\" by Mar...   \n",
      "10659     16607          6  In The \"Mooring Mast\" by @ORGANIZATION1 many p...   \n",
      "10660     16608          6  When Designing the dock for dirigibles on the ...   \n",
      "10661     16609          6  The @CAPS1 says the greatest obstacle would be...   \n",
      "10662     16610          6  The builders faced many problems that should h...   \n",
      "10663     16611          6  After finishing the @ORGANIZATION1, architects...   \n",
      "10664     16612          6  There were many obstacles the builder of the E...   \n",
      "10665     16613          6  While constructing the Empire State Building, ...   \n",
      "10666     16614          6  To design a mast for the dirigibles to land wa...   \n",
      "10667     16615          6  There were many obstacles which the builders o...   \n",
      "10668     16616          6  In the excerpt, \"The Mooring Mast\", by Marcia ...   \n",
      "10669     16617          6  The builders of the Empire State Building face...   \n",
      "10670     16618          6  In the @CAPS1, the docking of dirigibles had m...   \n",
      "10671     16619          6  The builders of the empire state building face...   \n",
      "10672     16620          6  In The Mooring Mast by Marcia Amidon LÃ¼sted yo...   \n",
      "10673     16621          6  The builders and architects of the Empire Stat...   \n",
      "10674     16622          6  An obstacle the builders of the Empire State b...   \n",
      "10675     16623          6  The architects had to go through many obstacle...   \n",
      "10676     16624          6  The @CAPS1 of the Empire State Building faced ...   \n",
      "10677     16625          6  The builders of the Empire State Building were...   \n",
      "10678     16626          6  Dirigibles are large steel-frame balloons encl...   \n",
      "10679     16627          6  The builders of the Empire State building had ...   \n",
      "10680     16628          6  There were many obstacles the builders of the ...   \n",
      "10681     16629          6  The one obstacle the builders had when trying ...   \n",
      "10682     16630          6  Some of the problems with the constructing of ...   \n",
      "10683     16631          6  The builders of the Empire State building face...   \n",
      "10684     16632          6  The obstacles the builders of the Empire State...   \n",
      "10685     16633          6  You want me to tell you what they had to go th...   \n",
      "\n",
      "       domain1_score  \n",
      "8886             5.0  \n",
      "8887             7.5  \n",
      "8888            10.0  \n",
      "8889             2.5  \n",
      "8890             7.5  \n",
      "8891            10.0  \n",
      "8892             7.5  \n",
      "8893             7.5  \n",
      "8894             7.5  \n",
      "8895             7.5  \n",
      "8896             7.5  \n",
      "8897            10.0  \n",
      "8898            10.0  \n",
      "8899             7.5  \n",
      "8900             5.0  \n",
      "8901             7.5  \n",
      "8902             2.5  \n",
      "8903             7.5  \n",
      "8904             7.5  \n",
      "8905             5.0  \n",
      "8906             7.5  \n",
      "8907            10.0  \n",
      "8908             5.0  \n",
      "8909            10.0  \n",
      "8910             7.5  \n",
      "8911             5.0  \n",
      "8912             7.5  \n",
      "8913             7.5  \n",
      "8914             7.5  \n",
      "8915             7.5  \n",
      "...              ...  \n",
      "10656           10.0  \n",
      "10657           10.0  \n",
      "10658            2.5  \n",
      "10659            7.5  \n",
      "10660            7.5  \n",
      "10661            2.5  \n",
      "10662            7.5  \n",
      "10663            7.5  \n",
      "10664            7.5  \n",
      "10665            7.5  \n",
      "10666            2.5  \n",
      "10667            5.0  \n",
      "10668           10.0  \n",
      "10669            7.5  \n",
      "10670            2.5  \n",
      "10671            5.0  \n",
      "10672           10.0  \n",
      "10673            7.5  \n",
      "10674            7.5  \n",
      "10675           10.0  \n",
      "10676            7.5  \n",
      "10677            5.0  \n",
      "10678            5.0  \n",
      "10679            7.5  \n",
      "10680            7.5  \n",
      "10681            0.0  \n",
      "10682            5.0  \n",
      "10683            7.5  \n",
      "10684            5.0  \n",
      "10685            5.0  \n",
      "\n",
      "[1800 rows x 4 columns],        essay_id  essay_set                                              essay  \\\n",
      "10686     17834          7  Patience is when your waiting .I was patience ...   \n",
      "10687     17836          7  I am not a patience person, like I canâ€™t sit i...   \n",
      "10688     17837          7  One day I was at basketball practice and I was...   \n",
      "10689     17838          7  I going to write about a time when I went to t...   \n",
      "10690     17839          7  It can be very hard for somebody to be patient...   \n",
      "10691     17840          7  There was a girl name @PERSON1. She loved spen...   \n",
      "10692     17841          7  Un Patience @CAPS1.   My name is @CAPS2 and I ...   \n",
      "10693     17842          7  A time when I was patient was when I preordere...   \n",
      "10694     17843          7  One time I was patience it was when I wanted a...   \n",
      "10695     17844          7  I think patience is a time when you have to be...   \n",
      "10696     17845          7  You know that life is so much harder when you ...   \n",
      "10697     17846          7  One nice sunny day I was traped in a doctors o...   \n",
      "10698     17847          7  A time I was patient was @DATE1, when I was in...   \n",
      "10699     17849          7  One day, my soccer team was in the chamionchip...   \n",
      "10700     17850          7  This is about a story I was patient I was on a...   \n",
      "10701     17851          7  Tick, tock, tick, tock. Being patient is hard ...   \n",
      "10702     17852          7  One day @CAPS1 went to school pretty earlie in...   \n",
      "10703     17853          7  I recall once a famous madican named @PERSON1 ...   \n",
      "10704     17854          7  One day, a few years ago, I woke up and my mom...   \n",
      "10705     17856          7  The time that I was patient was not long ago. ...   \n",
      "10706     17857          7  One day I was patient was when I tried out for...   \n",
      "10707     17858          7  A time that I was patient was last year at che...   \n",
      "10708     17859          7  Im writing about the time I was patient at a @...   \n",
      "10709     17860          7  Being patient? Being patient is very hard for ...   \n",
      "10710     17861          7  One when I was patient was when we were going ...   \n",
      "10711     17862          7  Patients is verey important. But I m not verey...   \n",
      "10712     17863          7  Patience is when you can take your time at som...   \n",
      "10713     17864          7  When I was patient. Was when I go down the riv...   \n",
      "10714     17865          7  Patience? I am not a patient type of person. I...   \n",
      "10715     17867          7  I am not a patient person at all. But sometime...   \n",
      "...         ...        ...                                                ...   \n",
      "12225     19529          7  â€œ@CAPS1!â€ @CAPS2 whined, her voice escalating ...   \n",
      "12226     19530          7  I was patient about getting my phone. I had to...   \n",
      "12227     19533          7  A time when I was patient I was patient on @CA...   \n",
      "12228     19534          7  One time me and my family were at @CAPS1 point...   \n",
      "12229     19535          7  I was patient once when my dad had said he was...   \n",
      "12230     19536          7  I was patient one time, really patient but kin...   \n",
      "12231     19537          7  Once my friend @PERSON1 was very patient,he wa...   \n",
      "12232     19538          7  The patient player. Once when I was four and m...   \n",
      "12233     19539          7  When I had to go get my check up with my asma ...   \n",
      "12234     19540          7  When my mom, @PERSON1 and I went to @ORGANIZAT...   \n",
      "12235     19541          7  A time when I was patient was when @DATE1 we d...   \n",
      "12236     19542          7  Not long ago I went to the pumpkin patch with ...   \n",
      "12237     19543          7  Will I was patiently waiting for my little sis...   \n",
      "12238     19544          7  At the @CAPS1â€™s office. I had an @CAPS2 appoin...   \n",
      "12239     19545          7  Patient people are the people that tolerate li...   \n",
      "12240     19546          7  In the @DATE1 time my mom had planned of trip ...   \n",
      "12241     19547          7  A time I was patient was when I was at a denti...   \n",
      "12242     19548          7  On a fine @DATE1 day I was heading toward the ...   \n",
      "12243     19549          7  When I was and the @NUM1 grade and I had to pu...   \n",
      "12244     19550          7  My friend an I were wateng to go no a new ride...   \n",
      "12245     19553          7  One early @TIME1 in @LOCATION2, @LOCATION1 for...   \n",
      "12246     19554          7  @CAPS1, @CAPS1 is a huge past of life to be ??...   \n",
      "12247     19555          7  To say really Iâ€™m not a very patient person, b...   \n",
      "12248     19556          7  One day, my mom was really patient. We were ou...   \n",
      "12249     19557          7  Awhile back my friends and I made a @CAPS2 gam...   \n",
      "12250     19558          7  One time I was getting a cool @CAPS1 game it w...   \n",
      "12251     19559          7  A patent person in my life is my mom. Aicason ...   \n",
      "12252     19561          7  A time when someone else I know was patient wa...   \n",
      "12253     19562          7  I hate weddings. I love when people get marrie...   \n",
      "12254     19563          7  A few weeks ago, we had a garage sale and a mo...   \n",
      "\n",
      "       domain1_score  \n",
      "10686            6.0  \n",
      "10687            5.2  \n",
      "10688            6.0  \n",
      "10689            6.8  \n",
      "10690            5.2  \n",
      "10691            9.2  \n",
      "10692            6.4  \n",
      "10693            7.2  \n",
      "10694            4.8  \n",
      "10695            4.0  \n",
      "10696            6.4  \n",
      "10697            7.6  \n",
      "10698            6.8  \n",
      "10699            5.6  \n",
      "10700            4.8  \n",
      "10701            6.4  \n",
      "10702            6.8  \n",
      "10703            6.4  \n",
      "10704            8.4  \n",
      "10705            5.6  \n",
      "10706            7.2  \n",
      "10707            9.6  \n",
      "10708            6.8  \n",
      "10709            6.4  \n",
      "10710            6.4  \n",
      "10711            3.6  \n",
      "10712            2.4  \n",
      "10713            4.0  \n",
      "10714            6.8  \n",
      "10715            6.4  \n",
      "...              ...  \n",
      "12225            9.6  \n",
      "12226            9.6  \n",
      "12227            5.6  \n",
      "12228            7.2  \n",
      "12229            6.4  \n",
      "12230            6.4  \n",
      "12231            5.2  \n",
      "12232            5.6  \n",
      "12233            5.6  \n",
      "12234            6.4  \n",
      "12235            5.2  \n",
      "12236            6.0  \n",
      "12237            6.4  \n",
      "12238            3.6  \n",
      "12239            6.0  \n",
      "12240            6.4  \n",
      "12241            7.2  \n",
      "12242            7.2  \n",
      "12243            3.2  \n",
      "12244            3.2  \n",
      "12245            7.6  \n",
      "12246            5.6  \n",
      "12247            7.2  \n",
      "12248            7.6  \n",
      "12249            9.6  \n",
      "12250            4.8  \n",
      "12251            6.4  \n",
      "12252            7.6  \n",
      "12253            8.8  \n",
      "12254            6.0  \n",
      "\n",
      "[1569 rows x 4 columns],        essay_id  essay_set                                              essay  \\\n",
      "12255     20716          8   A long time ago when I was in third grade I h...   \n",
      "12256     20717          8   Softball has to be one of the single most gre...   \n",
      "12257     20718          8   Some people like making people laugh, I love ...   \n",
      "12258     20719          8   \"LAUGHTER\"  @CAPS1 I hang out with my friends...   \n",
      "12259     20721          8  Well ima tell a story about the time i got @CA...   \n",
      "12260     20722          8  @CAPS3 I'm going to tell @CAPS18 a little stor...   \n",
      "12261     20723          8   My best friend, @PERSON1 and I have been clos...   \n",
      "12262     20724          8    Laughter is a really good thing to have in a...   \n",
      "12263     20725          8   I believe that with all people laughter, and ...   \n",
      "12264     20726          8   Laughter, one of life's greatest gifts. Have ...   \n",
      "12265     20727          8   Laugher Laughter is to express delight, fun, ...   \n",
      "12266     20728          8   Starting a story out with two @NUM1 @DATE1 ol...   \n",
      "12267     20732          8  When i meet new people its like alright cause ...   \n",
      "12268     20733          8   People always say laughter is a big part in a...   \n",
      "12269     20734          8      \"Laughter is the best medicine.\" The phras...   \n",
      "12270     20735          8   one time when i was skateboarding with my fri...   \n",
      "12271     20736          8                 \"Laughter\" A good relationship ...   \n",
      "12272     20737          8   Laughter. A story about laughter is what you ...   \n",
      "12273     20739          8   Laughter is a big part in my relationship. I ...   \n",
      "12274     20741          8   Everyone enjoys laughter. Everywhere you look...   \n",
      "12275     20742          8   Why is it that laughter can bring people so c...   \n",
      "12276     20743          8   \"@CAPS1 figure something out to do! I am real...   \n",
      "12277     20745          8  Interesting fly.It was the @DATE1 of this @DAT...   \n",
      "12278     20746          8   In many relationships I have had, there has a...   \n",
      "12279     20747          8   These days you don't really need a reason to ...   \n",
      "12280     20748          8  Friends are'nt always what we want them to be ...   \n",
      "12281     20751          8    It was a normal bright and warm @DATE1 day, ...   \n",
      "12282     20754          8  Making someone laugh is the best feeling to th...   \n",
      "12283     20755          8   \"Laughter is the key to the soul.\" @CAPS1 you...   \n",
      "12284     20756          8   Their are many joys in life, some are more re...   \n",
      "...         ...        ...                                                ...   \n",
      "12948     21592          8   We all understand the benefits of laughter. L...   \n",
      "12949     21594          8        It was midsummer, and i could feel the c...   \n",
      "12950     21595          8   Have you ever experienced a time with your fr...   \n",
      "12951     21596          8   I woke up just like any other day happy yet l...   \n",
      "12952     21598          8   Laughter is an important part of my life, eit...   \n",
      "12953     21599          8   I sat at the table, speechless, as they told ...   \n",
      "12954     21601          8   As I remember back, it was @DATE1. It was a h...   \n",
      "12955     21603          8   Those eyes, it was like I was looking out int...   \n",
      "12956     21604          8  Some say that laugh is the common language bet...   \n",
      "12957     21605          8   Laughter is an integral element to many situa...   \n",
      "12958     21606          8  One time I was at my friend @PERSON1's house, ...   \n",
      "12959     21607          8   LAUGHTER @CAPS1 knows that laughter is a heal...   \n",
      "12960     21608          8  One thing that people in the world love to do ...   \n",
      "12961     21609          8   Laughter, to me, is an important aspect of my...   \n",
      "12962     21610          8   People always say that the worst parts of lif...   \n",
      "12963     21611          8   Why is it that people can look back at someth...   \n",
      "12964     21613          8   Before my best friend moved away, we would st...   \n",
      "12965     21615          8                                @ORGANIZATION1  ...   \n",
      "12966     21617          8   Morose and somnolent, I woke up. I woke up to...   \n",
      "12967     21618          8   A while back my mom had decided to send me to...   \n",
      "12968     21619          8                              I dont like computers   \n",
      "12969     21620          8   Everyone knows how important a laugh can be. ...   \n",
      "12970     21621          8   Laughter is an important part of my family. W...   \n",
      "12971     21623          8   laughter is an important part of any kind of ...   \n",
      "12972     21624          8  Sometime ago on a hot @DATE1 day my @NUM1 ,@PE...   \n",
      "12973     21626          8   In most stories mothers and daughters are eit...   \n",
      "12974     21628          8   I never understood the meaning laughter is th...   \n",
      "12975     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
      "12976     21630          8                                 Trippin' on fen...   \n",
      "12977     21633          8   Many people believe that laughter can improve...   \n",
      "\n",
      "       domain1_score  \n",
      "12255       5.666667  \n",
      "12256       7.666667  \n",
      "12257       6.666667  \n",
      "12258       5.000000  \n",
      "12259       4.333333  \n",
      "12260       6.666667  \n",
      "12261       6.666667  \n",
      "12262       5.666667  \n",
      "12263       6.833333  \n",
      "12264       6.666667  \n",
      "12265       5.166667  \n",
      "12266       7.333333  \n",
      "12267       5.166667  \n",
      "12268       6.000000  \n",
      "12269       7.166667  \n",
      "12270       3.333333  \n",
      "12271       7.500000  \n",
      "12272       6.666667  \n",
      "12273       5.000000  \n",
      "12274       5.833333  \n",
      "12275       6.666667  \n",
      "12276       7.000000  \n",
      "12277       5.666667  \n",
      "12278       5.500000  \n",
      "12279       6.333333  \n",
      "12280       6.000000  \n",
      "12281       7.500000  \n",
      "12282       5.166667  \n",
      "12283       5.000000  \n",
      "12284       5.666667  \n",
      "...              ...  \n",
      "12948       6.666667  \n",
      "12949       5.333333  \n",
      "12950       6.000000  \n",
      "12951       5.166667  \n",
      "12952       5.000000  \n",
      "12953       7.833333  \n",
      "12954       6.666667  \n",
      "12955       5.833333  \n",
      "12956       5.500000  \n",
      "12957       6.000000  \n",
      "12958       6.000000  \n",
      "12959       8.000000  \n",
      "12960       6.666667  \n",
      "12961       6.666667  \n",
      "12962       6.666667  \n",
      "12963       7.000000  \n",
      "12964       6.666667  \n",
      "12965       5.333333  \n",
      "12966       6.000000  \n",
      "12967       6.666667  \n",
      "12968       1.666667  \n",
      "12969       5.500000  \n",
      "12970       7.333333  \n",
      "12971       5.833333  \n",
      "12972       5.000000  \n",
      "12973       5.833333  \n",
      "12974       5.333333  \n",
      "12975       6.666667  \n",
      "12976       6.666667  \n",
      "12977       6.666667  \n",
      "\n",
      "[723 rows x 4 columns]]\n"
     ]
    }
   ],
   "source": [
    "Set=[]\n",
    "\n",
    "for i in range(1,9):\n",
    "    Set.append(X[X['essay_set']==i]) \n",
    "print (Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential([\n",
    "        # 2D tensor for first layer: 1 timestep and 300 features\n",
    "        LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True),\n",
    "        LSTM(64, recurrent_dropout=0.4),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def essay_to_list(essay):\n",
    "    # Remove the tags\n",
    "    essay = re.sub(\"[^a-zA-Z]\", \" \", essay)\n",
    "    words = essay.lower().split()\n",
    "    return [w for w in words if not w in stopwords]\n",
    "\n",
    "def essay_to_sentences(essay):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_list(raw_sentence))\n",
    "    return sentences\n",
    "\n",
    "# Generate feature vector for the words\n",
    "def get_feature_vector(words, model, num_features, vec_type=\"sum\"):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    max_vec =  np.zeros((num_features,),dtype=\"float32\")\n",
    "    min_vec =  np.ones((num_features,),dtype=\"float32\")\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            max_vec = np.maximum(model[word], feature_vector)\n",
    "            min_vec = np.minimum(model[word], feature_vector)\n",
    "            feature_vector = np.add(feature_vector, model[word]) \n",
    "    \n",
    "    # return min vector + max vector\n",
    "    if vec_type == \"min+max\":\n",
    "        return np.add(min_vec, max_vec) \n",
    "    \n",
    "    # average of vectors\n",
    "    elif vec_type == \"average\":\n",
    "        return np.divide(feature_vector, num_words)\n",
    "\n",
    "    # default: return sum of word2vec vectors\n",
    "    return feature_vector\n",
    "\n",
    "# Generate word vectors from the mdoel\n",
    "def generate_essay_vectors(essays, model, num_features, vec_type=\"sum\"):\n",
    "    essayfeature_vectors = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for (i, essay) in enumerate(essays):\n",
    "        essayfeature_vectors[i] = get_feature_vector(essay, model, num_features, vec_type)\n",
    "    return essayfeature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_mse = []\n",
    "def train_model(X, y, dataset, vec_type=\"sum\"):\n",
    "    count = 1\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for train_set, test_set in dataset:\n",
    "        print(\"Fold #\", count)\n",
    "        X_test, X_train, y_test, y_train = X.iloc[test_set], X.iloc[train_set], y.iloc[test_set], y.iloc[train_set]\n",
    "        \n",
    "        train_essays = X_train['essay']\n",
    "        test_essays = X_test['essay']\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        for essay in train_essays:\n",
    "            sentences += essay_to_sentences(essay)\n",
    "                \n",
    "        # Initialize variables for word2vec model\n",
    "        num_features = 300 \n",
    "        min_word_count = 40\n",
    "        num_workers = 4\n",
    "        context = 10\n",
    "        downsampling = 1e-7\n",
    "\n",
    "        # Train the word2vec model\n",
    "        model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "        model.init_sims(replace=True)\n",
    "        \n",
    "        # Generate training vectors\n",
    "        clean_train_essays = []\n",
    "        for essay_vec in train_essays:\n",
    "            clean_train_essays.append(essay_to_list(essay_vec))\n",
    "        train_vectors = generate_essay_vectors(clean_train_essays, model, num_features, vec_type)\n",
    "        \n",
    "        # Generate test vectors\n",
    "        clean_test_essays = []\n",
    "        for essay_vec in test_essays:\n",
    "            clean_test_essays.append(essay_to_list( essay_vec))\n",
    "        test_vectors = generate_essay_vectors(clean_test_essays, model, num_features, vec_type)\n",
    "        \n",
    "        train_vectors = np.array(train_vectors)\n",
    "        test_vectors = np.array(test_vectors)\n",
    "\n",
    "        # Reshape the train and test vectors to 3 dimensions - 1 represents one timestamp \n",
    "        train_vectors = np.reshape(train_vectors, (train_vectors.shape[0], 1, train_vectors.shape[1]))\n",
    "        test_vectors = np.reshape(test_vectors, (test_vectors.shape[0], 1, test_vectors.shape[1]))\n",
    "        \n",
    "        # Call the LSTM to get the score predictions \n",
    "        lstm_model = get_model()\n",
    "        lstm_model.fit(train_vectors, y_train, batch_size=64, epochs=50)\n",
    "        y_pred = lstm_model.predict(test_vectors)\n",
    "        \n",
    "        # Round the prediction to the nearest integer\n",
    "        y_pred = np.around(y_pred)\n",
    "        \n",
    "        # Evaluate the model: quadratic kappa score of predictions against human grading\n",
    "        result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "        print(\"QWK: \", result)\n",
    "        results.append(result)\n",
    "        \n",
    "        LSTM_mse.append( round(metrics.mean_squared_error(y_test.values,y_pred),4 ) )\n",
    "        count += 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/ayushi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_103 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_104 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1423/1423 [==============================] - 10s 7ms/step - loss: 8.9540 - mean_absolute_error: 2.1833\n",
      "Epoch 2/50\n",
      "1423/1423 [==============================] - 1s 560us/step - loss: 2.0374 - mean_absolute_error: 1.1259\n",
      "Epoch 3/50\n",
      "1423/1423 [==============================] - 1s 566us/step - loss: 1.7954 - mean_absolute_error: 1.0601\n",
      "Epoch 4/50\n",
      "1423/1423 [==============================] - 1s 608us/step - loss: 1.7369 - mean_absolute_error: 1.0418\n",
      "Epoch 5/50\n",
      "1423/1423 [==============================] - 1s 581us/step - loss: 1.7266 - mean_absolute_error: 1.0422\n",
      "Epoch 6/50\n",
      "1423/1423 [==============================] - 1s 665us/step - loss: 1.6190 - mean_absolute_error: 0.9893\n",
      "Epoch 7/50\n",
      "1423/1423 [==============================] - 1s 683us/step - loss: 1.6072 - mean_absolute_error: 0.9944\n",
      "Epoch 8/50\n",
      "1423/1423 [==============================] - 1s 600us/step - loss: 1.5163 - mean_absolute_error: 0.9703\n",
      "Epoch 9/50\n",
      "1423/1423 [==============================] - 1s 578us/step - loss: 1.5820 - mean_absolute_error: 0.9883\n",
      "Epoch 10/50\n",
      "1423/1423 [==============================] - 1s 593us/step - loss: 1.4573 - mean_absolute_error: 0.9485\n",
      "Epoch 11/50\n",
      "1423/1423 [==============================] - 1s 589us/step - loss: 1.4175 - mean_absolute_error: 0.9466\n",
      "Epoch 12/50\n",
      "1423/1423 [==============================] - 1s 579us/step - loss: 1.4333 - mean_absolute_error: 0.9300\n",
      "Epoch 13/50\n",
      "1423/1423 [==============================] - 1s 580us/step - loss: 1.4809 - mean_absolute_error: 0.9714\n",
      "Epoch 14/50\n",
      "1423/1423 [==============================] - 1s 582us/step - loss: 1.4906 - mean_absolute_error: 0.9650\n",
      "Epoch 15/50\n",
      "1423/1423 [==============================] - 1s 578us/step - loss: 1.4252 - mean_absolute_error: 0.9432\n",
      "Epoch 16/50\n",
      "1423/1423 [==============================] - 1s 586us/step - loss: 1.3619 - mean_absolute_error: 0.9085\n",
      "Epoch 17/50\n",
      "1423/1423 [==============================] - 1s 588us/step - loss: 1.3848 - mean_absolute_error: 0.9220\n",
      "Epoch 18/50\n",
      "1423/1423 [==============================] - 1s 585us/step - loss: 1.4293 - mean_absolute_error: 0.9365\n",
      "Epoch 19/50\n",
      "1423/1423 [==============================] - 1s 586us/step - loss: 1.3893 - mean_absolute_error: 0.9255\n",
      "Epoch 20/50\n",
      "1423/1423 [==============================] - 1s 593us/step - loss: 1.4088 - mean_absolute_error: 0.9325\n",
      "Epoch 21/50\n",
      "1423/1423 [==============================] - 1s 588us/step - loss: 1.3512 - mean_absolute_error: 0.9185\n",
      "Epoch 22/50\n",
      "1423/1423 [==============================] - 1s 594us/step - loss: 1.2618 - mean_absolute_error: 0.8886\n",
      "Epoch 23/50\n",
      "1423/1423 [==============================] - 1s 594us/step - loss: 1.3184 - mean_absolute_error: 0.8963\n",
      "Epoch 24/50\n",
      "1423/1423 [==============================] - 1s 587us/step - loss: 1.3868 - mean_absolute_error: 0.9135\n",
      "Epoch 25/50\n",
      "1423/1423 [==============================] - 1s 592us/step - loss: 1.2537 - mean_absolute_error: 0.8798\n",
      "Epoch 26/50\n",
      "1423/1423 [==============================] - 1s 592us/step - loss: 1.3068 - mean_absolute_error: 0.9041\n",
      "Epoch 27/50\n",
      "1423/1423 [==============================] - 1s 591us/step - loss: 1.2030 - mean_absolute_error: 0.8702\n",
      "Epoch 28/50\n",
      "1423/1423 [==============================] - 1s 590us/step - loss: 1.2661 - mean_absolute_error: 0.8807\n",
      "Epoch 29/50\n",
      "1423/1423 [==============================] - 1s 591us/step - loss: 1.2987 - mean_absolute_error: 0.8990\n",
      "Epoch 30/50\n",
      "1423/1423 [==============================] - 1s 601us/step - loss: 1.2586 - mean_absolute_error: 0.8884\n",
      "Epoch 31/50\n",
      "1423/1423 [==============================] - 1s 604us/step - loss: 1.1527 - mean_absolute_error: 0.8414\n",
      "Epoch 32/50\n",
      "1423/1423 [==============================] - 1s 585us/step - loss: 1.1966 - mean_absolute_error: 0.8585\n",
      "Epoch 33/50\n",
      "1423/1423 [==============================] - 1s 598us/step - loss: 1.1830 - mean_absolute_error: 0.8550\n",
      "Epoch 34/50\n",
      "1423/1423 [==============================] - 1s 588us/step - loss: 1.1623 - mean_absolute_error: 0.8570\n",
      "Epoch 35/50\n",
      "1423/1423 [==============================] - 1s 609us/step - loss: 1.1428 - mean_absolute_error: 0.8324\n",
      "Epoch 36/50\n",
      "1423/1423 [==============================] - 1s 605us/step - loss: 1.0861 - mean_absolute_error: 0.8249\n",
      "Epoch 37/50\n",
      "1423/1423 [==============================] - 1s 604us/step - loss: 1.1299 - mean_absolute_error: 0.8286\n",
      "Epoch 38/50\n",
      "1423/1423 [==============================] - 1s 595us/step - loss: 1.1555 - mean_absolute_error: 0.8429\n",
      "Epoch 39/50\n",
      "1423/1423 [==============================] - 1s 593us/step - loss: 1.1481 - mean_absolute_error: 0.8304\n",
      "Epoch 40/50\n",
      "1423/1423 [==============================] - 1s 599us/step - loss: 1.0484 - mean_absolute_error: 0.8054\n",
      "Epoch 41/50\n",
      "1423/1423 [==============================] - 1s 575us/step - loss: 1.1328 - mean_absolute_error: 0.8238\n",
      "Epoch 42/50\n",
      "1423/1423 [==============================] - 1s 570us/step - loss: 1.0682 - mean_absolute_error: 0.8099\n",
      "Epoch 43/50\n",
      "1423/1423 [==============================] - 1s 571us/step - loss: 0.9644 - mean_absolute_error: 0.7709\n",
      "Epoch 44/50\n",
      "1423/1423 [==============================] - 1s 571us/step - loss: 1.0009 - mean_absolute_error: 0.7761\n",
      "Epoch 45/50\n",
      "1423/1423 [==============================] - 1s 566us/step - loss: 1.0857 - mean_absolute_error: 0.8083\n",
      "Epoch 46/50\n",
      "1423/1423 [==============================] - 1s 568us/step - loss: 1.0303 - mean_absolute_error: 0.8041\n",
      "Epoch 47/50\n",
      "1423/1423 [==============================] - 1s 568us/step - loss: 0.9717 - mean_absolute_error: 0.7756\n",
      "Epoch 48/50\n",
      "1423/1423 [==============================] - 1s 572us/step - loss: 0.9264 - mean_absolute_error: 0.7585\n",
      "Epoch 49/50\n",
      "1423/1423 [==============================] - 1s 568us/step - loss: 1.0087 - mean_absolute_error: 0.7897\n",
      "Epoch 50/50\n",
      "1423/1423 [==============================] - 1s 575us/step - loss: 0.9610 - mean_absolute_error: 0.7723\n",
      "QWK:  0.679916149319\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_105 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_106 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1423/1423 [==============================] - 10s 7ms/step - loss: 8.3885 - mean_absolute_error: 2.1208\n",
      "Epoch 2/50\n",
      "1423/1423 [==============================] - 1s 603us/step - loss: 2.0152 - mean_absolute_error: 1.1138\n",
      "Epoch 3/50\n",
      "1423/1423 [==============================] - 1s 592us/step - loss: 1.8090 - mean_absolute_error: 1.0639\n",
      "Epoch 4/50\n",
      "1423/1423 [==============================] - 1s 575us/step - loss: 1.6973 - mean_absolute_error: 1.0299\n",
      "Epoch 5/50\n",
      "1423/1423 [==============================] - 1s 625us/step - loss: 1.6091 - mean_absolute_error: 1.0116\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1423/1423 [==============================] - 1s 578us/step - loss: 1.5969 - mean_absolute_error: 0.9957\n",
      "Epoch 7/50\n",
      "1423/1423 [==============================] - 1s 695us/step - loss: 1.6172 - mean_absolute_error: 1.0016\n",
      "Epoch 8/50\n",
      "1423/1423 [==============================] - 1s 573us/step - loss: 1.5597 - mean_absolute_error: 0.9882\n",
      "Epoch 9/50\n",
      "1423/1423 [==============================] - 1s 595us/step - loss: 1.5367 - mean_absolute_error: 0.9766\n",
      "Epoch 10/50\n",
      "1423/1423 [==============================] - 1s 659us/step - loss: 1.5467 - mean_absolute_error: 0.9778\n",
      "Epoch 11/50\n",
      "1423/1423 [==============================] - 1s 596us/step - loss: 1.4765 - mean_absolute_error: 0.9518\n",
      "Epoch 12/50\n",
      "1423/1423 [==============================] - 1s 600us/step - loss: 1.4404 - mean_absolute_error: 0.9370\n",
      "Epoch 13/50\n",
      "1423/1423 [==============================] - 1s 675us/step - loss: 1.4028 - mean_absolute_error: 0.9328\n",
      "Epoch 14/50\n",
      "1423/1423 [==============================] - 1s 643us/step - loss: 1.4127 - mean_absolute_error: 0.9309\n",
      "Epoch 15/50\n",
      "1423/1423 [==============================] - 1s 651us/step - loss: 1.3800 - mean_absolute_error: 0.9246\n",
      "Epoch 16/50\n",
      "1423/1423 [==============================] - 1s 587us/step - loss: 1.4472 - mean_absolute_error: 0.9402\n",
      "Epoch 17/50\n",
      "1423/1423 [==============================] - 1s 595us/step - loss: 1.4535 - mean_absolute_error: 0.9465\n",
      "Epoch 18/50\n",
      "1423/1423 [==============================] - 1s 620us/step - loss: 1.3606 - mean_absolute_error: 0.9045\n",
      "Epoch 19/50\n",
      "1423/1423 [==============================] - 1s 668us/step - loss: 1.3550 - mean_absolute_error: 0.9113\n",
      "Epoch 20/50\n",
      "1423/1423 [==============================] - 1s 571us/step - loss: 1.3486 - mean_absolute_error: 0.9168\n",
      "Epoch 21/50\n",
      "1423/1423 [==============================] - 1s 688us/step - loss: 1.3693 - mean_absolute_error: 0.9238\n",
      "Epoch 22/50\n",
      "1423/1423 [==============================] - 1s 798us/step - loss: 1.3301 - mean_absolute_error: 0.9009\n",
      "Epoch 23/50\n",
      "1423/1423 [==============================] - 1s 721us/step - loss: 1.3406 - mean_absolute_error: 0.9097\n",
      "Epoch 24/50\n",
      "1423/1423 [==============================] - 1s 608us/step - loss: 1.3629 - mean_absolute_error: 0.9239\n",
      "Epoch 25/50\n",
      "1423/1423 [==============================] - 1s 587us/step - loss: 1.2846 - mean_absolute_error: 0.8953\n",
      "Epoch 26/50\n",
      "1423/1423 [==============================] - 1s 584us/step - loss: 1.2749 - mean_absolute_error: 0.8904\n",
      "Epoch 27/50\n",
      "1423/1423 [==============================] - 1s 583us/step - loss: 1.2932 - mean_absolute_error: 0.8922\n",
      "Epoch 28/50\n",
      "1423/1423 [==============================] - 1s 586us/step - loss: 1.2153 - mean_absolute_error: 0.8692\n",
      "Epoch 29/50\n",
      "1423/1423 [==============================] - 1s 579us/step - loss: 1.1666 - mean_absolute_error: 0.8436\n",
      "Epoch 30/50\n",
      "1423/1423 [==============================] - 1s 607us/step - loss: 1.2015 - mean_absolute_error: 0.8640\n",
      "Epoch 31/50\n",
      "1423/1423 [==============================] - 1s 596us/step - loss: 1.1848 - mean_absolute_error: 0.8518\n",
      "Epoch 32/50\n",
      "1423/1423 [==============================] - 1s 591us/step - loss: 1.1924 - mean_absolute_error: 0.8502\n",
      "Epoch 33/50\n",
      "1423/1423 [==============================] - 1s 589us/step - loss: 1.2674 - mean_absolute_error: 0.8799\n",
      "Epoch 34/50\n",
      "1423/1423 [==============================] - 1s 585us/step - loss: 1.2451 - mean_absolute_error: 0.8735\n",
      "Epoch 35/50\n",
      "1423/1423 [==============================] - 1s 590us/step - loss: 1.1018 - mean_absolute_error: 0.8180\n",
      "Epoch 36/50\n",
      "1423/1423 [==============================] - 1s 584us/step - loss: 1.0625 - mean_absolute_error: 0.8052\n",
      "Epoch 37/50\n",
      "1423/1423 [==============================] - 1s 590us/step - loss: 1.1427 - mean_absolute_error: 0.8393\n",
      "Epoch 38/50\n",
      "1423/1423 [==============================] - 1s 587us/step - loss: 1.0948 - mean_absolute_error: 0.8142\n",
      "Epoch 39/50\n",
      "1423/1423 [==============================] - 1s 585us/step - loss: 1.1558 - mean_absolute_error: 0.8434\n",
      "Epoch 40/50\n",
      "1423/1423 [==============================] - 1s 600us/step - loss: 1.0742 - mean_absolute_error: 0.8173\n",
      "Epoch 41/50\n",
      "1423/1423 [==============================] - 1s 590us/step - loss: 1.0694 - mean_absolute_error: 0.8002\n",
      "Epoch 42/50\n",
      "1423/1423 [==============================] - 1s 585us/step - loss: 1.0659 - mean_absolute_error: 0.8028\n",
      "Epoch 43/50\n",
      "1423/1423 [==============================] - 1s 605us/step - loss: 1.0068 - mean_absolute_error: 0.7987\n",
      "Epoch 44/50\n",
      "1423/1423 [==============================] - 1s 595us/step - loss: 1.0956 - mean_absolute_error: 0.8312\n",
      "Epoch 45/50\n",
      "1423/1423 [==============================] - 1s 601us/step - loss: 1.0281 - mean_absolute_error: 0.7945\n",
      "Epoch 46/50\n",
      "1423/1423 [==============================] - 1s 590us/step - loss: 1.1412 - mean_absolute_error: 0.8358\n",
      "Epoch 47/50\n",
      "1423/1423 [==============================] - 1s 588us/step - loss: 1.0316 - mean_absolute_error: 0.8048\n",
      "Epoch 48/50\n",
      "1423/1423 [==============================] - 1s 589us/step - loss: 0.9573 - mean_absolute_error: 0.7640\n",
      "Epoch 49/50\n",
      "1423/1423 [==============================] - 1s 594us/step - loss: 1.0722 - mean_absolute_error: 0.8190\n",
      "Epoch 50/50\n",
      "1423/1423 [==============================] - 1s 594us/step - loss: 0.9848 - mean_absolute_error: 0.7787\n",
      "QWK:  0.695515616821\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_107 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_108 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1428/1428 [==============================] - 11s 8ms/step - loss: 8.3900 - mean_absolute_error: 2.1074\n",
      "Epoch 2/50\n",
      "1428/1428 [==============================] - 1s 593us/step - loss: 1.9571 - mean_absolute_error: 1.0904 0s - loss: 2.0650 - mean_absolute_erro\n",
      "Epoch 3/50\n",
      "1428/1428 [==============================] - 1s 593us/step - loss: 1.6950 - mean_absolute_error: 1.0357\n",
      "Epoch 4/50\n",
      "1428/1428 [==============================] - 1s 575us/step - loss: 1.6453 - mean_absolute_error: 1.0043\n",
      "Epoch 5/50\n",
      "1428/1428 [==============================] - 1s 598us/step - loss: 1.6786 - mean_absolute_error: 1.0147\n",
      "Epoch 6/50\n",
      "1428/1428 [==============================] - 1s 594us/step - loss: 1.5183 - mean_absolute_error: 0.9611\n",
      "Epoch 7/50\n",
      "1428/1428 [==============================] - 1s 579us/step - loss: 1.5606 - mean_absolute_error: 1.0012\n",
      "Epoch 8/50\n",
      "1428/1428 [==============================] - 1s 592us/step - loss: 1.5268 - mean_absolute_error: 0.9750\n",
      "Epoch 9/50\n",
      "1428/1428 [==============================] - 1s 584us/step - loss: 1.5318 - mean_absolute_error: 0.9713\n",
      "Epoch 10/50\n",
      "1428/1428 [==============================] - 1s 593us/step - loss: 1.4862 - mean_absolute_error: 0.9595\n",
      "Epoch 11/50\n",
      "1428/1428 [==============================] - 1s 583us/step - loss: 1.4000 - mean_absolute_error: 0.9365\n",
      "Epoch 12/50\n",
      "1428/1428 [==============================] - 1s 592us/step - loss: 1.3598 - mean_absolute_error: 0.9105\n",
      "Epoch 13/50\n",
      "1428/1428 [==============================] - 1s 596us/step - loss: 1.4497 - mean_absolute_error: 0.9462\n",
      "Epoch 14/50\n",
      "1428/1428 [==============================] - 1s 583us/step - loss: 1.3900 - mean_absolute_error: 0.9338\n",
      "Epoch 15/50\n",
      "1428/1428 [==============================] - 1s 587us/step - loss: 1.3582 - mean_absolute_error: 0.9287\n",
      "Epoch 16/50\n",
      "1428/1428 [==============================] - 1s 585us/step - loss: 1.3742 - mean_absolute_error: 0.9213\n",
      "Epoch 17/50\n",
      "1428/1428 [==============================] - 1s 586us/step - loss: 1.3540 - mean_absolute_error: 0.9087\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428/1428 [==============================] - 1s 568us/step - loss: 1.3717 - mean_absolute_error: 0.9257\n",
      "Epoch 19/50\n",
      "1428/1428 [==============================] - 1s 556us/step - loss: 1.3944 - mean_absolute_error: 0.9309\n",
      "Epoch 20/50\n",
      "1428/1428 [==============================] - 1s 553us/step - loss: 1.2250 - mean_absolute_error: 0.8761\n",
      "Epoch 21/50\n",
      "1428/1428 [==============================] - 1s 558us/step - loss: 1.2954 - mean_absolute_error: 0.8951\n",
      "Epoch 22/50\n",
      "1428/1428 [==============================] - 1s 554us/step - loss: 1.3800 - mean_absolute_error: 0.9240\n",
      "Epoch 23/50\n",
      "1428/1428 [==============================] - 1s 562us/step - loss: 1.2405 - mean_absolute_error: 0.8694\n",
      "Epoch 24/50\n",
      "1428/1428 [==============================] - 1s 570us/step - loss: 1.2456 - mean_absolute_error: 0.8905\n",
      "Epoch 25/50\n",
      "1428/1428 [==============================] - 1s 573us/step - loss: 1.2613 - mean_absolute_error: 0.8794\n",
      "Epoch 26/50\n",
      "1428/1428 [==============================] - 1s 550us/step - loss: 1.2686 - mean_absolute_error: 0.8781\n",
      "Epoch 27/50\n",
      "1428/1428 [==============================] - 1s 572us/step - loss: 1.1938 - mean_absolute_error: 0.8615\n",
      "Epoch 28/50\n",
      "1428/1428 [==============================] - 1s 563us/step - loss: 1.1722 - mean_absolute_error: 0.8552\n",
      "Epoch 29/50\n",
      "1428/1428 [==============================] - 1s 555us/step - loss: 1.1934 - mean_absolute_error: 0.8550\n",
      "Epoch 30/50\n",
      "1428/1428 [==============================] - 1s 554us/step - loss: 1.1622 - mean_absolute_error: 0.8510\n",
      "Epoch 31/50\n",
      "1428/1428 [==============================] - 1s 567us/step - loss: 1.1985 - mean_absolute_error: 0.8637\n",
      "Epoch 32/50\n",
      "1428/1428 [==============================] - 1s 553us/step - loss: 1.2431 - mean_absolute_error: 0.8609\n",
      "Epoch 33/50\n",
      "1428/1428 [==============================] - 1s 558us/step - loss: 1.1907 - mean_absolute_error: 0.8605\n",
      "Epoch 34/50\n",
      "1428/1428 [==============================] - 1s 555us/step - loss: 1.1931 - mean_absolute_error: 0.8593\n",
      "Epoch 35/50\n",
      "1428/1428 [==============================] - 1s 550us/step - loss: 1.1576 - mean_absolute_error: 0.8394\n",
      "Epoch 36/50\n",
      "1428/1428 [==============================] - 1s 558us/step - loss: 1.1538 - mean_absolute_error: 0.8405\n",
      "Epoch 37/50\n",
      "1428/1428 [==============================] - 1s 556us/step - loss: 1.0818 - mean_absolute_error: 0.8107\n",
      "Epoch 38/50\n",
      "1428/1428 [==============================] - 1s 563us/step - loss: 1.0101 - mean_absolute_error: 0.7886\n",
      "Epoch 39/50\n",
      "1428/1428 [==============================] - 1s 562us/step - loss: 1.1154 - mean_absolute_error: 0.8253\n",
      "Epoch 40/50\n",
      "1428/1428 [==============================] - 1s 554us/step - loss: 1.0409 - mean_absolute_error: 0.8022\n",
      "Epoch 41/50\n",
      "1428/1428 [==============================] - 1s 554us/step - loss: 1.0896 - mean_absolute_error: 0.8244\n",
      "Epoch 42/50\n",
      "1428/1428 [==============================] - 1s 561us/step - loss: 1.0419 - mean_absolute_error: 0.8031\n",
      "Epoch 43/50\n",
      "1428/1428 [==============================] - 1s 563us/step - loss: 0.9979 - mean_absolute_error: 0.7951\n",
      "Epoch 44/50\n",
      "1428/1428 [==============================] - 1s 557us/step - loss: 1.0720 - mean_absolute_error: 0.8212\n",
      "Epoch 45/50\n",
      "1428/1428 [==============================] - 1s 561us/step - loss: 0.9938 - mean_absolute_error: 0.7841\n",
      "Epoch 46/50\n",
      "1428/1428 [==============================] - 1s 561us/step - loss: 0.9937 - mean_absolute_error: 0.7855\n",
      "Epoch 47/50\n",
      "1428/1428 [==============================] - 1s 565us/step - loss: 1.0353 - mean_absolute_error: 0.7842\n",
      "Epoch 48/50\n",
      "1428/1428 [==============================] - 1s 566us/step - loss: 1.0261 - mean_absolute_error: 0.7966\n",
      "Epoch 49/50\n",
      "1428/1428 [==============================] - 1s 566us/step - loss: 1.0393 - mean_absolute_error: 0.8001\n",
      "Epoch 50/50\n",
      "1428/1428 [==============================] - ETA: 0s - loss: 1.0028 - mean_absolute_error: 0.784 - 1s 563us/step - loss: 1.0104 - mean_absolute_error: 0.7901\n",
      "QWK:  0.598283545538\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_109 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_110 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1428/1428 [==============================] - 12s 8ms/step - loss: 8.7791 - mean_absolute_error: 2.1686\n",
      "Epoch 2/50\n",
      "1428/1428 [==============================] - 1s 598us/step - loss: 1.9053 - mean_absolute_error: 1.0846\n",
      "Epoch 3/50\n",
      "1428/1428 [==============================] - 1s 598us/step - loss: 1.7505 - mean_absolute_error: 1.0410\n",
      "Epoch 4/50\n",
      "1428/1428 [==============================] - 1s 587us/step - loss: 1.6916 - mean_absolute_error: 1.0261\n",
      "Epoch 5/50\n",
      "1428/1428 [==============================] - 1s 589us/step - loss: 1.6167 - mean_absolute_error: 1.0039\n",
      "Epoch 6/50\n",
      "1428/1428 [==============================] - 1s 585us/step - loss: 1.6746 - mean_absolute_error: 1.0194\n",
      "Epoch 7/50\n",
      "1428/1428 [==============================] - 1s 590us/step - loss: 1.7038 - mean_absolute_error: 1.0223\n",
      "Epoch 8/50\n",
      "1428/1428 [==============================] - 1s 596us/step - loss: 1.6300 - mean_absolute_error: 0.9887\n",
      "Epoch 9/50\n",
      "1428/1428 [==============================] - 1s 583us/step - loss: 1.4279 - mean_absolute_error: 0.9318\n",
      "Epoch 10/50\n",
      "1428/1428 [==============================] - 1s 586us/step - loss: 1.3620 - mean_absolute_error: 0.9186\n",
      "Epoch 11/50\n",
      "1428/1428 [==============================] - 1s 590us/step - loss: 1.4231 - mean_absolute_error: 0.9351\n",
      "Epoch 12/50\n",
      "1428/1428 [==============================] - 1s 584us/step - loss: 1.4842 - mean_absolute_error: 0.9576\n",
      "Epoch 13/50\n",
      "1428/1428 [==============================] - 1s 587us/step - loss: 1.4557 - mean_absolute_error: 0.9473\n",
      "Epoch 14/50\n",
      "1428/1428 [==============================] - 1s 588us/step - loss: 1.3771 - mean_absolute_error: 0.9329\n",
      "Epoch 15/50\n",
      "1428/1428 [==============================] - 1s 595us/step - loss: 1.4300 - mean_absolute_error: 0.9322\n",
      "Epoch 16/50\n",
      "1428/1428 [==============================] - 1s 583us/step - loss: 1.3559 - mean_absolute_error: 0.9100\n",
      "Epoch 17/50\n",
      "1428/1428 [==============================] - 1s 591us/step - loss: 1.4247 - mean_absolute_error: 0.9431\n",
      "Epoch 18/50\n",
      "1428/1428 [==============================] - 1s 580us/step - loss: 1.3800 - mean_absolute_error: 0.9155\n",
      "Epoch 19/50\n",
      "1428/1428 [==============================] - 1s 593us/step - loss: 1.2551 - mean_absolute_error: 0.8809\n",
      "Epoch 20/50\n",
      "1428/1428 [==============================] - 1s 582us/step - loss: 1.3681 - mean_absolute_error: 0.9153\n",
      "Epoch 21/50\n",
      "1428/1428 [==============================] - 1s 596us/step - loss: 1.3455 - mean_absolute_error: 0.9223\n",
      "Epoch 22/50\n",
      "1428/1428 [==============================] - 1s 588us/step - loss: 1.2472 - mean_absolute_error: 0.8837\n",
      "Epoch 23/50\n",
      "1428/1428 [==============================] - 1s 591us/step - loss: 1.3385 - mean_absolute_error: 0.9038\n",
      "Epoch 24/50\n",
      "1428/1428 [==============================] - 1s 583us/step - loss: 1.2655 - mean_absolute_error: 0.8804\n",
      "Epoch 25/50\n",
      "1428/1428 [==============================] - 1s 596us/step - loss: 1.3316 - mean_absolute_error: 0.9023\n",
      "Epoch 26/50\n",
      "1428/1428 [==============================] - 1s 595us/step - loss: 1.2609 - mean_absolute_error: 0.8769\n",
      "Epoch 27/50\n",
      "1428/1428 [==============================] - 1s 584us/step - loss: 1.2218 - mean_absolute_error: 0.8733\n",
      "Epoch 28/50\n",
      "1428/1428 [==============================] - 1s 584us/step - loss: 1.2325 - mean_absolute_error: 0.8671\n",
      "Epoch 29/50\n",
      "1428/1428 [==============================] - 1s 587us/step - loss: 1.1943 - mean_absolute_error: 0.8559\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428/1428 [==============================] - 1s 574us/step - loss: 1.1544 - mean_absolute_error: 0.8416\n",
      "Epoch 31/50\n",
      "1428/1428 [==============================] - 1s 560us/step - loss: 1.1252 - mean_absolute_error: 0.8344\n",
      "Epoch 32/50\n",
      "1428/1428 [==============================] - 1s 558us/step - loss: 1.2010 - mean_absolute_error: 0.8639\n",
      "Epoch 33/50\n",
      "1428/1428 [==============================] - 1s 564us/step - loss: 1.2022 - mean_absolute_error: 0.8651\n",
      "Epoch 34/50\n",
      "1428/1428 [==============================] - 1s 553us/step - loss: 1.1868 - mean_absolute_error: 0.8443\n",
      "Epoch 35/50\n",
      "1428/1428 [==============================] - 1s 563us/step - loss: 1.0988 - mean_absolute_error: 0.8311\n",
      "Epoch 36/50\n",
      "1428/1428 [==============================] - 1s 551us/step - loss: 1.1296 - mean_absolute_error: 0.8430\n",
      "Epoch 37/50\n",
      "1428/1428 [==============================] - 1s 570us/step - loss: 1.1779 - mean_absolute_error: 0.8479\n",
      "Epoch 38/50\n",
      "1428/1428 [==============================] - 1s 572us/step - loss: 1.0925 - mean_absolute_error: 0.8257\n",
      "Epoch 39/50\n",
      "1428/1428 [==============================] - 1s 551us/step - loss: 1.1582 - mean_absolute_error: 0.8379\n",
      "Epoch 40/50\n",
      "1428/1428 [==============================] - 1s 553us/step - loss: 1.1158 - mean_absolute_error: 0.8367\n",
      "Epoch 41/50\n",
      "1428/1428 [==============================] - 1s 558us/step - loss: 1.0848 - mean_absolute_error: 0.8131\n",
      "Epoch 42/50\n",
      "1428/1428 [==============================] - 1s 558us/step - loss: 1.1150 - mean_absolute_error: 0.8199\n",
      "Epoch 43/50\n",
      "1428/1428 [==============================] - 1s 552us/step - loss: 1.1077 - mean_absolute_error: 0.8370\n",
      "Epoch 44/50\n",
      "1428/1428 [==============================] - 1s 565us/step - loss: 0.9664 - mean_absolute_error: 0.7658\n",
      "Epoch 45/50\n",
      "1428/1428 [==============================] - 1s 558us/step - loss: 0.9781 - mean_absolute_error: 0.7710\n",
      "Epoch 46/50\n",
      "1428/1428 [==============================] - 1s 556us/step - loss: 1.0627 - mean_absolute_error: 0.8259\n",
      "Epoch 47/50\n",
      "1428/1428 [==============================] - 1s 554us/step - loss: 0.9949 - mean_absolute_error: 0.7862\n",
      "Epoch 48/50\n",
      "1428/1428 [==============================] - 1s 557us/step - loss: 1.0039 - mean_absolute_error: 0.7806\n",
      "Epoch 49/50\n",
      "1428/1428 [==============================] - 1s 570us/step - loss: 0.9692 - mean_absolute_error: 0.7704\n",
      "Epoch 50/50\n",
      "1428/1428 [==============================] - 1s 633us/step - loss: 1.0886 - mean_absolute_error: 0.8191\n",
      "QWK:  0.659045345617\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_111 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_112 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1430/1430 [==============================] - 11s 8ms/step - loss: 8.3276 - mean_absolute_error: 2.1274\n",
      "Epoch 2/50\n",
      "1430/1430 [==============================] - 1s 593us/step - loss: 2.0040 - mean_absolute_error: 1.1207\n",
      "Epoch 3/50\n",
      "1430/1430 [==============================] - 1s 584us/step - loss: 1.7797 - mean_absolute_error: 1.0559\n",
      "Epoch 4/50\n",
      "1430/1430 [==============================] - 1s 626us/step - loss: 1.7732 - mean_absolute_error: 1.0502\n",
      "Epoch 5/50\n",
      "1430/1430 [==============================] - 1s 548us/step - loss: 1.6408 - mean_absolute_error: 1.0056\n",
      "Epoch 6/50\n",
      "1430/1430 [==============================] - 1s 594us/step - loss: 1.5923 - mean_absolute_error: 0.9888\n",
      "Epoch 7/50\n",
      "1430/1430 [==============================] - 1s 587us/step - loss: 1.5103 - mean_absolute_error: 0.9660\n",
      "Epoch 8/50\n",
      "1430/1430 [==============================] - 1s 583us/step - loss: 1.5876 - mean_absolute_error: 0.9888\n",
      "Epoch 9/50\n",
      "1430/1430 [==============================] - 1s 591us/step - loss: 1.5136 - mean_absolute_error: 0.9555\n",
      "Epoch 10/50\n",
      "1430/1430 [==============================] - 1s 598us/step - loss: 1.4891 - mean_absolute_error: 0.9660\n",
      "Epoch 11/50\n",
      "1430/1430 [==============================] - 1s 586us/step - loss: 1.4794 - mean_absolute_error: 0.9536\n",
      "Epoch 12/50\n",
      "1430/1430 [==============================] - 1s 587us/step - loss: 1.4161 - mean_absolute_error: 0.9346\n",
      "Epoch 13/50\n",
      "1430/1430 [==============================] - 1s 588us/step - loss: 1.3884 - mean_absolute_error: 0.9264\n",
      "Epoch 14/50\n",
      "1430/1430 [==============================] - 1s 585us/step - loss: 1.3970 - mean_absolute_error: 0.9305\n",
      "Epoch 15/50\n",
      "1430/1430 [==============================] - 1s 593us/step - loss: 1.4069 - mean_absolute_error: 0.9362\n",
      "Epoch 16/50\n",
      "1430/1430 [==============================] - 1s 583us/step - loss: 1.4810 - mean_absolute_error: 0.9531\n",
      "Epoch 17/50\n",
      "1430/1430 [==============================] - 1s 590us/step - loss: 1.3574 - mean_absolute_error: 0.9275\n",
      "Epoch 18/50\n",
      "1430/1430 [==============================] - 1s 586us/step - loss: 1.3957 - mean_absolute_error: 0.9327\n",
      "Epoch 19/50\n",
      "1430/1430 [==============================] - 1s 579us/step - loss: 1.4398 - mean_absolute_error: 0.9432\n",
      "Epoch 20/50\n",
      "1430/1430 [==============================] - 1s 591us/step - loss: 1.3145 - mean_absolute_error: 0.9037\n",
      "Epoch 21/50\n",
      "1430/1430 [==============================] - 1s 586us/step - loss: 1.2702 - mean_absolute_error: 0.8808\n",
      "Epoch 22/50\n",
      "1430/1430 [==============================] - 1s 594us/step - loss: 1.3044 - mean_absolute_error: 0.9038\n",
      "Epoch 23/50\n",
      "1430/1430 [==============================] - 1s 588us/step - loss: 1.1692 - mean_absolute_error: 0.8500\n",
      "Epoch 24/50\n",
      "1430/1430 [==============================] - 1s 589us/step - loss: 1.2546 - mean_absolute_error: 0.8869\n",
      "Epoch 25/50\n",
      "1430/1430 [==============================] - 1s 590us/step - loss: 1.3243 - mean_absolute_error: 0.9066\n",
      "Epoch 26/50\n",
      "1430/1430 [==============================] - 1s 597us/step - loss: 1.3087 - mean_absolute_error: 0.8993\n",
      "Epoch 27/50\n",
      "1430/1430 [==============================] - 1s 590us/step - loss: 1.2330 - mean_absolute_error: 0.8770\n",
      "Epoch 28/50\n",
      "1430/1430 [==============================] - 1s 613us/step - loss: 1.2424 - mean_absolute_error: 0.8766\n",
      "Epoch 29/50\n",
      "1430/1430 [==============================] - 1s 617us/step - loss: 1.2371 - mean_absolute_error: 0.8685\n",
      "Epoch 30/50\n",
      "1430/1430 [==============================] - 1s 614us/step - loss: 1.2121 - mean_absolute_error: 0.8685\n",
      "Epoch 31/50\n",
      "1430/1430 [==============================] - 1s 614us/step - loss: 1.2555 - mean_absolute_error: 0.8820\n",
      "Epoch 32/50\n",
      "1430/1430 [==============================] - 1s 602us/step - loss: 1.1973 - mean_absolute_error: 0.8488\n",
      "Epoch 33/50\n",
      "1430/1430 [==============================] - 1s 608us/step - loss: 1.1650 - mean_absolute_error: 0.8487\n",
      "Epoch 34/50\n",
      "1430/1430 [==============================] - 1s 603us/step - loss: 1.1494 - mean_absolute_error: 0.8425\n",
      "Epoch 35/50\n",
      "1430/1430 [==============================] - 1s 624us/step - loss: 1.1956 - mean_absolute_error: 0.8688\n",
      "Epoch 36/50\n",
      "1430/1430 [==============================] - 1s 621us/step - loss: 1.1666 - mean_absolute_error: 0.8501\n",
      "Epoch 37/50\n",
      "1430/1430 [==============================] - 1s 592us/step - loss: 1.0170 - mean_absolute_error: 0.7937\n",
      "Epoch 38/50\n",
      "1430/1430 [==============================] - 1s 642us/step - loss: 1.0952 - mean_absolute_error: 0.8307\n",
      "Epoch 39/50\n",
      "1430/1430 [==============================] - 1s 598us/step - loss: 1.1043 - mean_absolute_error: 0.8236\n",
      "Epoch 40/50\n",
      "1430/1430 [==============================] - 1s 597us/step - loss: 1.1102 - mean_absolute_error: 0.8347\n",
      "Epoch 41/50\n",
      "1430/1430 [==============================] - 1s 627us/step - loss: 1.1398 - mean_absolute_error: 0.8352\n",
      "Epoch 42/50\n",
      "1430/1430 [==============================] - 1s 614us/step - loss: 1.0618 - mean_absolute_error: 0.8039\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430/1430 [==============================] - 1s 639us/step - loss: 1.1005 - mean_absolute_error: 0.8238\n",
      "Epoch 44/50\n",
      "1430/1430 [==============================] - 1s 604us/step - loss: 1.0684 - mean_absolute_error: 0.8130\n",
      "Epoch 45/50\n",
      "1430/1430 [==============================] - 1s 625us/step - loss: 1.1309 - mean_absolute_error: 0.8284\n",
      "Epoch 46/50\n",
      "1430/1430 [==============================] - 1s 592us/step - loss: 1.0821 - mean_absolute_error: 0.8171\n",
      "Epoch 47/50\n",
      "1430/1430 [==============================] - 1s 634us/step - loss: 0.9752 - mean_absolute_error: 0.7778\n",
      "Epoch 48/50\n",
      "1430/1430 [==============================] - 1s 641us/step - loss: 0.9887 - mean_absolute_error: 0.7904\n",
      "Epoch 49/50\n",
      "1430/1430 [==============================] - 1s 637us/step - loss: 0.9918 - mean_absolute_error: 0.7882\n",
      "Epoch 50/50\n",
      "1430/1430 [==============================] - 1s 631us/step - loss: 1.1127 - mean_absolute_error: 0.8180\n",
      "QWK:  0.631887590545\n",
      "MSE= 1.17927\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec  0.6529\n",
      "Fold # 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_113 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_114 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-505d9f221d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresults_min_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min+max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mresults_min_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_min_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mMin_Max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_min_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-1051b7b66764>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, y, dataset, vec_type)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Call the LSTM to get the score predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2478\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 192\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1312\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             tf_session.TF_ExtendGraph(self._session,\n\u001b[0;32m-> 1358\u001b[0;31m                                       graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1359\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "Min_Max=[]\n",
    "MM_mse=[]\n",
    "for data in Set: \n",
    "    y=pd.DataFrame()\n",
    "    X=pd.DataFrame()\n",
    "    X = data\n",
    "    y['domain1_score']=X['domain1_score']\n",
    "    X = X.replace('NaN', 0)\n",
    "    y['domain1_score']=y['domain1_score'].fillna(0.0).astype(int)\n",
    "    dataset = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    dataset = dataset.split(X, y)\n",
    "    results_min_max = train_model(X, y, dataset, \"min+max\")\n",
    "    results_min_max=np.around(np.array(results_min_max).mean(),decimals=4)\n",
    "    Min_Max.append(results_min_max)\n",
    "    MM_mse.append(mean(LSTM_mse))\n",
    "    print(\"MSE=\",mean(LSTM_mse))\n",
    "    print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec \",results_min_max)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65290000000000004]\n",
      "[1.17927]\n"
     ]
    }
   ],
   "source": [
    "print (Min_Max)\n",
    "print (MM_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "1423/1423 [==============================] - 3s 2ms/step - loss: 7.8279 - mean_absolute_error: 2.0367\n",
      "Epoch 2/50\n",
      "1423/1423 [==============================] - 1s 490us/step - loss: 1.8673 - mean_absolute_error: 1.0632\n",
      "Epoch 3/50\n",
      "1423/1423 [==============================] - 1s 536us/step - loss: 1.8120 - mean_absolute_error: 1.0658\n",
      "Epoch 4/50\n",
      "1423/1423 [==============================] - 1s 528us/step - loss: 1.6027 - mean_absolute_error: 0.9968\n",
      "Epoch 5/50\n",
      "1423/1423 [==============================] - 1s 510us/step - loss: 1.5979 - mean_absolute_error: 0.9947\n",
      "Epoch 6/50\n",
      "1423/1423 [==============================] - 1s 538us/step - loss: 1.6480 - mean_absolute_error: 0.9986\n",
      "Epoch 7/50\n",
      "1423/1423 [==============================] - 1s 548us/step - loss: 1.5893 - mean_absolute_error: 0.9889\n",
      "Epoch 8/50\n",
      "1423/1423 [==============================] - 1s 536us/step - loss: 1.5628 - mean_absolute_error: 0.9694\n",
      "Epoch 9/50\n",
      "1423/1423 [==============================] - 1s 503us/step - loss: 1.5017 - mean_absolute_error: 0.9559\n",
      "Epoch 10/50\n",
      "1423/1423 [==============================] - 1s 558us/step - loss: 1.4962 - mean_absolute_error: 0.9523\n",
      "Epoch 11/50\n",
      "1423/1423 [==============================] - 1s 522us/step - loss: 1.4088 - mean_absolute_error: 0.9424\n",
      "Epoch 12/50\n",
      "1423/1423 [==============================] - 1s 548us/step - loss: 1.4175 - mean_absolute_error: 0.9368\n",
      "Epoch 13/50\n",
      "1423/1423 [==============================] - 1s 521us/step - loss: 1.4570 - mean_absolute_error: 0.9559\n",
      "Epoch 14/50\n",
      "1423/1423 [==============================] - 1s 540us/step - loss: 1.3031 - mean_absolute_error: 0.8954\n",
      "Epoch 15/50\n",
      "1423/1423 [==============================] - 1s 508us/step - loss: 1.5029 - mean_absolute_error: 0.9654\n",
      "Epoch 16/50\n",
      "1423/1423 [==============================] - 1s 510us/step - loss: 1.3061 - mean_absolute_error: 0.8976\n",
      "Epoch 17/50\n",
      "1423/1423 [==============================] - 1s 491us/step - loss: 1.4607 - mean_absolute_error: 0.9507\n",
      "Epoch 18/50\n",
      "1423/1423 [==============================] - 1s 472us/step - loss: 1.3753 - mean_absolute_error: 0.9324\n",
      "Epoch 19/50\n",
      "1423/1423 [==============================] - 1s 472us/step - loss: 1.4112 - mean_absolute_error: 0.9404\n",
      "Epoch 20/50\n",
      "1423/1423 [==============================] - 1s 469us/step - loss: 1.3416 - mean_absolute_error: 0.9079\n",
      "Epoch 21/50\n",
      "1423/1423 [==============================] - 1s 535us/step - loss: 1.3285 - mean_absolute_error: 0.9177\n",
      "Epoch 22/50\n",
      "1423/1423 [==============================] - 1s 556us/step - loss: 1.2358 - mean_absolute_error: 0.8594\n",
      "Epoch 23/50\n",
      "1423/1423 [==============================] - 1s 583us/step - loss: 1.3294 - mean_absolute_error: 0.9141\n",
      "Epoch 24/50\n",
      "1423/1423 [==============================] - 1s 549us/step - loss: 1.2486 - mean_absolute_error: 0.8755\n",
      "Epoch 25/50\n",
      "1423/1423 [==============================] - 1s 573us/step - loss: 1.2410 - mean_absolute_error: 0.8664\n",
      "Epoch 26/50\n",
      "1423/1423 [==============================] - 1s 509us/step - loss: 1.2348 - mean_absolute_error: 0.8650\n",
      "Epoch 27/50\n",
      "1423/1423 [==============================] - 1s 542us/step - loss: 1.1515 - mean_absolute_error: 0.8424\n",
      "Epoch 28/50\n",
      "1423/1423 [==============================] - 1s 501us/step - loss: 1.1338 - mean_absolute_error: 0.8368\n",
      "Epoch 29/50\n",
      "1423/1423 [==============================] - 1s 469us/step - loss: 1.2308 - mean_absolute_error: 0.8742\n",
      "Epoch 30/50\n",
      "1423/1423 [==============================] - 1s 505us/step - loss: 1.1335 - mean_absolute_error: 0.8392\n",
      "Epoch 31/50\n",
      "1423/1423 [==============================] - 1s 543us/step - loss: 1.1827 - mean_absolute_error: 0.8531\n",
      "Epoch 32/50\n",
      "1423/1423 [==============================] - 1s 546us/step - loss: 1.1215 - mean_absolute_error: 0.8383\n",
      "Epoch 33/50\n",
      "1423/1423 [==============================] - 1s 567us/step - loss: 1.1599 - mean_absolute_error: 0.8498\n",
      "Epoch 34/50\n",
      "1423/1423 [==============================] - 1s 499us/step - loss: 1.1930 - mean_absolute_error: 0.8546\n",
      "Epoch 35/50\n",
      "1423/1423 [==============================] - 1s 472us/step - loss: 1.1389 - mean_absolute_error: 0.8417\n",
      "Epoch 36/50\n",
      "1423/1423 [==============================] - 1s 467us/step - loss: 1.0892 - mean_absolute_error: 0.8281\n",
      "Epoch 37/50\n",
      "1423/1423 [==============================] - 1s 483us/step - loss: 1.0660 - mean_absolute_error: 0.8121\n",
      "Epoch 38/50\n",
      "1423/1423 [==============================] - 1s 473us/step - loss: 1.0790 - mean_absolute_error: 0.8179\n",
      "Epoch 39/50\n",
      "1423/1423 [==============================] - 1s 467us/step - loss: 1.0368 - mean_absolute_error: 0.7958\n",
      "Epoch 40/50\n",
      "1423/1423 [==============================] - 1s 487us/step - loss: 1.0371 - mean_absolute_error: 0.8016\n",
      "Epoch 41/50\n",
      "1423/1423 [==============================] - 1s 478us/step - loss: 1.0881 - mean_absolute_error: 0.8158\n",
      "Epoch 42/50\n",
      "1423/1423 [==============================] - 1s 482us/step - loss: 1.1190 - mean_absolute_error: 0.8339\n",
      "Epoch 43/50\n",
      "1423/1423 [==============================] - 1s 483us/step - loss: 1.1323 - mean_absolute_error: 0.8357\n",
      "Epoch 44/50\n",
      "1423/1423 [==============================] - 1s 477us/step - loss: 1.0959 - mean_absolute_error: 0.8203\n",
      "Epoch 45/50\n",
      "1423/1423 [==============================] - 1s 473us/step - loss: 1.1200 - mean_absolute_error: 0.8339\n",
      "Epoch 46/50\n",
      "1423/1423 [==============================] - 1s 473us/step - loss: 1.0772 - mean_absolute_error: 0.8205\n",
      "Epoch 47/50\n",
      "1423/1423 [==============================] - 1s 473us/step - loss: 1.0499 - mean_absolute_error: 0.8016\n",
      "Epoch 48/50\n",
      "1423/1423 [==============================] - 1s 476us/step - loss: 1.0134 - mean_absolute_error: 0.8004\n",
      "Epoch 49/50\n",
      "1423/1423 [==============================] - 1s 476us/step - loss: 0.9954 - mean_absolute_error: 0.7806\n",
      "Epoch 50/50\n",
      "1423/1423 [==============================] - 1s 476us/step - loss: 1.0339 - mean_absolute_error: 0.7933\n",
      "QWK:  0.6864686468646866\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1423/1423 [==============================] - 2s 2ms/step - loss: 8.6724 - mean_absolute_error: 2.1621\n",
      "Epoch 2/50\n",
      "1423/1423 [==============================] - 1s 476us/step - loss: 1.9768 - mean_absolute_error: 1.1068\n",
      "Epoch 3/50\n",
      "1423/1423 [==============================] - 1s 484us/step - loss: 1.7857 - mean_absolute_error: 1.0516\n",
      "Epoch 4/50\n",
      "1423/1423 [==============================] - 1s 483us/step - loss: 1.7063 - mean_absolute_error: 1.0254\n",
      "Epoch 5/50\n",
      "1423/1423 [==============================] - 1s 500us/step - loss: 1.6579 - mean_absolute_error: 0.9908\n",
      "Epoch 6/50\n",
      "1423/1423 [==============================] - 1s 491us/step - loss: 1.7346 - mean_absolute_error: 1.0381\n",
      "Epoch 7/50\n",
      "1423/1423 [==============================] - 1s 500us/step - loss: 1.6439 - mean_absolute_error: 1.0006\n",
      "Epoch 8/50\n",
      "1423/1423 [==============================] - 1s 496us/step - loss: 1.5669 - mean_absolute_error: 0.9801\n",
      "Epoch 9/50\n",
      "1423/1423 [==============================] - 1s 496us/step - loss: 1.5266 - mean_absolute_error: 0.9843\n",
      "Epoch 10/50\n",
      "1423/1423 [==============================] - 1s 604us/step - loss: 1.5046 - mean_absolute_error: 0.9653\n",
      "Epoch 11/50\n",
      "1423/1423 [==============================] - 1s 537us/step - loss: 1.5028 - mean_absolute_error: 0.9631\n",
      "Epoch 12/50\n",
      "1423/1423 [==============================] - 1s 501us/step - loss: 1.4564 - mean_absolute_error: 0.9498\n",
      "Epoch 13/50\n",
      "1423/1423 [==============================] - 1s 500us/step - loss: 1.4833 - mean_absolute_error: 0.9648\n",
      "Epoch 14/50\n",
      "1423/1423 [==============================] - 1s 493us/step - loss: 1.4237 - mean_absolute_error: 0.9344\n",
      "Epoch 15/50\n",
      "1423/1423 [==============================] - 1s 492us/step - loss: 1.3909 - mean_absolute_error: 0.9291\n",
      "Epoch 16/50\n",
      "1423/1423 [==============================] - 1s 476us/step - loss: 1.4221 - mean_absolute_error: 0.9238\n",
      "Epoch 17/50\n",
      "1423/1423 [==============================] - 1s 476us/step - loss: 1.3548 - mean_absolute_error: 0.9207\n",
      "Epoch 18/50\n",
      "1423/1423 [==============================] - 1s 469us/step - loss: 1.4870 - mean_absolute_error: 0.9638\n",
      "Epoch 19/50\n",
      "1423/1423 [==============================] - 1s 482us/step - loss: 1.4402 - mean_absolute_error: 0.9418\n",
      "Epoch 20/50\n",
      "1423/1423 [==============================] - 1s 485us/step - loss: 1.3301 - mean_absolute_error: 0.9121\n",
      "Epoch 21/50\n",
      "1423/1423 [==============================] - 1s 500us/step - loss: 1.2658 - mean_absolute_error: 0.8823\n",
      "Epoch 22/50\n",
      "1423/1423 [==============================] - 1s 498us/step - loss: 1.3881 - mean_absolute_error: 0.9186\n",
      "Epoch 23/50\n",
      "1423/1423 [==============================] - 1s 490us/step - loss: 1.2866 - mean_absolute_error: 0.8894\n",
      "Epoch 24/50\n",
      "1423/1423 [==============================] - 1s 474us/step - loss: 1.2859 - mean_absolute_error: 0.8960\n",
      "Epoch 25/50\n",
      "1423/1423 [==============================] - 1s 474us/step - loss: 1.2419 - mean_absolute_error: 0.8692\n",
      "Epoch 26/50\n",
      "1423/1423 [==============================] - 1s 505us/step - loss: 1.3254 - mean_absolute_error: 0.9091\n",
      "Epoch 27/50\n",
      "1423/1423 [==============================] - 1s 485us/step - loss: 1.3124 - mean_absolute_error: 0.8992\n",
      "Epoch 28/50\n",
      "1423/1423 [==============================] - 1s 487us/step - loss: 1.1502 - mean_absolute_error: 0.8436\n",
      "Epoch 29/50\n",
      "1423/1423 [==============================] - 1s 520us/step - loss: 1.2285 - mean_absolute_error: 0.8779\n",
      "Epoch 30/50\n",
      "1423/1423 [==============================] - 1s 492us/step - loss: 1.2185 - mean_absolute_error: 0.8709\n",
      "Epoch 31/50\n",
      "1423/1423 [==============================] - 1s 496us/step - loss: 1.2056 - mean_absolute_error: 0.8513\n",
      "Epoch 32/50\n",
      "1423/1423 [==============================] - 1s 519us/step - loss: 1.2199 - mean_absolute_error: 0.8667\n",
      "Epoch 33/50\n",
      "1423/1423 [==============================] - 1s 491us/step - loss: 1.1948 - mean_absolute_error: 0.8540\n",
      "Epoch 34/50\n",
      "1423/1423 [==============================] - 1s 492us/step - loss: 1.1588 - mean_absolute_error: 0.8378\n",
      "Epoch 35/50\n",
      "1423/1423 [==============================] - 1s 492us/step - loss: 1.1679 - mean_absolute_error: 0.8460\n",
      "Epoch 36/50\n",
      "1423/1423 [==============================] - 1s 478us/step - loss: 1.1408 - mean_absolute_error: 0.8478\n",
      "Epoch 37/50\n",
      "1423/1423 [==============================] - 1s 474us/step - loss: 1.1275 - mean_absolute_error: 0.8375\n",
      "Epoch 38/50\n",
      "1423/1423 [==============================] - 1s 482us/step - loss: 1.1375 - mean_absolute_error: 0.8346\n",
      "Epoch 39/50\n",
      "1423/1423 [==============================] - 1s 477us/step - loss: 1.1061 - mean_absolute_error: 0.8169\n",
      "Epoch 40/50\n",
      "1423/1423 [==============================] - 1s 480us/step - loss: 1.1458 - mean_absolute_error: 0.8307\n",
      "Epoch 41/50\n",
      "1423/1423 [==============================] - 1s 481us/step - loss: 1.0422 - mean_absolute_error: 0.7958\n",
      "Epoch 42/50\n",
      "1423/1423 [==============================] - 1s 500us/step - loss: 1.1220 - mean_absolute_error: 0.8160\n",
      "Epoch 43/50\n",
      "1423/1423 [==============================] - 1s 482us/step - loss: 1.0509 - mean_absolute_error: 0.8138\n",
      "Epoch 44/50\n",
      "1423/1423 [==============================] - 1s 482us/step - loss: 1.1489 - mean_absolute_error: 0.8372\n",
      "Epoch 45/50\n",
      "1423/1423 [==============================] - 1s 480us/step - loss: 1.1015 - mean_absolute_error: 0.8189\n",
      "Epoch 46/50\n",
      "1423/1423 [==============================] - 1s 483us/step - loss: 1.0275 - mean_absolute_error: 0.7976\n",
      "Epoch 47/50\n",
      "1423/1423 [==============================] - 1s 501us/step - loss: 1.1095 - mean_absolute_error: 0.8368\n",
      "Epoch 48/50\n",
      "1423/1423 [==============================] - 1s 484us/step - loss: 1.1062 - mean_absolute_error: 0.8241\n",
      "Epoch 49/50\n",
      "1423/1423 [==============================] - 1s 493us/step - loss: 1.0127 - mean_absolute_error: 0.7954\n",
      "Epoch 50/50\n",
      "1423/1423 [==============================] - 1s 485us/step - loss: 1.0775 - mean_absolute_error: 0.8210\n",
      "QWK:  0.7631702360481476\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1428/1428 [==============================] - 3s 2ms/step - loss: 8.3573 - mean_absolute_error: 2.0783\n",
      "Epoch 2/50\n",
      "1428/1428 [==============================] - 1s 497us/step - loss: 1.8751 - mean_absolute_error: 1.0852\n",
      "Epoch 3/50\n",
      "1428/1428 [==============================] - 1s 505us/step - loss: 1.8091 - mean_absolute_error: 1.0550\n",
      "Epoch 4/50\n",
      "1428/1428 [==============================] - 1s 508us/step - loss: 1.7446 - mean_absolute_error: 1.0468\n",
      "Epoch 5/50\n",
      "1428/1428 [==============================] - 1s 496us/step - loss: 1.6148 - mean_absolute_error: 1.0171\n",
      "Epoch 6/50\n",
      "1428/1428 [==============================] - 1s 498us/step - loss: 1.5486 - mean_absolute_error: 0.9849\n",
      "Epoch 7/50\n",
      "1428/1428 [==============================] - 1s 584us/step - loss: 1.5420 - mean_absolute_error: 0.9828\n",
      "Epoch 8/50\n",
      "1428/1428 [==============================] - 1s 543us/step - loss: 1.5330 - mean_absolute_error: 0.9808\n",
      "Epoch 9/50\n",
      "1428/1428 [==============================] - 1s 518us/step - loss: 1.4446 - mean_absolute_error: 0.9347\n",
      "Epoch 10/50\n",
      "1428/1428 [==============================] - 1s 490us/step - loss: 1.5035 - mean_absolute_error: 0.9787\n",
      "Epoch 11/50\n",
      "1428/1428 [==============================] - 1s 491us/step - loss: 1.4999 - mean_absolute_error: 0.9684\n",
      "Epoch 12/50\n",
      "1428/1428 [==============================] - 1s 514us/step - loss: 1.3681 - mean_absolute_error: 0.9183\n",
      "Epoch 13/50\n",
      "1428/1428 [==============================] - 1s 492us/step - loss: 1.3579 - mean_absolute_error: 0.9149\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428/1428 [==============================] - 1s 502us/step - loss: 1.3201 - mean_absolute_error: 0.9092\n",
      "Epoch 15/50\n",
      "1428/1428 [==============================] - 1s 487us/step - loss: 1.3415 - mean_absolute_error: 0.9091\n",
      "Epoch 16/50\n",
      "1428/1428 [==============================] - 1s 472us/step - loss: 1.3656 - mean_absolute_error: 0.9104\n",
      "Epoch 17/50\n",
      "1428/1428 [==============================] - 1s 484us/step - loss: 1.3539 - mean_absolute_error: 0.9179\n",
      "Epoch 18/50\n",
      "1428/1428 [==============================] - 1s 478us/step - loss: 1.3886 - mean_absolute_error: 0.9170\n",
      "Epoch 19/50\n",
      "1428/1428 [==============================] - 1s 512us/step - loss: 1.4075 - mean_absolute_error: 0.9297\n",
      "Epoch 20/50\n",
      "1428/1428 [==============================] - 1s 481us/step - loss: 1.2979 - mean_absolute_error: 0.9117\n",
      "Epoch 21/50\n",
      "1428/1428 [==============================] - 1s 482us/step - loss: 1.2777 - mean_absolute_error: 0.8876\n",
      "Epoch 22/50\n",
      "1428/1428 [==============================] - 1s 534us/step - loss: 1.2969 - mean_absolute_error: 0.8914\n",
      "Epoch 23/50\n",
      "1428/1428 [==============================] - 1s 496us/step - loss: 1.2635 - mean_absolute_error: 0.8853\n",
      "Epoch 24/50\n",
      "1428/1428 [==============================] - 1s 475us/step - loss: 1.2723 - mean_absolute_error: 0.8821\n",
      "Epoch 25/50\n",
      "1428/1428 [==============================] - 1s 477us/step - loss: 1.2515 - mean_absolute_error: 0.8835\n",
      "Epoch 26/50\n",
      "1428/1428 [==============================] - 1s 473us/step - loss: 1.2925 - mean_absolute_error: 0.8920\n",
      "Epoch 27/50\n",
      "1428/1428 [==============================] - 1s 537us/step - loss: 1.2510 - mean_absolute_error: 0.8739 0s - loss: 1.2448 - mean_absolute_error: 0.872\n",
      "Epoch 28/50\n",
      "1428/1428 [==============================] - 1s 506us/step - loss: 1.2274 - mean_absolute_error: 0.8528\n",
      "Epoch 29/50\n",
      "1428/1428 [==============================] - 1s 515us/step - loss: 1.2156 - mean_absolute_error: 0.8517\n",
      "Epoch 30/50\n",
      "1428/1428 [==============================] - 1s 526us/step - loss: 1.2350 - mean_absolute_error: 0.8766\n",
      "Epoch 31/50\n",
      "1428/1428 [==============================] - 1s 521us/step - loss: 1.1403 - mean_absolute_error: 0.8493\n",
      "Epoch 32/50\n",
      "1428/1428 [==============================] - 1s 486us/step - loss: 1.1767 - mean_absolute_error: 0.8620\n",
      "Epoch 33/50\n",
      "1428/1428 [==============================] - 1s 474us/step - loss: 1.1817 - mean_absolute_error: 0.8440\n",
      "Epoch 34/50\n",
      "1428/1428 [==============================] - 1s 477us/step - loss: 1.1624 - mean_absolute_error: 0.8413\n",
      "Epoch 35/50\n",
      "1428/1428 [==============================] - 1s 478us/step - loss: 1.1402 - mean_absolute_error: 0.8421\n",
      "Epoch 36/50\n",
      "1428/1428 [==============================] - 1s 472us/step - loss: 1.1113 - mean_absolute_error: 0.8337\n",
      "Epoch 37/50\n",
      "1428/1428 [==============================] - 1s 469us/step - loss: 1.1139 - mean_absolute_error: 0.8239\n",
      "Epoch 38/50\n",
      "1428/1428 [==============================] - 1s 467us/step - loss: 1.0943 - mean_absolute_error: 0.8205\n",
      "Epoch 39/50\n",
      "1428/1428 [==============================] - 1s 465us/step - loss: 1.0635 - mean_absolute_error: 0.8083\n",
      "Epoch 40/50\n",
      "1428/1428 [==============================] - 1s 465us/step - loss: 1.0896 - mean_absolute_error: 0.8143\n",
      "Epoch 41/50\n",
      "1428/1428 [==============================] - 1s 471us/step - loss: 1.1509 - mean_absolute_error: 0.8425\n",
      "Epoch 42/50\n",
      "1428/1428 [==============================] - 1s 463us/step - loss: 1.1192 - mean_absolute_error: 0.8394\n",
      "Epoch 43/50\n",
      "1428/1428 [==============================] - 1s 459us/step - loss: 1.0236 - mean_absolute_error: 0.7971\n",
      "Epoch 44/50\n",
      "1428/1428 [==============================] - 1s 468us/step - loss: 1.0682 - mean_absolute_error: 0.8138\n",
      "Epoch 45/50\n",
      "1428/1428 [==============================] - 1s 472us/step - loss: 1.0279 - mean_absolute_error: 0.8007\n",
      "Epoch 46/50\n",
      "1428/1428 [==============================] - 1s 466us/step - loss: 0.9791 - mean_absolute_error: 0.7731\n",
      "Epoch 47/50\n",
      "1428/1428 [==============================] - 1s 475us/step - loss: 1.0063 - mean_absolute_error: 0.7890\n",
      "Epoch 48/50\n",
      "1428/1428 [==============================] - 1s 469us/step - loss: 1.0330 - mean_absolute_error: 0.7918\n",
      "Epoch 49/50\n",
      "1428/1428 [==============================] - 1s 467us/step - loss: 0.9951 - mean_absolute_error: 0.7789\n",
      "Epoch 50/50\n",
      "1428/1428 [==============================] - 1s 468us/step - loss: 1.0316 - mean_absolute_error: 0.7912\n",
      "QWK:  0.6764295802293103\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1428/1428 [==============================] - 3s 2ms/step - loss: 7.7740 - mean_absolute_error: 2.0320\n",
      "Epoch 2/50\n",
      "1428/1428 [==============================] - 1s 488us/step - loss: 1.8886 - mean_absolute_error: 1.0704\n",
      "Epoch 3/50\n",
      "1428/1428 [==============================] - 1s 478us/step - loss: 1.5679 - mean_absolute_error: 0.9884\n",
      "Epoch 4/50\n",
      "1428/1428 [==============================] - 1s 464us/step - loss: 1.6043 - mean_absolute_error: 0.9805\n",
      "Epoch 5/50\n",
      "1428/1428 [==============================] - 1s 465us/step - loss: 1.5192 - mean_absolute_error: 0.9664\n",
      "Epoch 6/50\n",
      "1428/1428 [==============================] - 1s 467us/step - loss: 1.5132 - mean_absolute_error: 0.9588\n",
      "Epoch 7/50\n",
      "1428/1428 [==============================] - 1s 467us/step - loss: 1.5368 - mean_absolute_error: 0.9833\n",
      "Epoch 8/50\n",
      "1428/1428 [==============================] - 1s 461us/step - loss: 1.4879 - mean_absolute_error: 0.9636\n",
      "Epoch 9/50\n",
      "1428/1428 [==============================] - 1s 477us/step - loss: 1.4523 - mean_absolute_error: 0.9493\n",
      "Epoch 10/50\n",
      "1428/1428 [==============================] - 1s 487us/step - loss: 1.4769 - mean_absolute_error: 0.9562\n",
      "Epoch 11/50\n",
      "1428/1428 [==============================] - 1s 493us/step - loss: 1.4450 - mean_absolute_error: 0.9430\n",
      "Epoch 12/50\n",
      "1428/1428 [==============================] - 1s 490us/step - loss: 1.4501 - mean_absolute_error: 0.9462\n",
      "Epoch 13/50\n",
      "1428/1428 [==============================] - 1s 487us/step - loss: 1.4308 - mean_absolute_error: 0.9380\n",
      "Epoch 14/50\n",
      "1428/1428 [==============================] - 1s 478us/step - loss: 1.3505 - mean_absolute_error: 0.9181\n",
      "Epoch 15/50\n",
      "1428/1428 [==============================] - 1s 481us/step - loss: 1.4011 - mean_absolute_error: 0.9294\n",
      "Epoch 16/50\n",
      "1428/1428 [==============================] - 1s 469us/step - loss: 1.3934 - mean_absolute_error: 0.9369\n",
      "Epoch 17/50\n",
      "1428/1428 [==============================] - 1s 466us/step - loss: 1.3326 - mean_absolute_error: 0.9022\n",
      "Epoch 18/50\n",
      "1428/1428 [==============================] - 1s 463us/step - loss: 1.2820 - mean_absolute_error: 0.8821\n",
      "Epoch 19/50\n",
      "1428/1428 [==============================] - 1s 472us/step - loss: 1.3733 - mean_absolute_error: 0.9346\n",
      "Epoch 20/50\n",
      "1428/1428 [==============================] - 1s 484us/step - loss: 1.3668 - mean_absolute_error: 0.9075\n",
      "Epoch 21/50\n",
      "1428/1428 [==============================] - 1s 431us/step - loss: 1.3504 - mean_absolute_error: 0.9031\n",
      "Epoch 22/50\n",
      "1428/1428 [==============================] - 1s 436us/step - loss: 1.1664 - mean_absolute_error: 0.8424\n",
      "Epoch 23/50\n",
      "1428/1428 [==============================] - 1s 441us/step - loss: 1.2745 - mean_absolute_error: 0.8925\n",
      "Epoch 24/50\n",
      "1428/1428 [==============================] - 1s 440us/step - loss: 1.3121 - mean_absolute_error: 0.9088\n",
      "Epoch 25/50\n",
      "1428/1428 [==============================] - 1s 440us/step - loss: 1.2388 - mean_absolute_error: 0.8842\n",
      "Epoch 26/50\n",
      "1428/1428 [==============================] - 1s 433us/step - loss: 1.2070 - mean_absolute_error: 0.8726\n",
      "Epoch 27/50\n",
      "1428/1428 [==============================] - 1s 436us/step - loss: 1.2618 - mean_absolute_error: 0.8901\n",
      "Epoch 28/50\n",
      "1428/1428 [==============================] - 1s 422us/step - loss: 1.2190 - mean_absolute_error: 0.8757\n",
      "Epoch 29/50\n",
      "1428/1428 [==============================] - 1s 418us/step - loss: 1.1577 - mean_absolute_error: 0.8486\n",
      "Epoch 30/50\n",
      "1428/1428 [==============================] - 1s 423us/step - loss: 1.2506 - mean_absolute_error: 0.8767\n",
      "Epoch 31/50\n",
      "1428/1428 [==============================] - 1s 423us/step - loss: 1.1829 - mean_absolute_error: 0.8589\n",
      "Epoch 32/50\n",
      "1428/1428 [==============================] - 1s 423us/step - loss: 1.1877 - mean_absolute_error: 0.8561\n",
      "Epoch 33/50\n",
      "1428/1428 [==============================] - 1s 425us/step - loss: 1.1294 - mean_absolute_error: 0.8323\n",
      "Epoch 34/50\n",
      "1428/1428 [==============================] - 1s 429us/step - loss: 1.0539 - mean_absolute_error: 0.8013\n",
      "Epoch 35/50\n",
      "1428/1428 [==============================] - 1s 440us/step - loss: 1.1443 - mean_absolute_error: 0.8247\n",
      "Epoch 36/50\n",
      "1428/1428 [==============================] - 1s 435us/step - loss: 1.2081 - mean_absolute_error: 0.8509\n",
      "Epoch 37/50\n",
      "1428/1428 [==============================] - 1s 436us/step - loss: 1.1089 - mean_absolute_error: 0.8229\n",
      "Epoch 38/50\n",
      "1428/1428 [==============================] - 1s 439us/step - loss: 1.0985 - mean_absolute_error: 0.8242\n",
      "Epoch 39/50\n",
      "1428/1428 [==============================] - 1s 438us/step - loss: 1.0772 - mean_absolute_error: 0.8224\n",
      "Epoch 40/50\n",
      "1428/1428 [==============================] - 1s 433us/step - loss: 1.0096 - mean_absolute_error: 0.7925\n",
      "Epoch 41/50\n",
      "1428/1428 [==============================] - 1s 420us/step - loss: 1.1142 - mean_absolute_error: 0.8250\n",
      "Epoch 42/50\n",
      "1428/1428 [==============================] - 1s 421us/step - loss: 0.9623 - mean_absolute_error: 0.7776\n",
      "Epoch 43/50\n",
      "1428/1428 [==============================] - 1s 424us/step - loss: 1.0128 - mean_absolute_error: 0.7924\n",
      "Epoch 44/50\n",
      "1428/1428 [==============================] - 1s 421us/step - loss: 1.0642 - mean_absolute_error: 0.8060\n",
      "Epoch 45/50\n",
      "1428/1428 [==============================] - 1s 424us/step - loss: 1.0722 - mean_absolute_error: 0.8128\n",
      "Epoch 46/50\n",
      "1428/1428 [==============================] - 1s 423us/step - loss: 1.0772 - mean_absolute_error: 0.8052\n",
      "Epoch 47/50\n",
      "1428/1428 [==============================] - 1s 428us/step - loss: 1.0069 - mean_absolute_error: 0.7857\n",
      "Epoch 48/50\n",
      "1428/1428 [==============================] - 1s 437us/step - loss: 1.0308 - mean_absolute_error: 0.7840\n",
      "Epoch 49/50\n",
      "1428/1428 [==============================] - 1s 434us/step - loss: 1.0760 - mean_absolute_error: 0.8145\n",
      "Epoch 50/50\n",
      "1428/1428 [==============================] - 1s 438us/step - loss: 1.0019 - mean_absolute_error: 0.7750\n",
      "QWK:  0.5747507541659438\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1430/1430 [==============================] - 3s 2ms/step - loss: 8.5500 - mean_absolute_error: 2.1224\n",
      "Epoch 2/50\n",
      "1430/1430 [==============================] - 1s 473us/step - loss: 2.0358 - mean_absolute_error: 1.1268\n",
      "Epoch 3/50\n",
      "1430/1430 [==============================] - 1s 467us/step - loss: 1.8122 - mean_absolute_error: 1.0506\n",
      "Epoch 4/50\n",
      "1430/1430 [==============================] - 1s 472us/step - loss: 1.8513 - mean_absolute_error: 1.0656\n",
      "Epoch 5/50\n",
      "1430/1430 [==============================] - 1s 468us/step - loss: 1.6272 - mean_absolute_error: 1.0013\n",
      "Epoch 6/50\n",
      "1430/1430 [==============================] - 1s 473us/step - loss: 1.5669 - mean_absolute_error: 0.9802\n",
      "Epoch 7/50\n",
      "1430/1430 [==============================] - 1s 562us/step - loss: 1.5957 - mean_absolute_error: 0.9898\n",
      "Epoch 8/50\n",
      "1430/1430 [==============================] - 1s 569us/step - loss: 1.5550 - mean_absolute_error: 0.9838\n",
      "Epoch 9/50\n",
      "1430/1430 [==============================] - 1s 571us/step - loss: 1.5890 - mean_absolute_error: 0.9940\n",
      "Epoch 10/50\n",
      "1430/1430 [==============================] - 1s 567us/step - loss: 1.4955 - mean_absolute_error: 0.9668\n",
      "Epoch 11/50\n",
      "1430/1430 [==============================] - 1s 567us/step - loss: 1.4634 - mean_absolute_error: 0.9577\n",
      "Epoch 12/50\n",
      "1430/1430 [==============================] - 1s 489us/step - loss: 1.3625 - mean_absolute_error: 0.9012\n",
      "Epoch 13/50\n",
      "1430/1430 [==============================] - 1s 489us/step - loss: 1.4166 - mean_absolute_error: 0.9402\n",
      "Epoch 14/50\n",
      "1430/1430 [==============================] - 1s 486us/step - loss: 1.4908 - mean_absolute_error: 0.9580\n",
      "Epoch 15/50\n",
      "1430/1430 [==============================] - 1s 488us/step - loss: 1.3639 - mean_absolute_error: 0.9247\n",
      "Epoch 16/50\n",
      "1430/1430 [==============================] - 1s 493us/step - loss: 1.3482 - mean_absolute_error: 0.9157\n",
      "Epoch 17/50\n",
      "1430/1430 [==============================] - 1s 484us/step - loss: 1.3504 - mean_absolute_error: 0.9235\n",
      "Epoch 18/50\n",
      "1430/1430 [==============================] - 1s 473us/step - loss: 1.3694 - mean_absolute_error: 0.9147\n",
      "Epoch 19/50\n",
      "1430/1430 [==============================] - 1s 481us/step - loss: 1.3295 - mean_absolute_error: 0.8889\n",
      "Epoch 20/50\n",
      "1430/1430 [==============================] - 1s 474us/step - loss: 1.2467 - mean_absolute_error: 0.8783\n",
      "Epoch 21/50\n",
      "1430/1430 [==============================] - 1s 475us/step - loss: 1.3054 - mean_absolute_error: 0.8894\n",
      "Epoch 22/50\n",
      "1430/1430 [==============================] - 1s 472us/step - loss: 1.3269 - mean_absolute_error: 0.9022\n",
      "Epoch 23/50\n",
      "1430/1430 [==============================] - 1s 478us/step - loss: 1.3496 - mean_absolute_error: 0.9057\n",
      "Epoch 24/50\n",
      "1430/1430 [==============================] - 1s 495us/step - loss: 1.2277 - mean_absolute_error: 0.8906\n",
      "Epoch 25/50\n",
      "1430/1430 [==============================] - 1s 493us/step - loss: 1.2690 - mean_absolute_error: 0.8849\n",
      "Epoch 26/50\n",
      "1430/1430 [==============================] - 1s 493us/step - loss: 1.3386 - mean_absolute_error: 0.9095\n",
      "Epoch 27/50\n",
      "1430/1430 [==============================] - 1s 492us/step - loss: 1.2093 - mean_absolute_error: 0.8645\n",
      "Epoch 28/50\n",
      "1430/1430 [==============================] - 1s 490us/step - loss: 1.2017 - mean_absolute_error: 0.8576\n",
      "Epoch 29/50\n",
      "1430/1430 [==============================] - 1s 481us/step - loss: 1.1348 - mean_absolute_error: 0.8370\n",
      "Epoch 30/50\n",
      "1430/1430 [==============================] - 1s 476us/step - loss: 1.1844 - mean_absolute_error: 0.8474\n",
      "Epoch 31/50\n",
      "1430/1430 [==============================] - 1s 474us/step - loss: 1.1736 - mean_absolute_error: 0.8495\n",
      "Epoch 32/50\n",
      "1430/1430 [==============================] - 1s 480us/step - loss: 1.1343 - mean_absolute_error: 0.8404\n",
      "Epoch 33/50\n",
      "1430/1430 [==============================] - 1s 473us/step - loss: 1.1701 - mean_absolute_error: 0.8356\n",
      "Epoch 34/50\n",
      "1430/1430 [==============================] - 1s 475us/step - loss: 1.1490 - mean_absolute_error: 0.8323\n",
      "Epoch 35/50\n",
      "1430/1430 [==============================] - 1s 490us/step - loss: 1.2818 - mean_absolute_error: 0.8926\n",
      "Epoch 36/50\n",
      "1430/1430 [==============================] - 1s 500us/step - loss: 1.1574 - mean_absolute_error: 0.8433\n",
      "Epoch 37/50\n",
      "1430/1430 [==============================] - 1s 494us/step - loss: 1.1127 - mean_absolute_error: 0.8293\n",
      "Epoch 38/50\n",
      "1430/1430 [==============================] - 1s 515us/step - loss: 1.1964 - mean_absolute_error: 0.8581\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430/1430 [==============================] - 1s 481us/step - loss: 1.1105 - mean_absolute_error: 0.8350\n",
      "Epoch 40/50\n",
      "1430/1430 [==============================] - 1s 514us/step - loss: 1.0733 - mean_absolute_error: 0.8172\n",
      "Epoch 41/50\n",
      "1430/1430 [==============================] - 1s 625us/step - loss: 1.0696 - mean_absolute_error: 0.8095\n",
      "Epoch 42/50\n",
      "1430/1430 [==============================] - 1s 626us/step - loss: 1.0481 - mean_absolute_error: 0.8037\n",
      "Epoch 43/50\n",
      "1430/1430 [==============================] - 1s 634us/step - loss: 1.0393 - mean_absolute_error: 0.8004\n",
      "Epoch 44/50\n",
      "1430/1430 [==============================] - 1s 629us/step - loss: 1.0770 - mean_absolute_error: 0.8240\n",
      "Epoch 45/50\n",
      "1430/1430 [==============================] - 1s 575us/step - loss: 1.0631 - mean_absolute_error: 0.8072\n",
      "Epoch 46/50\n",
      "1430/1430 [==============================] - 1s 562us/step - loss: 1.0557 - mean_absolute_error: 0.8034\n",
      "Epoch 47/50\n",
      "1430/1430 [==============================] - 1s 562us/step - loss: 1.1384 - mean_absolute_error: 0.8480\n",
      "Epoch 48/50\n",
      "1430/1430 [==============================] - 1s 560us/step - loss: 1.0454 - mean_absolute_error: 0.7888\n",
      "Epoch 49/50\n",
      "1430/1430 [==============================] - 1s 557us/step - loss: 1.0615 - mean_absolute_error: 0.8132\n",
      "Epoch 50/50\n",
      "1430/1430 [==============================] - 1s 509us/step - loss: 1.0469 - mean_absolute_error: 0.8018\n",
      "QWK:  0.6266282446203615\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.6655\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1436/1436 [==============================] - 4s 2ms/step - loss: 9.4780 - mean_absolute_error: 2.2555\n",
      "Epoch 2/50\n",
      "1436/1436 [==============================] - 1s 521us/step - loss: 1.9915 - mean_absolute_error: 1.1239\n",
      "Epoch 3/50\n",
      "1436/1436 [==============================] - 1s 565us/step - loss: 1.8016 - mean_absolute_error: 1.0791\n",
      "Epoch 4/50\n",
      "1436/1436 [==============================] - 1s 564us/step - loss: 1.5791 - mean_absolute_error: 0.9951\n",
      "Epoch 5/50\n",
      "1436/1436 [==============================] - 1s 561us/step - loss: 1.7201 - mean_absolute_error: 1.0413\n",
      "Epoch 6/50\n",
      "1436/1436 [==============================] - 1s 574us/step - loss: 1.5943 - mean_absolute_error: 1.0014\n",
      "Epoch 7/50\n",
      "1436/1436 [==============================] - 1s 517us/step - loss: 1.4835 - mean_absolute_error: 0.9871\n",
      "Epoch 8/50\n",
      "1436/1436 [==============================] - 1s 486us/step - loss: 1.6117 - mean_absolute_error: 1.0107\n",
      "Epoch 9/50\n",
      "1436/1436 [==============================] - 1s 493us/step - loss: 1.5026 - mean_absolute_error: 0.9724\n",
      "Epoch 10/50\n",
      "1436/1436 [==============================] - 1s 490us/step - loss: 1.4536 - mean_absolute_error: 0.9591\n",
      "Epoch 11/50\n",
      "1436/1436 [==============================] - 1s 490us/step - loss: 1.4767 - mean_absolute_error: 0.9720\n",
      "Epoch 12/50\n",
      "1436/1436 [==============================] - 1s 483us/step - loss: 1.3506 - mean_absolute_error: 0.9319\n",
      "Epoch 13/50\n",
      "1436/1436 [==============================] - 1s 480us/step - loss: 1.3694 - mean_absolute_error: 0.9249\n",
      "Epoch 14/50\n",
      "1436/1436 [==============================] - 1s 479us/step - loss: 1.2692 - mean_absolute_error: 0.8994\n",
      "Epoch 15/50\n",
      "1436/1436 [==============================] - 1s 473us/step - loss: 1.4262 - mean_absolute_error: 0.9594\n",
      "Epoch 16/50\n",
      "1436/1436 [==============================] - 1s 476us/step - loss: 1.3366 - mean_absolute_error: 0.9149\n",
      "Epoch 17/50\n",
      "1436/1436 [==============================] - 1s 479us/step - loss: 1.2995 - mean_absolute_error: 0.9028\n",
      "Epoch 18/50\n",
      "1436/1436 [==============================] - 1s 472us/step - loss: 1.2941 - mean_absolute_error: 0.9082\n",
      "Epoch 19/50\n",
      "1436/1436 [==============================] - 1s 492us/step - loss: 1.3152 - mean_absolute_error: 0.9058\n",
      "Epoch 20/50\n",
      "1436/1436 [==============================] - 1s 488us/step - loss: 1.2636 - mean_absolute_error: 0.8967\n",
      "Epoch 21/50\n",
      "1436/1436 [==============================] - 1s 487us/step - loss: 1.2106 - mean_absolute_error: 0.8812\n",
      "Epoch 22/50\n",
      "1436/1436 [==============================] - 1s 488us/step - loss: 1.2604 - mean_absolute_error: 0.8970\n",
      "Epoch 23/50\n",
      "1436/1436 [==============================] - 1s 494us/step - loss: 1.3117 - mean_absolute_error: 0.9194\n",
      "Epoch 24/50\n",
      "1436/1436 [==============================] - 1s 486us/step - loss: 1.2362 - mean_absolute_error: 0.8902\n",
      "Epoch 25/50\n",
      "1436/1436 [==============================] - 1s 468us/step - loss: 1.2456 - mean_absolute_error: 0.8912\n",
      "Epoch 26/50\n",
      "1436/1436 [==============================] - 1s 472us/step - loss: 1.1467 - mean_absolute_error: 0.8387\n",
      "Epoch 27/50\n",
      "1436/1436 [==============================] - 1s 475us/step - loss: 1.1735 - mean_absolute_error: 0.8522\n",
      "Epoch 28/50\n",
      "1436/1436 [==============================] - 1s 476us/step - loss: 1.1766 - mean_absolute_error: 0.8668\n",
      "Epoch 29/50\n",
      "1436/1436 [==============================] - 1s 470us/step - loss: 1.1217 - mean_absolute_error: 0.8419\n",
      "Epoch 30/50\n",
      "1436/1436 [==============================] - 1s 485us/step - loss: 1.1950 - mean_absolute_error: 0.8604\n",
      "Epoch 31/50\n",
      "1436/1436 [==============================] - 1s 491us/step - loss: 1.1443 - mean_absolute_error: 0.8522\n",
      "Epoch 32/50\n",
      "1436/1436 [==============================] - 1s 492us/step - loss: 1.1559 - mean_absolute_error: 0.8530\n",
      "Epoch 33/50\n",
      "1436/1436 [==============================] - 1s 493us/step - loss: 1.1369 - mean_absolute_error: 0.8524\n",
      "Epoch 34/50\n",
      "1436/1436 [==============================] - 1s 497us/step - loss: 1.0457 - mean_absolute_error: 0.8132\n",
      "Epoch 35/50\n",
      "1436/1436 [==============================] - 1s 500us/step - loss: 1.0786 - mean_absolute_error: 0.8182\n",
      "Epoch 36/50\n",
      "1436/1436 [==============================] - 1s 476us/step - loss: 1.1187 - mean_absolute_error: 0.8366\n",
      "Epoch 37/50\n",
      "1436/1436 [==============================] - 1s 480us/step - loss: 1.0838 - mean_absolute_error: 0.8345\n",
      "Epoch 38/50\n",
      "1436/1436 [==============================] - 1s 472us/step - loss: 1.0296 - mean_absolute_error: 0.8089\n",
      "Epoch 39/50\n",
      "1436/1436 [==============================] - 1s 468us/step - loss: 1.0788 - mean_absolute_error: 0.8206\n",
      "Epoch 40/50\n",
      "1436/1436 [==============================] - 1s 471us/step - loss: 0.9343 - mean_absolute_error: 0.7704\n",
      "Epoch 41/50\n",
      "1436/1436 [==============================] - 1s 477us/step - loss: 1.0303 - mean_absolute_error: 0.8017\n",
      "Epoch 42/50\n",
      "1436/1436 [==============================] - 1s 493us/step - loss: 1.0070 - mean_absolute_error: 0.8054\n",
      "Epoch 43/50\n",
      "1436/1436 [==============================] - 1s 492us/step - loss: 1.0320 - mean_absolute_error: 0.7973\n",
      "Epoch 44/50\n",
      "1436/1436 [==============================] - 1s 498us/step - loss: 1.0480 - mean_absolute_error: 0.8029\n",
      "Epoch 45/50\n",
      "1436/1436 [==============================] - 1s 495us/step - loss: 1.0535 - mean_absolute_error: 0.8145\n",
      "Epoch 46/50\n",
      "1436/1436 [==============================] - 1s 497us/step - loss: 1.0133 - mean_absolute_error: 0.7958\n",
      "Epoch 47/50\n",
      "1436/1436 [==============================] - 1s 560us/step - loss: 0.9370 - mean_absolute_error: 0.7630\n",
      "Epoch 48/50\n",
      "1436/1436 [==============================] - 1s 652us/step - loss: 1.0145 - mean_absolute_error: 0.7903\n",
      "Epoch 49/50\n",
      "1436/1436 [==============================] - 1s 636us/step - loss: 0.9085 - mean_absolute_error: 0.7510\n",
      "Epoch 50/50\n",
      "1436/1436 [==============================] - 1s 640us/step - loss: 0.9267 - mean_absolute_error: 0.7758\n",
      "QWK:  0.7165631360988278\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1437/1437 [==============================] - 4s 3ms/step - loss: 9.9512 - mean_absolute_error: 2.3561\n",
      "Epoch 2/50\n",
      "1437/1437 [==============================] - 1s 481us/step - loss: 2.0428 - mean_absolute_error: 1.1499\n",
      "Epoch 3/50\n",
      "1437/1437 [==============================] - 1s 484us/step - loss: 1.9450 - mean_absolute_error: 1.1153\n",
      "Epoch 4/50\n",
      "1437/1437 [==============================] - 1s 485us/step - loss: 1.8532 - mean_absolute_error: 1.0851\n",
      "Epoch 5/50\n",
      "1437/1437 [==============================] - 1s 471us/step - loss: 1.6642 - mean_absolute_error: 1.0185\n",
      "Epoch 6/50\n",
      "1437/1437 [==============================] - 1s 481us/step - loss: 1.6504 - mean_absolute_error: 1.0177\n",
      "Epoch 7/50\n",
      "1437/1437 [==============================] - 1s 482us/step - loss: 1.5977 - mean_absolute_error: 1.0146\n",
      "Epoch 8/50\n",
      "1437/1437 [==============================] - 1s 468us/step - loss: 1.5542 - mean_absolute_error: 0.9948\n",
      "Epoch 9/50\n",
      "1437/1437 [==============================] - 1s 475us/step - loss: 1.4385 - mean_absolute_error: 0.9543\n",
      "Epoch 10/50\n",
      "1437/1437 [==============================] - 1s 466us/step - loss: 1.4460 - mean_absolute_error: 0.9588\n",
      "Epoch 11/50\n",
      "1437/1437 [==============================] - 1s 470us/step - loss: 1.4667 - mean_absolute_error: 0.9591\n",
      "Epoch 12/50\n",
      "1437/1437 [==============================] - 1s 473us/step - loss: 1.4199 - mean_absolute_error: 0.9535\n",
      "Epoch 13/50\n",
      "1437/1437 [==============================] - 1s 567us/step - loss: 1.4284 - mean_absolute_error: 0.9507\n",
      "Epoch 14/50\n",
      "1437/1437 [==============================] - 1s 564us/step - loss: 1.3226 - mean_absolute_error: 0.9219\n",
      "Epoch 15/50\n",
      "1437/1437 [==============================] - 1s 568us/step - loss: 1.3211 - mean_absolute_error: 0.9157\n",
      "Epoch 16/50\n",
      "1437/1437 [==============================] - 1s 574us/step - loss: 1.3517 - mean_absolute_error: 0.9309\n",
      "Epoch 17/50\n",
      "1437/1437 [==============================] - 1s 601us/step - loss: 1.2822 - mean_absolute_error: 0.8988\n",
      "Epoch 18/50\n",
      "1437/1437 [==============================] - 1s 546us/step - loss: 1.3363 - mean_absolute_error: 0.9228\n",
      "Epoch 19/50\n",
      "1437/1437 [==============================] - 1s 552us/step - loss: 1.3077 - mean_absolute_error: 0.9146\n",
      "Epoch 20/50\n",
      "1437/1437 [==============================] - 1s 557us/step - loss: 1.3048 - mean_absolute_error: 0.8958\n",
      "Epoch 21/50\n",
      "1437/1437 [==============================] - 1s 586us/step - loss: 1.2396 - mean_absolute_error: 0.8879\n",
      "Epoch 22/50\n",
      "1437/1437 [==============================] - 1s 536us/step - loss: 1.2068 - mean_absolute_error: 0.8751\n",
      "Epoch 23/50\n",
      "1437/1437 [==============================] - 1s 534us/step - loss: 1.2735 - mean_absolute_error: 0.9000\n",
      "Epoch 24/50\n",
      "1437/1437 [==============================] - 1s 562us/step - loss: 1.2384 - mean_absolute_error: 0.8959\n",
      "Epoch 25/50\n",
      "1437/1437 [==============================] - 1s 538us/step - loss: 1.2684 - mean_absolute_error: 0.8884\n",
      "Epoch 26/50\n",
      "1437/1437 [==============================] - 1s 512us/step - loss: 1.1610 - mean_absolute_error: 0.8669\n",
      "Epoch 27/50\n",
      "1437/1437 [==============================] - 1s 507us/step - loss: 1.1864 - mean_absolute_error: 0.8746\n",
      "Epoch 28/50\n",
      "1437/1437 [==============================] - 1s 528us/step - loss: 1.2171 - mean_absolute_error: 0.8772\n",
      "Epoch 29/50\n",
      "1437/1437 [==============================] - 1s 564us/step - loss: 1.1715 - mean_absolute_error: 0.8519\n",
      "Epoch 30/50\n",
      "1437/1437 [==============================] - 1s 504us/step - loss: 1.1020 - mean_absolute_error: 0.8404\n",
      "Epoch 31/50\n",
      "1437/1437 [==============================] - 1s 513us/step - loss: 1.2302 - mean_absolute_error: 0.8932\n",
      "Epoch 32/50\n",
      "1437/1437 [==============================] - 1s 521us/step - loss: 1.1696 - mean_absolute_error: 0.8571\n",
      "Epoch 33/50\n",
      "1437/1437 [==============================] - 1s 636us/step - loss: 1.1935 - mean_absolute_error: 0.8695\n",
      "Epoch 34/50\n",
      "1437/1437 [==============================] - 1s 738us/step - loss: 1.1333 - mean_absolute_error: 0.8466\n",
      "Epoch 35/50\n",
      "1437/1437 [==============================] - 1s 681us/step - loss: 1.1243 - mean_absolute_error: 0.8460\n",
      "Epoch 36/50\n",
      "1437/1437 [==============================] - 1s 680us/step - loss: 1.0762 - mean_absolute_error: 0.8218\n",
      "Epoch 37/50\n",
      "1437/1437 [==============================] - 1s 689us/step - loss: 1.0954 - mean_absolute_error: 0.8402\n",
      "Epoch 38/50\n",
      "1437/1437 [==============================] - 1s 604us/step - loss: 1.1113 - mean_absolute_error: 0.8387\n",
      "Epoch 39/50\n",
      "1437/1437 [==============================] - 1s 577us/step - loss: 1.0719 - mean_absolute_error: 0.8165\n",
      "Epoch 40/50\n",
      "1437/1437 [==============================] - 1s 582us/step - loss: 1.0547 - mean_absolute_error: 0.8087\n",
      "Epoch 41/50\n",
      "1437/1437 [==============================] - 1s 581us/step - loss: 1.1016 - mean_absolute_error: 0.8304\n",
      "Epoch 42/50\n",
      "1437/1437 [==============================] - 1s 498us/step - loss: 1.0987 - mean_absolute_error: 0.8357\n",
      "Epoch 43/50\n",
      "1437/1437 [==============================] - 1s 490us/step - loss: 1.1017 - mean_absolute_error: 0.8311\n",
      "Epoch 44/50\n",
      "1437/1437 [==============================] - 1s 499us/step - loss: 1.0020 - mean_absolute_error: 0.7986\n",
      "Epoch 45/50\n",
      "1437/1437 [==============================] - 1s 492us/step - loss: 1.0045 - mean_absolute_error: 0.7961\n",
      "Epoch 46/50\n",
      "1437/1437 [==============================] - 1s 489us/step - loss: 0.9520 - mean_absolute_error: 0.7882\n",
      "Epoch 47/50\n",
      "1437/1437 [==============================] - 1s 528us/step - loss: 0.9729 - mean_absolute_error: 0.7815\n",
      "Epoch 48/50\n",
      "1437/1437 [==============================] - 1s 581us/step - loss: 0.9618 - mean_absolute_error: 0.7794\n",
      "Epoch 49/50\n",
      "1437/1437 [==============================] - 1s 573us/step - loss: 1.0233 - mean_absolute_error: 0.8019\n",
      "Epoch 50/50\n",
      "1437/1437 [==============================] - 1s 567us/step - loss: 0.9771 - mean_absolute_error: 0.7965\n",
      "QWK:  0.6928220350501813\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 4s 3ms/step - loss: 9.8868 - mean_absolute_error: 2.3230\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 1s 477us/step - loss: 2.0354 - mean_absolute_error: 1.1455\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 1s 473us/step - loss: 1.8312 - mean_absolute_error: 1.0724\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 1s 482us/step - loss: 1.7595 - mean_absolute_error: 1.0442\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 1s 478us/step - loss: 1.5266 - mean_absolute_error: 0.9944\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 1s 472us/step - loss: 1.5488 - mean_absolute_error: 0.9998\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 1s 480us/step - loss: 1.6093 - mean_absolute_error: 1.0217\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 1s 516us/step - loss: 1.4866 - mean_absolute_error: 0.9735\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 1s 579us/step - loss: 1.4367 - mean_absolute_error: 0.9563\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 1s 575us/step - loss: 1.5462 - mean_absolute_error: 0.9977\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 1s 575us/step - loss: 1.4501 - mean_absolute_error: 0.9621\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 1s 568us/step - loss: 1.4588 - mean_absolute_error: 0.9523\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 1s 532us/step - loss: 1.4346 - mean_absolute_error: 0.9568\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 1s 491us/step - loss: 1.4387 - mean_absolute_error: 0.9521\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 1s 498us/step - loss: 1.3591 - mean_absolute_error: 0.9268\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 1s 495us/step - loss: 1.3559 - mean_absolute_error: 0.9436\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 1s 502us/step - loss: 1.2785 - mean_absolute_error: 0.9071\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 1s 494us/step - loss: 1.3087 - mean_absolute_error: 0.9142\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442/1442 [==============================] - 1s 464us/step - loss: 1.3394 - mean_absolute_error: 0.9258\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 1s 468us/step - loss: 1.2429 - mean_absolute_error: 0.8847\n",
      "Epoch 21/50\n",
      "1442/1442 [==============================] - 1s 465us/step - loss: 1.2365 - mean_absolute_error: 0.8839\n",
      "Epoch 22/50\n",
      "1442/1442 [==============================] - 1s 470us/step - loss: 1.2950 - mean_absolute_error: 0.9104\n",
      "Epoch 23/50\n",
      "1442/1442 [==============================] - 1s 468us/step - loss: 1.2345 - mean_absolute_error: 0.8838\n",
      "Epoch 24/50\n",
      "1442/1442 [==============================] - 1s 468us/step - loss: 1.2133 - mean_absolute_error: 0.8760\n",
      "Epoch 25/50\n",
      "1442/1442 [==============================] - 1s 488us/step - loss: 1.1651 - mean_absolute_error: 0.8645\n",
      "Epoch 26/50\n",
      "1442/1442 [==============================] - 1s 489us/step - loss: 1.2137 - mean_absolute_error: 0.8758\n",
      "Epoch 27/50\n",
      "1442/1442 [==============================] - 1s 485us/step - loss: 1.1933 - mean_absolute_error: 0.8664\n",
      "Epoch 28/50\n",
      "1442/1442 [==============================] - 1s 486us/step - loss: 1.1175 - mean_absolute_error: 0.8505\n",
      "Epoch 29/50\n",
      "1442/1442 [==============================] - 1s 493us/step - loss: 1.0937 - mean_absolute_error: 0.8299\n",
      "Epoch 30/50\n",
      "1442/1442 [==============================] - 1s 541us/step - loss: 1.1488 - mean_absolute_error: 0.8459\n",
      "Epoch 31/50\n",
      "1442/1442 [==============================] - 1s 627us/step - loss: 1.1136 - mean_absolute_error: 0.8407\n",
      "Epoch 32/50\n",
      "1442/1442 [==============================] - 1s 632us/step - loss: 1.2229 - mean_absolute_error: 0.8901\n",
      "Epoch 33/50\n",
      "1442/1442 [==============================] - 1s 632us/step - loss: 1.1197 - mean_absolute_error: 0.8446\n",
      "Epoch 34/50\n",
      "1442/1442 [==============================] - 1s 626us/step - loss: 1.1008 - mean_absolute_error: 0.8375\n",
      "Epoch 35/50\n",
      "1442/1442 [==============================] - 1s 574us/step - loss: 1.0729 - mean_absolute_error: 0.8156\n",
      "Epoch 36/50\n",
      "1442/1442 [==============================] - 1s 576us/step - loss: 1.0241 - mean_absolute_error: 0.8101\n",
      "Epoch 37/50\n",
      "1442/1442 [==============================] - 1s 568us/step - loss: 1.1077 - mean_absolute_error: 0.8384\n",
      "Epoch 38/50\n",
      "1442/1442 [==============================] - 1s 559us/step - loss: 1.0255 - mean_absolute_error: 0.8032\n",
      "Epoch 39/50\n",
      "1442/1442 [==============================] - 1s 565us/step - loss: 1.0954 - mean_absolute_error: 0.8342\n",
      "Epoch 40/50\n",
      "1442/1442 [==============================] - 1s 488us/step - loss: 1.0550 - mean_absolute_error: 0.8156\n",
      "Epoch 41/50\n",
      "1442/1442 [==============================] - 1s 492us/step - loss: 0.9984 - mean_absolute_error: 0.7830\n",
      "Epoch 42/50\n",
      "1442/1442 [==============================] - 1s 492us/step - loss: 1.0242 - mean_absolute_error: 0.7975\n",
      "Epoch 43/50\n",
      "1442/1442 [==============================] - 1s 493us/step - loss: 1.0127 - mean_absolute_error: 0.7926\n",
      "Epoch 44/50\n",
      "1442/1442 [==============================] - 1s 485us/step - loss: 0.9668 - mean_absolute_error: 0.7745\n",
      "Epoch 45/50\n",
      "1442/1442 [==============================] - 1s 478us/step - loss: 0.8948 - mean_absolute_error: 0.7487\n",
      "Epoch 46/50\n",
      "1442/1442 [==============================] - 1s 481us/step - loss: 0.9705 - mean_absolute_error: 0.7855\n",
      "Epoch 47/50\n",
      "1442/1442 [==============================] - 1s 493us/step - loss: 0.9531 - mean_absolute_error: 0.7774\n",
      "Epoch 48/50\n",
      "1442/1442 [==============================] - 1s 474us/step - loss: 0.9770 - mean_absolute_error: 0.7783\n",
      "Epoch 49/50\n",
      "1442/1442 [==============================] - 1s 478us/step - loss: 0.9612 - mean_absolute_error: 0.7783\n",
      "Epoch 50/50\n",
      "1442/1442 [==============================] - 1s 473us/step - loss: 0.9306 - mean_absolute_error: 0.7584\n",
      "QWK:  0.6386731592475596\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 4s 3ms/step - loss: 9.9590 - mean_absolute_error: 2.3520\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 1s 477us/step - loss: 2.0442 - mean_absolute_error: 1.1371\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 1s 471us/step - loss: 1.8681 - mean_absolute_error: 1.0856\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 1s 479us/step - loss: 1.6858 - mean_absolute_error: 1.0350\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 1s 472us/step - loss: 1.7765 - mean_absolute_error: 1.0386\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 1s 476us/step - loss: 1.6133 - mean_absolute_error: 1.0123\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 1s 473us/step - loss: 1.5760 - mean_absolute_error: 0.9980\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 1s 478us/step - loss: 1.6147 - mean_absolute_error: 1.0039\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 1s 473us/step - loss: 1.4532 - mean_absolute_error: 0.9699\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 1s 477us/step - loss: 1.4828 - mean_absolute_error: 0.9759\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 1s 472us/step - loss: 1.5026 - mean_absolute_error: 0.9855\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 1s 537us/step - loss: 1.4319 - mean_absolute_error: 0.9562\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 1s 567us/step - loss: 1.3317 - mean_absolute_error: 0.9252\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 1s 571us/step - loss: 1.3417 - mean_absolute_error: 0.9211\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 1s 574us/step - loss: 1.3848 - mean_absolute_error: 0.9351\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 1s 578us/step - loss: 1.4768 - mean_absolute_error: 0.9618\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 1s 510us/step - loss: 1.3398 - mean_absolute_error: 0.9067\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 1s 502us/step - loss: 1.4119 - mean_absolute_error: 0.9455\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 1s 494us/step - loss: 1.2718 - mean_absolute_error: 0.8970\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 1s 498us/step - loss: 1.2386 - mean_absolute_error: 0.8735\n",
      "Epoch 21/50\n",
      "1442/1442 [==============================] - 1s 491us/step - loss: 1.3322 - mean_absolute_error: 0.9177\n",
      "Epoch 22/50\n",
      "1442/1442 [==============================] - 1s 515us/step - loss: 1.1816 - mean_absolute_error: 0.8748\n",
      "Epoch 23/50\n",
      "1442/1442 [==============================] - 1s 577us/step - loss: 1.2450 - mean_absolute_error: 0.8901\n",
      "Epoch 24/50\n",
      "1442/1442 [==============================] - 1s 571us/step - loss: 1.2571 - mean_absolute_error: 0.8970\n",
      "Epoch 25/50\n",
      "1442/1442 [==============================] - 1s 570us/step - loss: 1.1915 - mean_absolute_error: 0.8637\n",
      "Epoch 26/50\n",
      "1442/1442 [==============================] - 1s 583us/step - loss: 1.2587 - mean_absolute_error: 0.8963\n",
      "Epoch 27/50\n",
      "1442/1442 [==============================] - 1s 552us/step - loss: 1.2441 - mean_absolute_error: 0.8833\n",
      "Epoch 28/50\n",
      "1442/1442 [==============================] - 1s 494us/step - loss: 1.1730 - mean_absolute_error: 0.8575\n",
      "Epoch 29/50\n",
      "1442/1442 [==============================] - 1s 497us/step - loss: 1.1792 - mean_absolute_error: 0.8682\n",
      "Epoch 30/50\n",
      "1442/1442 [==============================] - 1s 507us/step - loss: 1.1365 - mean_absolute_error: 0.8495\n",
      "Epoch 31/50\n",
      "1442/1442 [==============================] - 1s 498us/step - loss: 1.2125 - mean_absolute_error: 0.8717\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442/1442 [==============================] - 1s 492us/step - loss: 1.1419 - mean_absolute_error: 0.8457\n",
      "Epoch 33/50\n",
      "1442/1442 [==============================] - 1s 549us/step - loss: 1.1156 - mean_absolute_error: 0.8406\n",
      "Epoch 34/50\n",
      "1442/1442 [==============================] - 1s 559us/step - loss: 1.1250 - mean_absolute_error: 0.8444\n",
      "Epoch 35/50\n",
      "1442/1442 [==============================] - 1s 564us/step - loss: 1.0946 - mean_absolute_error: 0.8275\n",
      "Epoch 36/50\n",
      "1442/1442 [==============================] - 1s 562us/step - loss: 1.1117 - mean_absolute_error: 0.8405\n",
      "Epoch 37/50\n",
      "1442/1442 [==============================] - 1s 565us/step - loss: 1.0407 - mean_absolute_error: 0.8116\n",
      "Epoch 38/50\n",
      "1442/1442 [==============================] - 1s 493us/step - loss: 1.0635 - mean_absolute_error: 0.8191\n",
      "Epoch 39/50\n",
      "1442/1442 [==============================] - 1s 486us/step - loss: 1.0397 - mean_absolute_error: 0.8108\n",
      "Epoch 40/50\n",
      "1442/1442 [==============================] - 1s 502us/step - loss: 1.0174 - mean_absolute_error: 0.7965\n",
      "Epoch 41/50\n",
      "1442/1442 [==============================] - 1s 494us/step - loss: 0.9505 - mean_absolute_error: 0.7726\n",
      "Epoch 42/50\n",
      "1442/1442 [==============================] - 1s 519us/step - loss: 1.0712 - mean_absolute_error: 0.8176\n",
      "Epoch 43/50\n",
      "1442/1442 [==============================] - 1s 550us/step - loss: 1.0483 - mean_absolute_error: 0.8137\n",
      "Epoch 44/50\n",
      "1442/1442 [==============================] - 1s 684us/step - loss: 1.0245 - mean_absolute_error: 0.8006\n",
      "Epoch 45/50\n",
      "1442/1442 [==============================] - 1s 658us/step - loss: 1.0429 - mean_absolute_error: 0.8103\n",
      "Epoch 46/50\n",
      "1442/1442 [==============================] - 1s 634us/step - loss: 1.0208 - mean_absolute_error: 0.8056\n",
      "Epoch 47/50\n",
      "1442/1442 [==============================] - 1s 629us/step - loss: 0.9230 - mean_absolute_error: 0.7621\n",
      "Epoch 48/50\n",
      "1442/1442 [==============================] - 1s 546us/step - loss: 0.9075 - mean_absolute_error: 0.7525\n",
      "Epoch 49/50\n",
      "1442/1442 [==============================] - 1s 540us/step - loss: 0.9079 - mean_absolute_error: 0.7678\n",
      "Epoch 50/50\n",
      "1442/1442 [==============================] - 1s 538us/step - loss: 0.9400 - mean_absolute_error: 0.7769\n",
      "QWK:  0.6843224942520023\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 5s 3ms/step - loss: 9.5853 - mean_absolute_error: 2.3137\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 1s 474us/step - loss: 2.1764 - mean_absolute_error: 1.1681\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 1s 478us/step - loss: 1.7874 - mean_absolute_error: 1.0730\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 1s 471us/step - loss: 1.7656 - mean_absolute_error: 1.0626\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 1s 474us/step - loss: 1.5463 - mean_absolute_error: 1.0011\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 1s 476us/step - loss: 1.4857 - mean_absolute_error: 0.9796\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 1s 464us/step - loss: 1.5701 - mean_absolute_error: 0.9973\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 1s 571us/step - loss: 1.4982 - mean_absolute_error: 0.9579\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 1s 584us/step - loss: 1.4356 - mean_absolute_error: 0.9556\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 1s 568us/step - loss: 1.4870 - mean_absolute_error: 0.9705\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 1s 565us/step - loss: 1.4096 - mean_absolute_error: 0.9567\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 1s 572us/step - loss: 1.4606 - mean_absolute_error: 0.9680\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 1s 499us/step - loss: 1.4757 - mean_absolute_error: 0.9600\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 1s 492us/step - loss: 1.4833 - mean_absolute_error: 0.9753\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 1s 494us/step - loss: 1.3257 - mean_absolute_error: 0.9097\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 1s 488us/step - loss: 1.3776 - mean_absolute_error: 0.9439\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 1s 494us/step - loss: 1.4158 - mean_absolute_error: 0.9455\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 1s 515us/step - loss: 1.3258 - mean_absolute_error: 0.9200\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 1s 566us/step - loss: 1.3079 - mean_absolute_error: 0.9132\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 1s 579us/step - loss: 1.3638 - mean_absolute_error: 0.9356\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 1s 569us/step - loss: 1.3360 - mean_absolute_error: 0.9174\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 1s 565us/step - loss: 1.2475 - mean_absolute_error: 0.8847\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 1s 542us/step - loss: 1.3015 - mean_absolute_error: 0.9108\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 1s 493us/step - loss: 1.1923 - mean_absolute_error: 0.8745\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 1s 495us/step - loss: 1.1735 - mean_absolute_error: 0.8601\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 1s 489us/step - loss: 1.2415 - mean_absolute_error: 0.8944\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 1s 495us/step - loss: 1.2708 - mean_absolute_error: 0.8955\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 1s 488us/step - loss: 1.2107 - mean_absolute_error: 0.8680\n",
      "Epoch 29/50\n",
      "1443/1443 [==============================] - 1s 553us/step - loss: 1.1978 - mean_absolute_error: 0.8761\n",
      "Epoch 30/50\n",
      "1443/1443 [==============================] - 1s 573us/step - loss: 1.1790 - mean_absolute_error: 0.8647\n",
      "Epoch 31/50\n",
      "1443/1443 [==============================] - 1s 566us/step - loss: 1.2415 - mean_absolute_error: 0.8868\n",
      "Epoch 32/50\n",
      "1443/1443 [==============================] - 1s 568us/step - loss: 1.1322 - mean_absolute_error: 0.8571\n",
      "Epoch 33/50\n",
      "1443/1443 [==============================] - 1s 569us/step - loss: 1.1058 - mean_absolute_error: 0.8382\n",
      "Epoch 34/50\n",
      "1443/1443 [==============================] - 1s 510us/step - loss: 1.1214 - mean_absolute_error: 0.8358\n",
      "Epoch 35/50\n",
      "1443/1443 [==============================] - 1s 495us/step - loss: 1.1156 - mean_absolute_error: 0.8294\n",
      "Epoch 36/50\n",
      "1443/1443 [==============================] - 1s 490us/step - loss: 1.0851 - mean_absolute_error: 0.8255\n",
      "Epoch 37/50\n",
      "1443/1443 [==============================] - 1s 494us/step - loss: 1.0814 - mean_absolute_error: 0.8251\n",
      "Epoch 38/50\n",
      "1443/1443 [==============================] - 1s 490us/step - loss: 1.0424 - mean_absolute_error: 0.8101\n",
      "Epoch 39/50\n",
      "1443/1443 [==============================] - 1s 508us/step - loss: 1.0942 - mean_absolute_error: 0.8250\n",
      "Epoch 40/50\n",
      "1443/1443 [==============================] - 1s 577us/step - loss: 1.0377 - mean_absolute_error: 0.8180\n",
      "Epoch 41/50\n",
      "1443/1443 [==============================] - 1s 572us/step - loss: 1.0280 - mean_absolute_error: 0.8081\n",
      "Epoch 42/50\n",
      "1443/1443 [==============================] - 1s 573us/step - loss: 1.0495 - mean_absolute_error: 0.8135\n",
      "Epoch 43/50\n",
      "1443/1443 [==============================] - 1s 571us/step - loss: 1.0499 - mean_absolute_error: 0.8145\n",
      "Epoch 44/50\n",
      "1443/1443 [==============================] - 1s 539us/step - loss: 0.9953 - mean_absolute_error: 0.7937\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443/1443 [==============================] - 1s 481us/step - loss: 0.9645 - mean_absolute_error: 0.7847\n",
      "Epoch 46/50\n",
      "1443/1443 [==============================] - 1s 489us/step - loss: 0.9635 - mean_absolute_error: 0.7719\n",
      "Epoch 47/50\n",
      "1443/1443 [==============================] - 1s 480us/step - loss: 0.9068 - mean_absolute_error: 0.7563\n",
      "Epoch 48/50\n",
      "1443/1443 [==============================] - 1s 482us/step - loss: 0.9628 - mean_absolute_error: 0.7827\n",
      "Epoch 49/50\n",
      "1443/1443 [==============================] - 1s 481us/step - loss: 0.9302 - mean_absolute_error: 0.7736\n",
      "Epoch 50/50\n",
      "1443/1443 [==============================] - 1s 541us/step - loss: 1.0403 - mean_absolute_error: 0.8064\n",
      "QWK:  0.6827257125089667\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.683\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1379/1379 [==============================] - 5s 4ms/step - loss: 15.7283 - mean_absolute_error: 3.1101\n",
      "Epoch 2/50\n",
      "1379/1379 [==============================] - 1s 459us/step - loss: 6.1770 - mean_absolute_error: 1.9913\n",
      "Epoch 3/50\n",
      "1379/1379 [==============================] - 1s 461us/step - loss: 5.4592 - mean_absolute_error: 1.8392\n",
      "Epoch 4/50\n",
      "1379/1379 [==============================] - 1s 454us/step - loss: 5.1855 - mean_absolute_error: 1.7740\n",
      "Epoch 5/50\n",
      "1379/1379 [==============================] - 1s 468us/step - loss: 5.1509 - mean_absolute_error: 1.7805\n",
      "Epoch 6/50\n",
      "1379/1379 [==============================] - 1s 492us/step - loss: 4.7011 - mean_absolute_error: 1.7040\n",
      "Epoch 7/50\n",
      "1379/1379 [==============================] - 1s 488us/step - loss: 4.7900 - mean_absolute_error: 1.7224\n",
      "Epoch 8/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 4.6984 - mean_absolute_error: 1.6978\n",
      "Epoch 9/50\n",
      "1379/1379 [==============================] - 1s 465us/step - loss: 4.5558 - mean_absolute_error: 1.6854\n",
      "Epoch 10/50\n",
      "1379/1379 [==============================] - 1s 469us/step - loss: 4.7932 - mean_absolute_error: 1.7180\n",
      "Epoch 11/50\n",
      "1379/1379 [==============================] - 1s 468us/step - loss: 4.5755 - mean_absolute_error: 1.6474\n",
      "Epoch 12/50\n",
      "1379/1379 [==============================] - 1s 522us/step - loss: 4.4628 - mean_absolute_error: 1.6333\n",
      "Epoch 13/50\n",
      "1379/1379 [==============================] - 1s 577us/step - loss: 4.3641 - mean_absolute_error: 1.6356\n",
      "Epoch 14/50\n",
      "1379/1379 [==============================] - 1s 568us/step - loss: 4.3738 - mean_absolute_error: 1.6354\n",
      "Epoch 15/50\n",
      "1379/1379 [==============================] - 1s 587us/step - loss: 4.2226 - mean_absolute_error: 1.6008\n",
      "Epoch 16/50\n",
      "1379/1379 [==============================] - 1s 559us/step - loss: 4.4861 - mean_absolute_error: 1.6638\n",
      "Epoch 17/50\n",
      "1379/1379 [==============================] - 1s 529us/step - loss: 4.2822 - mean_absolute_error: 1.6098\n",
      "Epoch 18/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 4.1954 - mean_absolute_error: 1.6013\n",
      "Epoch 19/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 4.1706 - mean_absolute_error: 1.6032\n",
      "Epoch 20/50\n",
      "1379/1379 [==============================] - 1s 478us/step - loss: 4.1210 - mean_absolute_error: 1.5785\n",
      "Epoch 21/50\n",
      "1379/1379 [==============================] - 1s 492us/step - loss: 4.0989 - mean_absolute_error: 1.5733\n",
      "Epoch 22/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 4.1272 - mean_absolute_error: 1.5790\n",
      "Epoch 23/50\n",
      "1379/1379 [==============================] - 1s 519us/step - loss: 4.0480 - mean_absolute_error: 1.5732\n",
      "Epoch 24/50\n",
      "1379/1379 [==============================] - 1s 566us/step - loss: 3.9749 - mean_absolute_error: 1.5278\n",
      "Epoch 25/50\n",
      "1379/1379 [==============================] - 1s 570us/step - loss: 3.9416 - mean_absolute_error: 1.5421\n",
      "Epoch 26/50\n",
      "1379/1379 [==============================] - 1s 569us/step - loss: 3.8207 - mean_absolute_error: 1.5131\n",
      "Epoch 27/50\n",
      "1379/1379 [==============================] - 1s 566us/step - loss: 3.7438 - mean_absolute_error: 1.4903\n",
      "Epoch 28/50\n",
      "1379/1379 [==============================] - 1s 537us/step - loss: 3.6854 - mean_absolute_error: 1.4891\n",
      "Epoch 29/50\n",
      "1379/1379 [==============================] - 1s 485us/step - loss: 3.5490 - mean_absolute_error: 1.4631\n",
      "Epoch 30/50\n",
      "1379/1379 [==============================] - 1s 493us/step - loss: 3.5990 - mean_absolute_error: 1.4644\n",
      "Epoch 31/50\n",
      "1379/1379 [==============================] - 1s 481us/step - loss: 3.4904 - mean_absolute_error: 1.4407\n",
      "Epoch 32/50\n",
      "1379/1379 [==============================] - 1s 483us/step - loss: 3.4483 - mean_absolute_error: 1.4283\n",
      "Epoch 33/50\n",
      "1379/1379 [==============================] - 1s 500us/step - loss: 3.2139 - mean_absolute_error: 1.3820\n",
      "Epoch 34/50\n",
      "1379/1379 [==============================] - 1s 520us/step - loss: 3.3422 - mean_absolute_error: 1.4062\n",
      "Epoch 35/50\n",
      "1379/1379 [==============================] - 1s 565us/step - loss: 3.2753 - mean_absolute_error: 1.3951\n",
      "Epoch 36/50\n",
      "1379/1379 [==============================] - 1s 569us/step - loss: 3.0696 - mean_absolute_error: 1.3549\n",
      "Epoch 37/50\n",
      "1379/1379 [==============================] - 1s 568us/step - loss: 2.8503 - mean_absolute_error: 1.2961\n",
      "Epoch 38/50\n",
      "1379/1379 [==============================] - 1s 570us/step - loss: 3.0032 - mean_absolute_error: 1.3539\n",
      "Epoch 39/50\n",
      "1379/1379 [==============================] - 1s 547us/step - loss: 3.0240 - mean_absolute_error: 1.3647\n",
      "Epoch 40/50\n",
      "1379/1379 [==============================] - 1s 493us/step - loss: 2.7468 - mean_absolute_error: 1.2770\n",
      "Epoch 41/50\n",
      "1379/1379 [==============================] - 1s 491us/step - loss: 2.6259 - mean_absolute_error: 1.2637\n",
      "Epoch 42/50\n",
      "1379/1379 [==============================] - 1s 487us/step - loss: 2.8476 - mean_absolute_error: 1.2900\n",
      "Epoch 43/50\n",
      "1379/1379 [==============================] - 1s 485us/step - loss: 2.7138 - mean_absolute_error: 1.2707\n",
      "Epoch 44/50\n",
      "1379/1379 [==============================] - 1s 487us/step - loss: 2.6847 - mean_absolute_error: 1.2541\n",
      "Epoch 45/50\n",
      "1379/1379 [==============================] - 1s 511us/step - loss: 2.3446 - mean_absolute_error: 1.1821\n",
      "Epoch 46/50\n",
      "1379/1379 [==============================] - 1s 568us/step - loss: 2.6427 - mean_absolute_error: 1.2689\n",
      "Epoch 47/50\n",
      "1379/1379 [==============================] - 1s 577us/step - loss: 2.4664 - mean_absolute_error: 1.2118\n",
      "Epoch 48/50\n",
      "1379/1379 [==============================] - 1s 572us/step - loss: 2.2256 - mean_absolute_error: 1.1543\n",
      "Epoch 49/50\n",
      "1379/1379 [==============================] - 1s 566us/step - loss: 2.4275 - mean_absolute_error: 1.2022\n",
      "Epoch 50/50\n",
      "1379/1379 [==============================] - 1s 552us/step - loss: 2.2580 - mean_absolute_error: 1.1601\n",
      "QWK:  0.6321441953683493\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1379/1379 [==============================] - 6s 4ms/step - loss: 15.0080 - mean_absolute_error: 3.0231\n",
      "Epoch 2/50\n",
      "1379/1379 [==============================] - 1s 475us/step - loss: 5.7296 - mean_absolute_error: 1.8841\n",
      "Epoch 3/50\n",
      "1379/1379 [==============================] - 1s 512us/step - loss: 5.2694 - mean_absolute_error: 1.8231\n",
      "Epoch 4/50\n",
      "1379/1379 [==============================] - 1s 471us/step - loss: 4.9814 - mean_absolute_error: 1.7604\n",
      "Epoch 5/50\n",
      "1379/1379 [==============================] - 1s 470us/step - loss: 4.6416 - mean_absolute_error: 1.6796\n",
      "Epoch 6/50\n",
      "1379/1379 [==============================] - 1s 476us/step - loss: 4.8255 - mean_absolute_error: 1.7194\n",
      "Epoch 7/50\n",
      "1379/1379 [==============================] - 1s 478us/step - loss: 4.6190 - mean_absolute_error: 1.6841\n",
      "Epoch 8/50\n",
      "1379/1379 [==============================] - 1s 483us/step - loss: 4.4171 - mean_absolute_error: 1.6586\n",
      "Epoch 9/50\n",
      "1379/1379 [==============================] - 1s 475us/step - loss: 4.3959 - mean_absolute_error: 1.6367\n",
      "Epoch 10/50\n",
      "1379/1379 [==============================] - 1s 487us/step - loss: 4.4335 - mean_absolute_error: 1.6396\n",
      "Epoch 11/50\n",
      "1379/1379 [==============================] - 1s 484us/step - loss: 4.3043 - mean_absolute_error: 1.6196\n",
      "Epoch 12/50\n",
      "1379/1379 [==============================] - 1s 541us/step - loss: 4.2879 - mean_absolute_error: 1.6071\n",
      "Epoch 13/50\n",
      "1379/1379 [==============================] - 1s 634us/step - loss: 4.2736 - mean_absolute_error: 1.6170\n",
      "Epoch 14/50\n",
      "1379/1379 [==============================] - 1s 635us/step - loss: 4.3091 - mean_absolute_error: 1.5999\n",
      "Epoch 15/50\n",
      "1379/1379 [==============================] - 1s 630us/step - loss: 4.3499 - mean_absolute_error: 1.6132\n",
      "Epoch 16/50\n",
      "1379/1379 [==============================] - 1s 630us/step - loss: 4.0735 - mean_absolute_error: 1.5775\n",
      "Epoch 17/50\n",
      "1379/1379 [==============================] - 1s 576us/step - loss: 3.9738 - mean_absolute_error: 1.5524\n",
      "Epoch 18/50\n",
      "1379/1379 [==============================] - 1s 562us/step - loss: 3.8870 - mean_absolute_error: 1.5352\n",
      "Epoch 19/50\n",
      "1379/1379 [==============================] - 1s 565us/step - loss: 3.9452 - mean_absolute_error: 1.5477\n",
      "Epoch 20/50\n",
      "1379/1379 [==============================] - 1s 561us/step - loss: 3.9775 - mean_absolute_error: 1.5347\n",
      "Epoch 21/50\n",
      "1379/1379 [==============================] - 1s 560us/step - loss: 3.7753 - mean_absolute_error: 1.5004\n",
      "Epoch 22/50\n",
      "1379/1379 [==============================] - 1s 513us/step - loss: 3.6604 - mean_absolute_error: 1.4726\n",
      "Epoch 23/50\n",
      "1379/1379 [==============================] - 1s 484us/step - loss: 3.7598 - mean_absolute_error: 1.5085\n",
      "Epoch 24/50\n",
      "1379/1379 [==============================] - 1s 482us/step - loss: 3.5001 - mean_absolute_error: 1.4513\n",
      "Epoch 25/50\n",
      "1379/1379 [==============================] - 1s 491us/step - loss: 3.6040 - mean_absolute_error: 1.4714\n",
      "Epoch 26/50\n",
      "1379/1379 [==============================] - 1s 485us/step - loss: 3.5629 - mean_absolute_error: 1.4534\n",
      "Epoch 27/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 3.3936 - mean_absolute_error: 1.4240\n",
      "Epoch 28/50\n",
      "1379/1379 [==============================] - 1s 541us/step - loss: 3.4495 - mean_absolute_error: 1.4390\n",
      "Epoch 29/50\n",
      "1379/1379 [==============================] - 1s 578us/step - loss: 3.3115 - mean_absolute_error: 1.4062\n",
      "Epoch 30/50\n",
      "1379/1379 [==============================] - 1s 587us/step - loss: 3.2888 - mean_absolute_error: 1.3936\n",
      "Epoch 31/50\n",
      "1379/1379 [==============================] - 1s 563us/step - loss: 3.1396 - mean_absolute_error: 1.3560\n",
      "Epoch 32/50\n",
      "1379/1379 [==============================] - 1s 566us/step - loss: 2.9421 - mean_absolute_error: 1.3228\n",
      "Epoch 33/50\n",
      "1379/1379 [==============================] - 1s 517us/step - loss: 3.1404 - mean_absolute_error: 1.3673\n",
      "Epoch 34/50\n",
      "1379/1379 [==============================] - 1s 488us/step - loss: 2.8847 - mean_absolute_error: 1.3052\n",
      "Epoch 35/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 3.0428 - mean_absolute_error: 1.3294\n",
      "Epoch 36/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 2.9862 - mean_absolute_error: 1.3496\n",
      "Epoch 37/50\n",
      "1379/1379 [==============================] - 1s 494us/step - loss: 2.7825 - mean_absolute_error: 1.2775\n",
      "Epoch 38/50\n",
      "1379/1379 [==============================] - 1s 479us/step - loss: 2.7981 - mean_absolute_error: 1.2745\n",
      "Epoch 39/50\n",
      "1379/1379 [==============================] - 1s 537us/step - loss: 2.7220 - mean_absolute_error: 1.2660\n",
      "Epoch 40/50\n",
      "1379/1379 [==============================] - 1s 574us/step - loss: 2.5536 - mean_absolute_error: 1.2401\n",
      "Epoch 41/50\n",
      "1379/1379 [==============================] - 1s 560us/step - loss: 2.6285 - mean_absolute_error: 1.2446\n",
      "Epoch 42/50\n",
      "1379/1379 [==============================] - 1s 567us/step - loss: 2.6531 - mean_absolute_error: 1.2466\n",
      "Epoch 43/50\n",
      "1379/1379 [==============================] - 1s 574us/step - loss: 2.5555 - mean_absolute_error: 1.2399\n",
      "Epoch 44/50\n",
      "1379/1379 [==============================] - 1s 526us/step - loss: 2.4933 - mean_absolute_error: 1.2102\n",
      "Epoch 45/50\n",
      "1379/1379 [==============================] - 1s 491us/step - loss: 2.3200 - mean_absolute_error: 1.1762\n",
      "Epoch 46/50\n",
      "1379/1379 [==============================] - 1s 488us/step - loss: 2.2144 - mean_absolute_error: 1.1523\n",
      "Epoch 47/50\n",
      "1379/1379 [==============================] - 1s 486us/step - loss: 2.4797 - mean_absolute_error: 1.2078\n",
      "Epoch 48/50\n",
      "1379/1379 [==============================] - 1s 495us/step - loss: 2.1881 - mean_absolute_error: 1.1396\n",
      "Epoch 49/50\n",
      "1379/1379 [==============================] - 1s 488us/step - loss: 2.2169 - mean_absolute_error: 1.1590\n",
      "Epoch 50/50\n",
      "1379/1379 [==============================] - 1s 542us/step - loss: 2.1975 - mean_absolute_error: 1.1374\n",
      "QWK:  0.5978242095696185\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1381/1381 [==============================] - 6s 4ms/step - loss: 15.9005 - mean_absolute_error: 3.1541\n",
      "Epoch 2/50\n",
      "1381/1381 [==============================] - 1s 472us/step - loss: 5.6072 - mean_absolute_error: 1.8632\n",
      "Epoch 3/50\n",
      "1381/1381 [==============================] - 1s 471us/step - loss: 5.1279 - mean_absolute_error: 1.7866\n",
      "Epoch 4/50\n",
      "1381/1381 [==============================] - 1s 471us/step - loss: 5.1168 - mean_absolute_error: 1.7983\n",
      "Epoch 5/50\n",
      "1381/1381 [==============================] - 1s 482us/step - loss: 4.9516 - mean_absolute_error: 1.7496\n",
      "Epoch 6/50\n",
      "1381/1381 [==============================] - 1s 483us/step - loss: 4.7576 - mean_absolute_error: 1.7277\n",
      "Epoch 7/50\n",
      "1381/1381 [==============================] - 1s 474us/step - loss: 4.4314 - mean_absolute_error: 1.6436\n",
      "Epoch 8/50\n",
      "1381/1381 [==============================] - 1s 475us/step - loss: 4.4252 - mean_absolute_error: 1.6448\n",
      "Epoch 9/50\n",
      "1381/1381 [==============================] - 1s 481us/step - loss: 4.4537 - mean_absolute_error: 1.6268\n",
      "Epoch 10/50\n",
      "1381/1381 [==============================] - 1s 478us/step - loss: 4.4630 - mean_absolute_error: 1.6628\n",
      "Epoch 11/50\n",
      "1381/1381 [==============================] - 1s 560us/step - loss: 4.3569 - mean_absolute_error: 1.6371\n",
      "Epoch 12/50\n",
      "1381/1381 [==============================] - 1s 569us/step - loss: 4.2637 - mean_absolute_error: 1.6075\n",
      "Epoch 13/50\n",
      "1381/1381 [==============================] - 1s 576us/step - loss: 4.3438 - mean_absolute_error: 1.6156\n",
      "Epoch 14/50\n",
      "1381/1381 [==============================] - 1s 575us/step - loss: 4.3512 - mean_absolute_error: 1.6167\n",
      "Epoch 15/50\n",
      "1381/1381 [==============================] - 1s 576us/step - loss: 4.3145 - mean_absolute_error: 1.6126\n",
      "Epoch 16/50\n",
      "1381/1381 [==============================] - 1s 511us/step - loss: 4.2354 - mean_absolute_error: 1.5872\n",
      "Epoch 17/50\n",
      "1381/1381 [==============================] - 1s 501us/step - loss: 4.0297 - mean_absolute_error: 1.5573\n",
      "Epoch 18/50\n",
      "1381/1381 [==============================] - 1s 498us/step - loss: 3.9109 - mean_absolute_error: 1.5316\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381/1381 [==============================] - 1s 487us/step - loss: 4.0140 - mean_absolute_error: 1.5357\n",
      "Epoch 20/50\n",
      "1381/1381 [==============================] - 1s 485us/step - loss: 3.9462 - mean_absolute_error: 1.5257\n",
      "Epoch 21/50\n",
      "1381/1381 [==============================] - 1s 488us/step - loss: 4.0984 - mean_absolute_error: 1.5666\n",
      "Epoch 22/50\n",
      "1381/1381 [==============================] - 1s 560us/step - loss: 3.8235 - mean_absolute_error: 1.5082\n",
      "Epoch 23/50\n",
      "1381/1381 [==============================] - 1s 558us/step - loss: 3.9007 - mean_absolute_error: 1.5185\n",
      "Epoch 24/50\n",
      "1381/1381 [==============================] - 1s 559us/step - loss: 3.8288 - mean_absolute_error: 1.5117\n",
      "Epoch 25/50\n",
      "1381/1381 [==============================] - 1s 562us/step - loss: 3.4871 - mean_absolute_error: 1.4475\n",
      "Epoch 26/50\n",
      "1381/1381 [==============================] - 1s 562us/step - loss: 3.5391 - mean_absolute_error: 1.4406\n",
      "Epoch 27/50\n",
      "1381/1381 [==============================] - 1s 502us/step - loss: 3.4843 - mean_absolute_error: 1.4567\n",
      "Epoch 28/50\n",
      "1381/1381 [==============================] - 1s 481us/step - loss: 3.4698 - mean_absolute_error: 1.4312\n",
      "Epoch 29/50\n",
      "1381/1381 [==============================] - 1s 483us/step - loss: 3.5111 - mean_absolute_error: 1.4425\n",
      "Epoch 30/50\n",
      "1381/1381 [==============================] - 1s 482us/step - loss: 3.2358 - mean_absolute_error: 1.3707\n",
      "Epoch 31/50\n",
      "1381/1381 [==============================] - 1s 482us/step - loss: 3.1008 - mean_absolute_error: 1.3839\n",
      "Epoch 32/50\n",
      "1381/1381 [==============================] - 1s 482us/step - loss: 3.2967 - mean_absolute_error: 1.4210\n",
      "Epoch 33/50\n",
      "1381/1381 [==============================] - 1s 546us/step - loss: 3.0174 - mean_absolute_error: 1.3353\n",
      "Epoch 34/50\n",
      "1381/1381 [==============================] - 1s 564us/step - loss: 2.9804 - mean_absolute_error: 1.3302\n",
      "Epoch 35/50\n",
      "1381/1381 [==============================] - 1s 582us/step - loss: 2.8652 - mean_absolute_error: 1.2991\n",
      "Epoch 36/50\n",
      "1381/1381 [==============================] - 1s 562us/step - loss: 2.8438 - mean_absolute_error: 1.3074\n",
      "Epoch 37/50\n",
      "1381/1381 [==============================] - 1s 571us/step - loss: 3.0206 - mean_absolute_error: 1.3356\n",
      "Epoch 38/50\n",
      "1381/1381 [==============================] - 1s 510us/step - loss: 2.8214 - mean_absolute_error: 1.2902\n",
      "Epoch 39/50\n",
      "1381/1381 [==============================] - 1s 490us/step - loss: 2.7302 - mean_absolute_error: 1.2737\n",
      "Epoch 40/50\n",
      "1381/1381 [==============================] - 1s 481us/step - loss: 2.8132 - mean_absolute_error: 1.2775\n",
      "Epoch 41/50\n",
      "1381/1381 [==============================] - 1s 488us/step - loss: 2.6908 - mean_absolute_error: 1.2723\n",
      "Epoch 42/50\n",
      "1381/1381 [==============================] - 1s 489us/step - loss: 2.6681 - mean_absolute_error: 1.2377\n",
      "Epoch 43/50\n",
      "1381/1381 [==============================] - 1s 483us/step - loss: 2.4049 - mean_absolute_error: 1.2014\n",
      "Epoch 44/50\n",
      "1381/1381 [==============================] - 1s 550us/step - loss: 2.6024 - mean_absolute_error: 1.2318\n",
      "Epoch 45/50\n",
      "1381/1381 [==============================] - 1s 561us/step - loss: 2.4449 - mean_absolute_error: 1.1943\n",
      "Epoch 46/50\n",
      "1381/1381 [==============================] - 1s 563us/step - loss: 2.3202 - mean_absolute_error: 1.1678\n",
      "Epoch 47/50\n",
      "1381/1381 [==============================] - 1s 559us/step - loss: 2.3059 - mean_absolute_error: 1.1938\n",
      "Epoch 48/50\n",
      "1381/1381 [==============================] - 1s 574us/step - loss: 2.3913 - mean_absolute_error: 1.1849\n",
      "Epoch 49/50\n",
      "1381/1381 [==============================] - 1s 521us/step - loss: 2.1918 - mean_absolute_error: 1.1383\n",
      "Epoch 50/50\n",
      "1381/1381 [==============================] - 1s 488us/step - loss: 2.3507 - mean_absolute_error: 1.1775\n",
      "QWK:  0.5615297035383\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1382/1382 [==============================] - 6s 4ms/step - loss: 15.2249 - mean_absolute_error: 3.0593\n",
      "Epoch 2/50\n",
      "1382/1382 [==============================] - 1s 472us/step - loss: 5.7551 - mean_absolute_error: 1.9018\n",
      "Epoch 3/50\n",
      "1382/1382 [==============================] - 1s 492us/step - loss: 5.4076 - mean_absolute_error: 1.8464\n",
      "Epoch 4/50\n",
      "1382/1382 [==============================] - 1s 488us/step - loss: 5.0784 - mean_absolute_error: 1.7604\n",
      "Epoch 5/50\n",
      "1382/1382 [==============================] - 1s 488us/step - loss: 4.7831 - mean_absolute_error: 1.6979\n",
      "Epoch 6/50\n",
      "1382/1382 [==============================] - 1s 488us/step - loss: 4.7681 - mean_absolute_error: 1.7089\n",
      "Epoch 7/50\n",
      "1382/1382 [==============================] - 1s 483us/step - loss: 4.6815 - mean_absolute_error: 1.6842\n",
      "Epoch 8/50\n",
      "1382/1382 [==============================] - 1s 539us/step - loss: 4.7718 - mean_absolute_error: 1.7096\n",
      "Epoch 9/50\n",
      "1382/1382 [==============================] - 1s 639us/step - loss: 4.5423 - mean_absolute_error: 1.6322\n",
      "Epoch 10/50\n",
      "1382/1382 [==============================] - 1s 637us/step - loss: 4.3766 - mean_absolute_error: 1.6310\n",
      "Epoch 11/50\n",
      "1382/1382 [==============================] - 1s 634us/step - loss: 4.3730 - mean_absolute_error: 1.6184\n",
      "Epoch 12/50\n",
      "1382/1382 [==============================] - 1s 646us/step - loss: 4.3612 - mean_absolute_error: 1.6254\n",
      "Epoch 13/50\n",
      "1382/1382 [==============================] - 1s 583us/step - loss: 4.4491 - mean_absolute_error: 1.6203\n",
      "Epoch 14/50\n",
      "1382/1382 [==============================] - 1s 570us/step - loss: 4.2255 - mean_absolute_error: 1.5798\n",
      "Epoch 15/50\n",
      "1382/1382 [==============================] - 1s 575us/step - loss: 4.2676 - mean_absolute_error: 1.6093\n",
      "Epoch 16/50\n",
      "1382/1382 [==============================] - 1s 568us/step - loss: 4.2474 - mean_absolute_error: 1.6110\n",
      "Epoch 17/50\n",
      "1382/1382 [==============================] - 1s 569us/step - loss: 4.2000 - mean_absolute_error: 1.5848\n",
      "Epoch 18/50\n",
      "1382/1382 [==============================] - 1s 518us/step - loss: 4.0705 - mean_absolute_error: 1.5593\n",
      "Epoch 19/50\n",
      "1382/1382 [==============================] - 1s 488us/step - loss: 3.8856 - mean_absolute_error: 1.5202\n",
      "Epoch 20/50\n",
      "1382/1382 [==============================] - 1s 491us/step - loss: 4.0322 - mean_absolute_error: 1.5471\n",
      "Epoch 21/50\n",
      "1382/1382 [==============================] - 1s 494us/step - loss: 3.9863 - mean_absolute_error: 1.5461\n",
      "Epoch 22/50\n",
      "1382/1382 [==============================] - 1s 490us/step - loss: 3.7638 - mean_absolute_error: 1.4806\n",
      "Epoch 23/50\n",
      "1382/1382 [==============================] - 1s 494us/step - loss: 3.7198 - mean_absolute_error: 1.4893\n",
      "Epoch 24/50\n",
      "1382/1382 [==============================] - 1s 558us/step - loss: 3.6665 - mean_absolute_error: 1.4729\n",
      "Epoch 25/50\n",
      "1382/1382 [==============================] - 1s 575us/step - loss: 3.7171 - mean_absolute_error: 1.5000\n",
      "Epoch 26/50\n",
      "1382/1382 [==============================] - 1s 575us/step - loss: 3.3988 - mean_absolute_error: 1.4171\n",
      "Epoch 27/50\n",
      "1382/1382 [==============================] - 1s 581us/step - loss: 3.3901 - mean_absolute_error: 1.4225\n",
      "Epoch 28/50\n",
      "1382/1382 [==============================] - 1s 597us/step - loss: 3.4230 - mean_absolute_error: 1.4185\n",
      "Epoch 29/50\n",
      "1382/1382 [==============================] - 1s 509us/step - loss: 3.4535 - mean_absolute_error: 1.4132\n",
      "Epoch 30/50\n",
      "1382/1382 [==============================] - 1s 489us/step - loss: 3.3530 - mean_absolute_error: 1.4164\n",
      "Epoch 31/50\n",
      "1382/1382 [==============================] - 1s 492us/step - loss: 3.2700 - mean_absolute_error: 1.3937\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382/1382 [==============================] - 1s 484us/step - loss: 3.0513 - mean_absolute_error: 1.3364\n",
      "Epoch 33/50\n",
      "1382/1382 [==============================] - 1s 488us/step - loss: 3.1670 - mean_absolute_error: 1.3740\n",
      "Epoch 34/50\n",
      "1382/1382 [==============================] - 1s 485us/step - loss: 3.1710 - mean_absolute_error: 1.3616\n",
      "Epoch 35/50\n",
      "1382/1382 [==============================] - 1s 562us/step - loss: 3.1600 - mean_absolute_error: 1.3600\n",
      "Epoch 36/50\n",
      "1382/1382 [==============================] - 1s 594us/step - loss: 3.0316 - mean_absolute_error: 1.3375\n",
      "Epoch 37/50\n",
      "1382/1382 [==============================] - 1s 556us/step - loss: 2.9342 - mean_absolute_error: 1.3226\n",
      "Epoch 38/50\n",
      "1382/1382 [==============================] - 1s 565us/step - loss: 2.8694 - mean_absolute_error: 1.2897\n",
      "Epoch 39/50\n",
      "1382/1382 [==============================] - 1s 561us/step - loss: 2.8589 - mean_absolute_error: 1.3007\n",
      "Epoch 40/50\n",
      "1382/1382 [==============================] - 1s 490us/step - loss: 2.6168 - mean_absolute_error: 1.2337\n",
      "Epoch 41/50\n",
      "1382/1382 [==============================] - 1s 479us/step - loss: 2.6662 - mean_absolute_error: 1.2547\n",
      "Epoch 42/50\n",
      "1382/1382 [==============================] - 1s 483us/step - loss: 2.5648 - mean_absolute_error: 1.2349\n",
      "Epoch 43/50\n",
      "1382/1382 [==============================] - 1s 485us/step - loss: 2.4592 - mean_absolute_error: 1.2082\n",
      "Epoch 44/50\n",
      "1382/1382 [==============================] - 1s 483us/step - loss: 2.6315 - mean_absolute_error: 1.2506\n",
      "Epoch 45/50\n",
      "1382/1382 [==============================] - 1s 482us/step - loss: 2.5312 - mean_absolute_error: 1.2178\n",
      "Epoch 46/50\n",
      "1382/1382 [==============================] - 1s 482us/step - loss: 2.5912 - mean_absolute_error: 1.2295\n",
      "Epoch 47/50\n",
      "1382/1382 [==============================] - 1s 489us/step - loss: 2.4206 - mean_absolute_error: 1.1882\n",
      "Epoch 48/50\n",
      "1382/1382 [==============================] - 1s 483us/step - loss: 2.3133 - mean_absolute_error: 1.1590\n",
      "Epoch 49/50\n",
      "1382/1382 [==============================] - 1s 480us/step - loss: 2.4161 - mean_absolute_error: 1.1921\n",
      "Epoch 50/50\n",
      "1382/1382 [==============================] - 1s 494us/step - loss: 2.3263 - mean_absolute_error: 1.1759\n",
      "QWK:  0.5698788807287767\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1383/1383 [==============================] - 6s 5ms/step - loss: 15.9271 - mean_absolute_error: 3.1378\n",
      "Epoch 2/50\n",
      "1383/1383 [==============================] - 1s 471us/step - loss: 5.9755 - mean_absolute_error: 1.9590\n",
      "Epoch 3/50\n",
      "1383/1383 [==============================] - 1s 469us/step - loss: 5.4263 - mean_absolute_error: 1.8642\n",
      "Epoch 4/50\n",
      "1383/1383 [==============================] - 1s 470us/step - loss: 4.8398 - mean_absolute_error: 1.7334\n",
      "Epoch 5/50\n",
      "1383/1383 [==============================] - 1s 471us/step - loss: 4.9270 - mean_absolute_error: 1.7409\n",
      "Epoch 6/50\n",
      "1383/1383 [==============================] - 1s 470us/step - loss: 4.6099 - mean_absolute_error: 1.7042\n",
      "Epoch 7/50\n",
      "1383/1383 [==============================] - 1s 471us/step - loss: 4.5341 - mean_absolute_error: 1.6757\n",
      "Epoch 8/50\n",
      "1383/1383 [==============================] - 1s 472us/step - loss: 4.4555 - mean_absolute_error: 1.6341\n",
      "Epoch 9/50\n",
      "1383/1383 [==============================] - 1s 520us/step - loss: 4.5185 - mean_absolute_error: 1.6638\n",
      "Epoch 10/50\n",
      "1383/1383 [==============================] - 1s 567us/step - loss: 4.4678 - mean_absolute_error: 1.6522\n",
      "Epoch 11/50\n",
      "1383/1383 [==============================] - 1s 568us/step - loss: 4.3141 - mean_absolute_error: 1.6479\n",
      "Epoch 12/50\n",
      "1383/1383 [==============================] - 1s 580us/step - loss: 4.2963 - mean_absolute_error: 1.6187\n",
      "Epoch 13/50\n",
      "1383/1383 [==============================] - 1s 583us/step - loss: 4.3266 - mean_absolute_error: 1.6373\n",
      "Epoch 14/50\n",
      "1383/1383 [==============================] - 1s 540us/step - loss: 4.4814 - mean_absolute_error: 1.6526\n",
      "Epoch 15/50\n",
      "1383/1383 [==============================] - 1s 493us/step - loss: 4.0771 - mean_absolute_error: 1.5918\n",
      "Epoch 16/50\n",
      "1383/1383 [==============================] - 1s 500us/step - loss: 4.2158 - mean_absolute_error: 1.5951\n",
      "Epoch 17/50\n",
      "1383/1383 [==============================] - 1s 492us/step - loss: 4.3908 - mean_absolute_error: 1.6481\n",
      "Epoch 18/50\n",
      "1383/1383 [==============================] - 1s 489us/step - loss: 4.1757 - mean_absolute_error: 1.5830\n",
      "Epoch 19/50\n",
      "1383/1383 [==============================] - 1s 498us/step - loss: 4.0789 - mean_absolute_error: 1.5626\n",
      "Epoch 20/50\n",
      "1383/1383 [==============================] - 1s 539us/step - loss: 3.9719 - mean_absolute_error: 1.5540\n",
      "Epoch 21/50\n",
      "1383/1383 [==============================] - 1s 574us/step - loss: 3.8140 - mean_absolute_error: 1.5131\n",
      "Epoch 22/50\n",
      "1383/1383 [==============================] - 1s 573us/step - loss: 3.9731 - mean_absolute_error: 1.5283\n",
      "Epoch 23/50\n",
      "1383/1383 [==============================] - 1s 575us/step - loss: 3.9003 - mean_absolute_error: 1.5297\n",
      "Epoch 24/50\n",
      "1383/1383 [==============================] - 1s 572us/step - loss: 3.6340 - mean_absolute_error: 1.4808\n",
      "Epoch 25/50\n",
      "1383/1383 [==============================] - 1s 532us/step - loss: 3.5576 - mean_absolute_error: 1.4836\n",
      "Epoch 26/50\n",
      "1383/1383 [==============================] - 1s 491us/step - loss: 3.7739 - mean_absolute_error: 1.5101\n",
      "Epoch 27/50\n",
      "1383/1383 [==============================] - 1s 496us/step - loss: 3.5689 - mean_absolute_error: 1.4413\n",
      "Epoch 28/50\n",
      "1383/1383 [==============================] - 1s 490us/step - loss: 3.5272 - mean_absolute_error: 1.4535\n",
      "Epoch 29/50\n",
      "1383/1383 [==============================] - 1s 512us/step - loss: 3.5217 - mean_absolute_error: 1.4358\n",
      "Epoch 30/50\n",
      "1383/1383 [==============================] - 1s 488us/step - loss: 3.3927 - mean_absolute_error: 1.4049\n",
      "Epoch 31/50\n",
      "1383/1383 [==============================] - 1s 559us/step - loss: 3.3913 - mean_absolute_error: 1.4312\n",
      "Epoch 32/50\n",
      "1383/1383 [==============================] - 1s 586us/step - loss: 3.2725 - mean_absolute_error: 1.4001\n",
      "Epoch 33/50\n",
      "1383/1383 [==============================] - 1s 585us/step - loss: 3.2099 - mean_absolute_error: 1.3882\n",
      "Epoch 34/50\n",
      "1383/1383 [==============================] - 1s 572us/step - loss: 3.0288 - mean_absolute_error: 1.3520\n",
      "Epoch 35/50\n",
      "1383/1383 [==============================] - 1s 586us/step - loss: 3.0075 - mean_absolute_error: 1.3300\n",
      "Epoch 36/50\n",
      "1383/1383 [==============================] - 1s 520us/step - loss: 3.1590 - mean_absolute_error: 1.3578\n",
      "Epoch 37/50\n",
      "1383/1383 [==============================] - 1s 492us/step - loss: 2.8162 - mean_absolute_error: 1.3033\n",
      "Epoch 38/50\n",
      "1383/1383 [==============================] - 1s 501us/step - loss: 3.0645 - mean_absolute_error: 1.3508\n",
      "Epoch 39/50\n",
      "1383/1383 [==============================] - 1s 493us/step - loss: 2.6570 - mean_absolute_error: 1.2550\n",
      "Epoch 40/50\n",
      "1383/1383 [==============================] - 1s 493us/step - loss: 2.7174 - mean_absolute_error: 1.2763\n",
      "Epoch 41/50\n",
      "1383/1383 [==============================] - 1s 494us/step - loss: 2.6910 - mean_absolute_error: 1.2795\n",
      "Epoch 42/50\n",
      "1383/1383 [==============================] - 1s 570us/step - loss: 2.6812 - mean_absolute_error: 1.2552\n",
      "Epoch 43/50\n",
      "1383/1383 [==============================] - 1s 586us/step - loss: 2.6032 - mean_absolute_error: 1.2640\n",
      "Epoch 44/50\n",
      "1383/1383 [==============================] - 1s 570us/step - loss: 2.3751 - mean_absolute_error: 1.1944\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383/1383 [==============================] - 1s 568us/step - loss: 2.4865 - mean_absolute_error: 1.2236\n",
      "Epoch 46/50\n",
      "1383/1383 [==============================] - 1s 556us/step - loss: 2.1975 - mean_absolute_error: 1.1299\n",
      "Epoch 47/50\n",
      "1383/1383 [==============================] - 1s 496us/step - loss: 2.3126 - mean_absolute_error: 1.1644\n",
      "Epoch 48/50\n",
      "1383/1383 [==============================] - 1s 486us/step - loss: 2.3712 - mean_absolute_error: 1.1894\n",
      "Epoch 49/50\n",
      "1383/1383 [==============================] - 1s 484us/step - loss: 2.3828 - mean_absolute_error: 1.1918\n",
      "Epoch 50/50\n",
      "1383/1383 [==============================] - 1s 478us/step - loss: 2.1658 - mean_absolute_error: 1.1279\n",
      "QWK:  0.6033675473677493\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.5929\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1416/1416 [==============================] - 9s 6ms/step - loss: 10.1395 - mean_absolute_error: 2.3954\n",
      "Epoch 2/50\n",
      "1416/1416 [==============================] - 1s 486us/step - loss: 4.4446 - mean_absolute_error: 1.6727\n",
      "Epoch 3/50\n",
      "1416/1416 [==============================] - 1s 510us/step - loss: 4.0078 - mean_absolute_error: 1.5998\n",
      "Epoch 4/50\n",
      "1416/1416 [==============================] - 1s 572us/step - loss: 4.0930 - mean_absolute_error: 1.5860\n",
      "Epoch 5/50\n",
      "1416/1416 [==============================] - 1s 493us/step - loss: 3.9027 - mean_absolute_error: 1.5520\n",
      "Epoch 6/50\n",
      "1416/1416 [==============================] - 1s 468us/step - loss: 3.8393 - mean_absolute_error: 1.5469\n",
      "Epoch 7/50\n",
      "1416/1416 [==============================] - 1s 469us/step - loss: 3.7172 - mean_absolute_error: 1.5046\n",
      "Epoch 8/50\n",
      "1416/1416 [==============================] - 1s 481us/step - loss: 3.6147 - mean_absolute_error: 1.4903\n",
      "Epoch 9/50\n",
      "1416/1416 [==============================] - 1s 472us/step - loss: 3.5344 - mean_absolute_error: 1.4906\n",
      "Epoch 10/50\n",
      "1416/1416 [==============================] - 1s 478us/step - loss: 3.6375 - mean_absolute_error: 1.4959\n",
      "Epoch 11/50\n",
      "1416/1416 [==============================] - 1s 492us/step - loss: 3.4011 - mean_absolute_error: 1.4332\n",
      "Epoch 12/50\n",
      "1416/1416 [==============================] - 1s 700us/step - loss: 3.6064 - mean_absolute_error: 1.4838\n",
      "Epoch 13/50\n",
      "1416/1416 [==============================] - 1s 743us/step - loss: 3.3415 - mean_absolute_error: 1.4210\n",
      "Epoch 14/50\n",
      "1416/1416 [==============================] - 1s 611us/step - loss: 3.3974 - mean_absolute_error: 1.4290\n",
      "Epoch 15/50\n",
      "1416/1416 [==============================] - 1s 591us/step - loss: 3.3643 - mean_absolute_error: 1.4302\n",
      "Epoch 16/50\n",
      "1416/1416 [==============================] - 1s 652us/step - loss: 3.2282 - mean_absolute_error: 1.3845\n",
      "Epoch 17/50\n",
      "1416/1416 [==============================] - 1s 577us/step - loss: 3.2157 - mean_absolute_error: 1.3766\n",
      "Epoch 18/50\n",
      "1416/1416 [==============================] - 1s 559us/step - loss: 2.9688 - mean_absolute_error: 1.3460\n",
      "Epoch 19/50\n",
      "1416/1416 [==============================] - 1s 547us/step - loss: 3.0980 - mean_absolute_error: 1.3759\n",
      "Epoch 20/50\n",
      "1416/1416 [==============================] - 1s 557us/step - loss: 3.0978 - mean_absolute_error: 1.3783\n",
      "Epoch 21/50\n",
      "1416/1416 [==============================] - 1s 562us/step - loss: 3.0190 - mean_absolute_error: 1.3465\n",
      "Epoch 22/50\n",
      "1416/1416 [==============================] - 1s 600us/step - loss: 3.1648 - mean_absolute_error: 1.3875\n",
      "Epoch 23/50\n",
      "1416/1416 [==============================] - 1s 632us/step - loss: 3.0578 - mean_absolute_error: 1.3583\n",
      "Epoch 24/50\n",
      "1416/1416 [==============================] - 1s 663us/step - loss: 2.9457 - mean_absolute_error: 1.3154\n",
      "Epoch 25/50\n",
      "1416/1416 [==============================] - 1s 666us/step - loss: 2.9335 - mean_absolute_error: 1.3304\n",
      "Epoch 26/50\n",
      "1416/1416 [==============================] - 1s 733us/step - loss: 2.6896 - mean_absolute_error: 1.2815\n",
      "Epoch 27/50\n",
      "1416/1416 [==============================] - 1s 836us/step - loss: 2.8067 - mean_absolute_error: 1.3142\n",
      "Epoch 28/50\n",
      "1416/1416 [==============================] - 1s 804us/step - loss: 2.8244 - mean_absolute_error: 1.2975\n",
      "Epoch 29/50\n",
      "1416/1416 [==============================] - 1s 839us/step - loss: 2.7472 - mean_absolute_error: 1.2882\n",
      "Epoch 30/50\n",
      "1416/1416 [==============================] - 1s 707us/step - loss: 2.5634 - mean_absolute_error: 1.2532\n",
      "Epoch 31/50\n",
      "1416/1416 [==============================] - 1s 712us/step - loss: 2.6720 - mean_absolute_error: 1.2580\n",
      "Epoch 32/50\n",
      "1416/1416 [==============================] - 1s 659us/step - loss: 2.5138 - mean_absolute_error: 1.2323\n",
      "Epoch 33/50\n",
      "1416/1416 [==============================] - 1s 680us/step - loss: 2.5768 - mean_absolute_error: 1.2494\n",
      "Epoch 34/50\n",
      "1416/1416 [==============================] - 1s 623us/step - loss: 2.3926 - mean_absolute_error: 1.2099\n",
      "Epoch 35/50\n",
      "1416/1416 [==============================] - 1s 599us/step - loss: 2.4633 - mean_absolute_error: 1.2213\n",
      "Epoch 36/50\n",
      "1416/1416 [==============================] - 1s 591us/step - loss: 2.3157 - mean_absolute_error: 1.1954\n",
      "Epoch 37/50\n",
      "1416/1416 [==============================] - 1s 616us/step - loss: 2.3463 - mean_absolute_error: 1.1822\n",
      "Epoch 38/50\n",
      "1416/1416 [==============================] - 1s 613us/step - loss: 2.2262 - mean_absolute_error: 1.1462\n",
      "Epoch 39/50\n",
      "1416/1416 [==============================] - 1s 594us/step - loss: 2.2478 - mean_absolute_error: 1.1524\n",
      "Epoch 40/50\n",
      "1416/1416 [==============================] - 1s 544us/step - loss: 2.2147 - mean_absolute_error: 1.1281\n",
      "Epoch 41/50\n",
      "1416/1416 [==============================] - 1s 540us/step - loss: 2.1086 - mean_absolute_error: 1.1160\n",
      "Epoch 42/50\n",
      "1416/1416 [==============================] - 1s 508us/step - loss: 2.1444 - mean_absolute_error: 1.1301\n",
      "Epoch 43/50\n",
      "1416/1416 [==============================] - 1s 541us/step - loss: 2.1293 - mean_absolute_error: 1.1361\n",
      "Epoch 44/50\n",
      "1416/1416 [==============================] - 1s 610us/step - loss: 1.9914 - mean_absolute_error: 1.0780\n",
      "Epoch 45/50\n",
      "1416/1416 [==============================] - 1s 590us/step - loss: 2.0185 - mean_absolute_error: 1.0997\n",
      "Epoch 46/50\n",
      "1416/1416 [==============================] - 1s 625us/step - loss: 1.9233 - mean_absolute_error: 1.0665\n",
      "Epoch 47/50\n",
      "1416/1416 [==============================] - 1s 633us/step - loss: 1.9163 - mean_absolute_error: 1.0547\n",
      "Epoch 48/50\n",
      "1416/1416 [==============================] - 1s 589us/step - loss: 2.1008 - mean_absolute_error: 1.1280\n",
      "Epoch 49/50\n",
      "1416/1416 [==============================] - 1s 518us/step - loss: 1.7754 - mean_absolute_error: 1.0234\n",
      "Epoch 50/50\n",
      "1416/1416 [==============================] - 1s 557us/step - loss: 1.8045 - mean_absolute_error: 1.0367\n",
      "QWK:  0.8052686854542273\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1417/1417 [==============================] - 7s 5ms/step - loss: 9.6555 - mean_absolute_error: 2.3437\n",
      "Epoch 2/50\n",
      "1417/1417 [==============================] - 1s 537us/step - loss: 4.0543 - mean_absolute_error: 1.5767\n",
      "Epoch 3/50\n",
      "1417/1417 [==============================] - 1s 533us/step - loss: 3.9056 - mean_absolute_error: 1.5656\n",
      "Epoch 4/50\n",
      "1417/1417 [==============================] - 1s 544us/step - loss: 3.5735 - mean_absolute_error: 1.4852\n",
      "Epoch 5/50\n",
      "1417/1417 [==============================] - 1s 526us/step - loss: 3.6608 - mean_absolute_error: 1.5208\n",
      "Epoch 6/50\n",
      "1417/1417 [==============================] - 1s 515us/step - loss: 3.4223 - mean_absolute_error: 1.4677\n",
      "Epoch 7/50\n",
      "1417/1417 [==============================] - 1s 649us/step - loss: 3.4391 - mean_absolute_error: 1.4616\n",
      "Epoch 8/50\n",
      "1417/1417 [==============================] - 1s 669us/step - loss: 3.2086 - mean_absolute_error: 1.4349\n",
      "Epoch 9/50\n",
      "1417/1417 [==============================] - 1s 640us/step - loss: 3.3763 - mean_absolute_error: 1.4552\n",
      "Epoch 10/50\n",
      "1417/1417 [==============================] - 1s 643us/step - loss: 3.3411 - mean_absolute_error: 1.4462\n",
      "Epoch 11/50\n",
      "1417/1417 [==============================] - 1s 591us/step - loss: 3.1905 - mean_absolute_error: 1.4040\n",
      "Epoch 12/50\n",
      "1417/1417 [==============================] - 1s 574us/step - loss: 3.0889 - mean_absolute_error: 1.3732\n",
      "Epoch 13/50\n",
      "1417/1417 [==============================] - 1s 559us/step - loss: 3.2088 - mean_absolute_error: 1.4033\n",
      "Epoch 14/50\n",
      "1417/1417 [==============================] - 1s 570us/step - loss: 3.1143 - mean_absolute_error: 1.3813\n",
      "Epoch 15/50\n",
      "1417/1417 [==============================] - 1s 573us/step - loss: 3.1523 - mean_absolute_error: 1.3907\n",
      "Epoch 16/50\n",
      "1417/1417 [==============================] - 1s 519us/step - loss: 2.9140 - mean_absolute_error: 1.3313\n",
      "Epoch 17/50\n",
      "1417/1417 [==============================] - 1s 488us/step - loss: 3.0915 - mean_absolute_error: 1.3816\n",
      "Epoch 18/50\n",
      "1417/1417 [==============================] - 1s 489us/step - loss: 2.9959 - mean_absolute_error: 1.3506\n",
      "Epoch 19/50\n",
      "1417/1417 [==============================] - 1s 490us/step - loss: 3.0659 - mean_absolute_error: 1.3705\n",
      "Epoch 20/50\n",
      "1417/1417 [==============================] - 1s 490us/step - loss: 2.8233 - mean_absolute_error: 1.3177\n",
      "Epoch 21/50\n",
      "1417/1417 [==============================] - 1s 496us/step - loss: 2.8256 - mean_absolute_error: 1.2977\n",
      "Epoch 22/50\n",
      "1417/1417 [==============================] - 1s 559us/step - loss: 2.9558 - mean_absolute_error: 1.3558\n",
      "Epoch 23/50\n",
      "1417/1417 [==============================] - 1s 561us/step - loss: 2.6665 - mean_absolute_error: 1.2617\n",
      "Epoch 24/50\n",
      "1417/1417 [==============================] - 1s 569us/step - loss: 2.6613 - mean_absolute_error: 1.2639\n",
      "Epoch 25/50\n",
      "1417/1417 [==============================] - 1s 565us/step - loss: 2.5818 - mean_absolute_error: 1.2519\n",
      "Epoch 26/50\n",
      "1417/1417 [==============================] - 1s 570us/step - loss: 2.6397 - mean_absolute_error: 1.2742\n",
      "Epoch 27/50\n",
      "1417/1417 [==============================] - 1s 508us/step - loss: 2.6380 - mean_absolute_error: 1.2658\n",
      "Epoch 28/50\n",
      "1417/1417 [==============================] - 1s 495us/step - loss: 2.5946 - mean_absolute_error: 1.2515\n",
      "Epoch 29/50\n",
      "1417/1417 [==============================] - 1s 494us/step - loss: 2.5414 - mean_absolute_error: 1.2273\n",
      "Epoch 30/50\n",
      "1417/1417 [==============================] - 1s 495us/step - loss: 2.4725 - mean_absolute_error: 1.2252\n",
      "Epoch 31/50\n",
      "1417/1417 [==============================] - 1s 493us/step - loss: 2.5216 - mean_absolute_error: 1.2301\n",
      "Epoch 32/50\n",
      "1417/1417 [==============================] - 1s 517us/step - loss: 2.4418 - mean_absolute_error: 1.2008\n",
      "Epoch 33/50\n",
      "1417/1417 [==============================] - 1s 578us/step - loss: 2.5228 - mean_absolute_error: 1.2119\n",
      "Epoch 34/50\n",
      "1417/1417 [==============================] - 1s 571us/step - loss: 2.3550 - mean_absolute_error: 1.1970\n",
      "Epoch 35/50\n",
      "1417/1417 [==============================] - 1s 576us/step - loss: 2.2578 - mean_absolute_error: 1.1717\n",
      "Epoch 36/50\n",
      "1417/1417 [==============================] - 1s 579us/step - loss: 2.2539 - mean_absolute_error: 1.1565\n",
      "Epoch 37/50\n",
      "1417/1417 [==============================] - 1s 553us/step - loss: 2.2244 - mean_absolute_error: 1.1518\n",
      "Epoch 38/50\n",
      "1417/1417 [==============================] - 1s 492us/step - loss: 2.2815 - mean_absolute_error: 1.1646\n",
      "Epoch 39/50\n",
      "1417/1417 [==============================] - 1s 499us/step - loss: 2.0372 - mean_absolute_error: 1.1024\n",
      "Epoch 40/50\n",
      "1417/1417 [==============================] - 1s 498us/step - loss: 2.0933 - mean_absolute_error: 1.1190\n",
      "Epoch 41/50\n",
      "1417/1417 [==============================] - 1s 487us/step - loss: 2.0712 - mean_absolute_error: 1.1095\n",
      "Epoch 42/50\n",
      "1417/1417 [==============================] - 1s 493us/step - loss: 2.0688 - mean_absolute_error: 1.1023\n",
      "Epoch 43/50\n",
      "1417/1417 [==============================] - 1s 549us/step - loss: 2.0068 - mean_absolute_error: 1.1001\n",
      "Epoch 44/50\n",
      "1417/1417 [==============================] - 1s 571us/step - loss: 1.9158 - mean_absolute_error: 1.0592\n",
      "Epoch 45/50\n",
      "1417/1417 [==============================] - 1s 582us/step - loss: 1.8803 - mean_absolute_error: 1.0596\n",
      "Epoch 46/50\n",
      "1417/1417 [==============================] - 1s 572us/step - loss: 2.0059 - mean_absolute_error: 1.0712\n",
      "Epoch 47/50\n",
      "1417/1417 [==============================] - 1s 571us/step - loss: 1.8415 - mean_absolute_error: 1.0524\n",
      "Epoch 48/50\n",
      "1417/1417 [==============================] - 1s 515us/step - loss: 1.7350 - mean_absolute_error: 1.0155\n",
      "Epoch 49/50\n",
      "1417/1417 [==============================] - 1s 490us/step - loss: 1.8419 - mean_absolute_error: 1.0483\n",
      "Epoch 50/50\n",
      "1417/1417 [==============================] - 1s 497us/step - loss: 1.8044 - mean_absolute_error: 1.0298\n",
      "QWK:  0.7572481736509814\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_35 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1417/1417 [==============================] - 8s 5ms/step - loss: 10.0322 - mean_absolute_error: 2.4054\n",
      "Epoch 2/50\n",
      "1417/1417 [==============================] - 1s 512us/step - loss: 4.3590 - mean_absolute_error: 1.6619\n",
      "Epoch 3/50\n",
      "1417/1417 [==============================] - 1s 505us/step - loss: 3.8751 - mean_absolute_error: 1.5646\n",
      "Epoch 4/50\n",
      "1417/1417 [==============================] - 1s 508us/step - loss: 3.7275 - mean_absolute_error: 1.5331\n",
      "Epoch 5/50\n",
      "1417/1417 [==============================] - 1s 538us/step - loss: 3.6100 - mean_absolute_error: 1.4853\n",
      "Epoch 6/50\n",
      "1417/1417 [==============================] - 1s 656us/step - loss: 3.6252 - mean_absolute_error: 1.5083\n",
      "Epoch 7/50\n",
      "1417/1417 [==============================] - 1s 672us/step - loss: 3.5808 - mean_absolute_error: 1.4911\n",
      "Epoch 8/50\n",
      "1417/1417 [==============================] - 1s 668us/step - loss: 3.3984 - mean_absolute_error: 1.4549\n",
      "Epoch 9/50\n",
      "1417/1417 [==============================] - 1s 657us/step - loss: 3.2823 - mean_absolute_error: 1.4253\n",
      "Epoch 10/50\n",
      "1417/1417 [==============================] - 1s 588us/step - loss: 3.3432 - mean_absolute_error: 1.4469\n",
      "Epoch 11/50\n",
      "1417/1417 [==============================] - 1s 588us/step - loss: 3.3639 - mean_absolute_error: 1.4356\n",
      "Epoch 12/50\n",
      "1417/1417 [==============================] - 1s 590us/step - loss: 3.3126 - mean_absolute_error: 1.4184\n",
      "Epoch 13/50\n",
      "1417/1417 [==============================] - 1s 586us/step - loss: 3.0907 - mean_absolute_error: 1.3700\n",
      "Epoch 14/50\n",
      "1417/1417 [==============================] - 1s 576us/step - loss: 3.1895 - mean_absolute_error: 1.4048\n",
      "Epoch 15/50\n",
      "1417/1417 [==============================] - 1s 504us/step - loss: 3.1994 - mean_absolute_error: 1.4157\n",
      "Epoch 16/50\n",
      "1417/1417 [==============================] - 1s 497us/step - loss: 3.1506 - mean_absolute_error: 1.3917\n",
      "Epoch 17/50\n",
      "1417/1417 [==============================] - 1s 510us/step - loss: 3.1210 - mean_absolute_error: 1.3708\n",
      "Epoch 18/50\n",
      "1417/1417 [==============================] - 1s 502us/step - loss: 2.9626 - mean_absolute_error: 1.3508\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1417/1417 [==============================] - 1s 493us/step - loss: 3.0728 - mean_absolute_error: 1.3699\n",
      "Epoch 20/50\n",
      "1417/1417 [==============================] - 1s 538us/step - loss: 3.0766 - mean_absolute_error: 1.3865\n",
      "Epoch 21/50\n",
      "1417/1417 [==============================] - 1s 572us/step - loss: 2.9870 - mean_absolute_error: 1.3647\n",
      "Epoch 22/50\n",
      "1417/1417 [==============================] - 1s 567us/step - loss: 2.8497 - mean_absolute_error: 1.3238\n",
      "Epoch 23/50\n",
      "1417/1417 [==============================] - 1s 568us/step - loss: 3.0174 - mean_absolute_error: 1.3597\n",
      "Epoch 24/50\n",
      "1417/1417 [==============================] - 1s 568us/step - loss: 2.6691 - mean_absolute_error: 1.2874\n",
      "Epoch 25/50\n",
      "1417/1417 [==============================] - 1s 648us/step - loss: 2.7960 - mean_absolute_error: 1.3075\n",
      "Epoch 26/50\n",
      "1417/1417 [==============================] - 1s 721us/step - loss: 2.6989 - mean_absolute_error: 1.2667\n",
      "Epoch 27/50\n",
      "1417/1417 [==============================] - 1s 700us/step - loss: 2.6776 - mean_absolute_error: 1.2746\n",
      "Epoch 28/50\n",
      "1417/1417 [==============================] - 1s 713us/step - loss: 2.6965 - mean_absolute_error: 1.2666\n",
      "Epoch 29/50\n",
      "1417/1417 [==============================] - 1s 668us/step - loss: 2.6473 - mean_absolute_error: 1.2711\n",
      "Epoch 30/50\n",
      "1417/1417 [==============================] - 1s 639us/step - loss: 2.5720 - mean_absolute_error: 1.2369\n",
      "Epoch 31/50\n",
      "1417/1417 [==============================] - 1s 641us/step - loss: 2.5869 - mean_absolute_error: 1.2466\n",
      "Epoch 32/50\n",
      "1417/1417 [==============================] - 1s 638us/step - loss: 2.6742 - mean_absolute_error: 1.2706\n",
      "Epoch 33/50\n",
      "1417/1417 [==============================] - 1s 625us/step - loss: 2.4664 - mean_absolute_error: 1.2173\n",
      "Epoch 34/50\n",
      "1417/1417 [==============================] - 1s 570us/step - loss: 2.3464 - mean_absolute_error: 1.1964\n",
      "Epoch 35/50\n",
      "1417/1417 [==============================] - 1s 577us/step - loss: 2.4960 - mean_absolute_error: 1.2235\n",
      "Epoch 36/50\n",
      "1417/1417 [==============================] - 1s 576us/step - loss: 2.4180 - mean_absolute_error: 1.1966\n",
      "Epoch 37/50\n",
      "1417/1417 [==============================] - 1s 578us/step - loss: 2.1502 - mean_absolute_error: 1.1375\n",
      "Epoch 38/50\n",
      "1417/1417 [==============================] - 1s 550us/step - loss: 2.3574 - mean_absolute_error: 1.1927\n",
      "Epoch 39/50\n",
      "1417/1417 [==============================] - 1s 498us/step - loss: 2.2264 - mean_absolute_error: 1.1635\n",
      "Epoch 40/50\n",
      "1417/1417 [==============================] - 1s 495us/step - loss: 2.2710 - mean_absolute_error: 1.1662\n",
      "Epoch 41/50\n",
      "1417/1417 [==============================] - 1s 500us/step - loss: 2.0577 - mean_absolute_error: 1.1128\n",
      "Epoch 42/50\n",
      "1417/1417 [==============================] - 1s 496us/step - loss: 2.1748 - mean_absolute_error: 1.1476\n",
      "Epoch 43/50\n",
      "1417/1417 [==============================] - 1s 494us/step - loss: 2.1362 - mean_absolute_error: 1.1315\n",
      "Epoch 44/50\n",
      "1417/1417 [==============================] - 1s 547us/step - loss: 2.0620 - mean_absolute_error: 1.1147\n",
      "Epoch 45/50\n",
      "1417/1417 [==============================] - 1s 577us/step - loss: 2.0746 - mean_absolute_error: 1.1283\n",
      "Epoch 46/50\n",
      "1417/1417 [==============================] - 1s 571us/step - loss: 1.9578 - mean_absolute_error: 1.0814\n",
      "Epoch 47/50\n",
      "1417/1417 [==============================] - 1s 574us/step - loss: 1.9273 - mean_absolute_error: 1.0653\n",
      "Epoch 48/50\n",
      "1417/1417 [==============================] - 1s 582us/step - loss: 2.0289 - mean_absolute_error: 1.0992\n",
      "Epoch 49/50\n",
      "1417/1417 [==============================] - 1s 514us/step - loss: 1.8144 - mean_absolute_error: 1.0466\n",
      "Epoch 50/50\n",
      "1417/1417 [==============================] - 1s 501us/step - loss: 1.7184 - mean_absolute_error: 1.0181\n",
      "QWK:  0.7537865265949308\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_37 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1419/1419 [==============================] - 8s 6ms/step - loss: 10.0137 - mean_absolute_error: 2.3930\n",
      "Epoch 2/50\n",
      "1419/1419 [==============================] - 1s 505us/step - loss: 4.4556 - mean_absolute_error: 1.6593\n",
      "Epoch 3/50\n",
      "1419/1419 [==============================] - 1s 501us/step - loss: 4.1228 - mean_absolute_error: 1.5989\n",
      "Epoch 4/50\n",
      "1419/1419 [==============================] - 1s 503us/step - loss: 3.7924 - mean_absolute_error: 1.5350\n",
      "Epoch 5/50\n",
      "1419/1419 [==============================] - 1s 499us/step - loss: 3.5709 - mean_absolute_error: 1.5120\n",
      "Epoch 6/50\n",
      "1419/1419 [==============================] - 1s 500us/step - loss: 3.6392 - mean_absolute_error: 1.5066\n",
      "Epoch 7/50\n",
      "1419/1419 [==============================] - 1s 624us/step - loss: 3.6320 - mean_absolute_error: 1.4833\n",
      "Epoch 8/50\n",
      "1419/1419 [==============================] - 1s 651us/step - loss: 3.5824 - mean_absolute_error: 1.4909\n",
      "Epoch 9/50\n",
      "1419/1419 [==============================] - 1s 650us/step - loss: 3.5286 - mean_absolute_error: 1.4815\n",
      "Epoch 10/50\n",
      "1419/1419 [==============================] - 1s 669us/step - loss: 3.2759 - mean_absolute_error: 1.4164\n",
      "Epoch 11/50\n",
      "1419/1419 [==============================] - 1s 613us/step - loss: 3.1933 - mean_absolute_error: 1.4137\n",
      "Epoch 12/50\n",
      "1419/1419 [==============================] - 1s 585us/step - loss: 3.3035 - mean_absolute_error: 1.4214\n",
      "Epoch 13/50\n",
      "1419/1419 [==============================] - 1s 584us/step - loss: 3.1851 - mean_absolute_error: 1.4092\n",
      "Epoch 14/50\n",
      "1419/1419 [==============================] - 1s 589us/step - loss: 3.0859 - mean_absolute_error: 1.3725\n",
      "Epoch 15/50\n",
      "1419/1419 [==============================] - 1s 582us/step - loss: 3.1893 - mean_absolute_error: 1.4080\n",
      "Epoch 16/50\n",
      "1419/1419 [==============================] - 1s 524us/step - loss: 3.0167 - mean_absolute_error: 1.3491\n",
      "Epoch 17/50\n",
      "1419/1419 [==============================] - 1s 505us/step - loss: 3.0681 - mean_absolute_error: 1.3840\n",
      "Epoch 18/50\n",
      "1419/1419 [==============================] - 1s 498us/step - loss: 3.0052 - mean_absolute_error: 1.3567\n",
      "Epoch 19/50\n",
      "1419/1419 [==============================] - 1s 505us/step - loss: 3.0103 - mean_absolute_error: 1.3440\n",
      "Epoch 20/50\n",
      "1419/1419 [==============================] - 1s 496us/step - loss: 2.9021 - mean_absolute_error: 1.3305\n",
      "Epoch 21/50\n",
      "1419/1419 [==============================] - 1s 502us/step - loss: 2.8457 - mean_absolute_error: 1.3034\n",
      "Epoch 22/50\n",
      "1419/1419 [==============================] - 1s 621us/step - loss: 2.8500 - mean_absolute_error: 1.3058\n",
      "Epoch 23/50\n",
      "1419/1419 [==============================] - 1s 586us/step - loss: 2.9098 - mean_absolute_error: 1.3362\n",
      "Epoch 24/50\n",
      "1419/1419 [==============================] - 1s 587us/step - loss: 2.7821 - mean_absolute_error: 1.3272\n",
      "Epoch 25/50\n",
      "1419/1419 [==============================] - 1s 583us/step - loss: 2.7315 - mean_absolute_error: 1.2715\n",
      "Epoch 26/50\n",
      "1419/1419 [==============================] - 1s 566us/step - loss: 2.7857 - mean_absolute_error: 1.2888\n",
      "Epoch 27/50\n",
      "1419/1419 [==============================] - 1s 501us/step - loss: 2.6981 - mean_absolute_error: 1.2748\n",
      "Epoch 28/50\n",
      "1419/1419 [==============================] - 1s 508us/step - loss: 2.7698 - mean_absolute_error: 1.2958\n",
      "Epoch 29/50\n",
      "1419/1419 [==============================] - 1s 506us/step - loss: 2.6579 - mean_absolute_error: 1.2757\n",
      "Epoch 30/50\n",
      "1419/1419 [==============================] - 1s 510us/step - loss: 2.4535 - mean_absolute_error: 1.2083\n",
      "Epoch 31/50\n",
      "1419/1419 [==============================] - 1s 504us/step - loss: 2.6009 - mean_absolute_error: 1.2417\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419/1419 [==============================] - 1s 555us/step - loss: 2.5600 - mean_absolute_error: 1.2464\n",
      "Epoch 33/50\n",
      "1419/1419 [==============================] - 1s 580us/step - loss: 2.4953 - mean_absolute_error: 1.2233\n",
      "Epoch 34/50\n",
      "1419/1419 [==============================] - 1s 561us/step - loss: 2.3806 - mean_absolute_error: 1.2000\n",
      "Epoch 35/50\n",
      "1419/1419 [==============================] - 1s 573us/step - loss: 2.4143 - mean_absolute_error: 1.1997\n",
      "Epoch 36/50\n",
      "1419/1419 [==============================] - 1s 567us/step - loss: 2.2239 - mean_absolute_error: 1.1430\n",
      "Epoch 37/50\n",
      "1419/1419 [==============================] - 1s 505us/step - loss: 2.2572 - mean_absolute_error: 1.1687\n",
      "Epoch 38/50\n",
      "1419/1419 [==============================] - 1s 486us/step - loss: 2.2809 - mean_absolute_error: 1.1797\n",
      "Epoch 39/50\n",
      "1419/1419 [==============================] - 1s 495us/step - loss: 2.1046 - mean_absolute_error: 1.1267\n",
      "Epoch 40/50\n",
      "1419/1419 [==============================] - 1s 493us/step - loss: 2.1545 - mean_absolute_error: 1.1516\n",
      "Epoch 41/50\n",
      "1419/1419 [==============================] - 1s 484us/step - loss: 2.1915 - mean_absolute_error: 1.1563\n",
      "Epoch 42/50\n",
      "1419/1419 [==============================] - 1s 503us/step - loss: 2.2129 - mean_absolute_error: 1.1558\n",
      "Epoch 43/50\n",
      "1419/1419 [==============================] - 1s 568us/step - loss: 1.9272 - mean_absolute_error: 1.0620\n",
      "Epoch 44/50\n",
      "1419/1419 [==============================] - 1s 575us/step - loss: 1.9547 - mean_absolute_error: 1.0938\n",
      "Epoch 45/50\n",
      "1419/1419 [==============================] - 1s 571us/step - loss: 1.9567 - mean_absolute_error: 1.0881\n",
      "Epoch 46/50\n",
      "1419/1419 [==============================] - 1s 579us/step - loss: 1.8513 - mean_absolute_error: 1.0454\n",
      "Epoch 47/50\n",
      "1419/1419 [==============================] - 1s 564us/step - loss: 1.8915 - mean_absolute_error: 1.0686\n",
      "Epoch 48/50\n",
      "1419/1419 [==============================] - 1s 496us/step - loss: 1.7538 - mean_absolute_error: 1.0135\n",
      "Epoch 49/50\n",
      "1419/1419 [==============================] - 1s 493us/step - loss: 1.8111 - mean_absolute_error: 1.0441\n",
      "Epoch 50/50\n",
      "1419/1419 [==============================] - 1s 501us/step - loss: 1.6893 - mean_absolute_error: 1.0112\n",
      "QWK:  0.7777343899240503\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_39 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1419/1419 [==============================] - 8s 6ms/step - loss: 9.5592 - mean_absolute_error: 2.3251\n",
      "Epoch 2/50\n",
      "1419/1419 [==============================] - 1s 502us/step - loss: 4.3574 - mean_absolute_error: 1.6607\n",
      "Epoch 3/50\n",
      "1419/1419 [==============================] - 1s 512us/step - loss: 3.9641 - mean_absolute_error: 1.5637\n",
      "Epoch 4/50\n",
      "1419/1419 [==============================] - 1s 498us/step - loss: 3.8771 - mean_absolute_error: 1.5632\n",
      "Epoch 5/50\n",
      "1419/1419 [==============================] - 1s 641us/step - loss: 3.7054 - mean_absolute_error: 1.5392\n",
      "Epoch 6/50\n",
      "1419/1419 [==============================] - 1s 646us/step - loss: 3.5777 - mean_absolute_error: 1.4933\n",
      "Epoch 7/50\n",
      "1419/1419 [==============================] - 1s 651us/step - loss: 3.7747 - mean_absolute_error: 1.5324\n",
      "Epoch 8/50\n",
      "1419/1419 [==============================] - 1s 644us/step - loss: 3.5198 - mean_absolute_error: 1.4758\n",
      "Epoch 9/50\n",
      "1419/1419 [==============================] - 1s 607us/step - loss: 3.3072 - mean_absolute_error: 1.4443\n",
      "Epoch 10/50\n",
      "1419/1419 [==============================] - 1s 579us/step - loss: 3.3568 - mean_absolute_error: 1.4415\n",
      "Epoch 11/50\n",
      "1419/1419 [==============================] - 1s 580us/step - loss: 3.4479 - mean_absolute_error: 1.4511\n",
      "Epoch 12/50\n",
      "1419/1419 [==============================] - 1s 579us/step - loss: 3.3719 - mean_absolute_error: 1.4173\n",
      "Epoch 13/50\n",
      "1419/1419 [==============================] - 1s 581us/step - loss: 3.2111 - mean_absolute_error: 1.4135\n",
      "Epoch 14/50\n",
      "1419/1419 [==============================] - 1s 524us/step - loss: 3.3192 - mean_absolute_error: 1.4248\n",
      "Epoch 15/50\n",
      "1419/1419 [==============================] - 1s 498us/step - loss: 3.1795 - mean_absolute_error: 1.3806\n",
      "Epoch 16/50\n",
      "1419/1419 [==============================] - 1s 508us/step - loss: 3.3120 - mean_absolute_error: 1.4490\n",
      "Epoch 17/50\n",
      "1419/1419 [==============================] - 1s 495us/step - loss: 3.1388 - mean_absolute_error: 1.3845\n",
      "Epoch 18/50\n",
      "1419/1419 [==============================] - 1s 498us/step - loss: 2.9416 - mean_absolute_error: 1.3291\n",
      "Epoch 19/50\n",
      "1419/1419 [==============================] - 1s 518us/step - loss: 3.1042 - mean_absolute_error: 1.3796\n",
      "Epoch 20/50\n",
      "1419/1419 [==============================] - 1s 582us/step - loss: 2.9322 - mean_absolute_error: 1.3318\n",
      "Epoch 21/50\n",
      "1419/1419 [==============================] - 1s 576us/step - loss: 2.9890 - mean_absolute_error: 1.3548\n",
      "Epoch 22/50\n",
      "1419/1419 [==============================] - 1s 586us/step - loss: 2.7782 - mean_absolute_error: 1.3082\n",
      "Epoch 23/50\n",
      "1419/1419 [==============================] - 1s 583us/step - loss: 2.7791 - mean_absolute_error: 1.3100\n",
      "Epoch 24/50\n",
      "1419/1419 [==============================] - 1s 558us/step - loss: 2.7987 - mean_absolute_error: 1.2998\n",
      "Epoch 25/50\n",
      "1419/1419 [==============================] - 1s 498us/step - loss: 2.7320 - mean_absolute_error: 1.2798\n",
      "Epoch 26/50\n",
      "1419/1419 [==============================] - 1s 501us/step - loss: 2.7847 - mean_absolute_error: 1.2823\n",
      "Epoch 27/50\n",
      "1419/1419 [==============================] - 1s 504us/step - loss: 2.5184 - mean_absolute_error: 1.2425\n",
      "Epoch 28/50\n",
      "1419/1419 [==============================] - 1s 505us/step - loss: 2.6852 - mean_absolute_error: 1.2651\n",
      "Epoch 29/50\n",
      "1419/1419 [==============================] - 1s 505us/step - loss: 2.6089 - mean_absolute_error: 1.2606\n",
      "Epoch 30/50\n",
      "1419/1419 [==============================] - 1s 562us/step - loss: 2.3998 - mean_absolute_error: 1.2145\n",
      "Epoch 31/50\n",
      "1419/1419 [==============================] - 1s 584us/step - loss: 2.5000 - mean_absolute_error: 1.2172\n",
      "Epoch 32/50\n",
      "1419/1419 [==============================] - 1s 594us/step - loss: 2.4220 - mean_absolute_error: 1.2233\n",
      "Epoch 33/50\n",
      "1419/1419 [==============================] - 1s 591us/step - loss: 2.4460 - mean_absolute_error: 1.2099\n",
      "Epoch 34/50\n",
      "1419/1419 [==============================] - 1s 585us/step - loss: 2.3239 - mean_absolute_error: 1.1867\n",
      "Epoch 35/50\n",
      "1419/1419 [==============================] - 1s 508us/step - loss: 2.2554 - mean_absolute_error: 1.1572\n",
      "Epoch 36/50\n",
      "1419/1419 [==============================] - 1s 508us/step - loss: 2.2057 - mean_absolute_error: 1.1575\n",
      "Epoch 37/50\n",
      "1419/1419 [==============================] - 1s 505us/step - loss: 2.3632 - mean_absolute_error: 1.1696\n",
      "Epoch 38/50\n",
      "1419/1419 [==============================] - 1s 501us/step - loss: 2.1750 - mean_absolute_error: 1.1479\n",
      "Epoch 39/50\n",
      "1419/1419 [==============================] - 1s 503us/step - loss: 2.1437 - mean_absolute_error: 1.1332\n",
      "Epoch 40/50\n",
      "1419/1419 [==============================] - 1s 528us/step - loss: 1.9952 - mean_absolute_error: 1.1143\n",
      "Epoch 41/50\n",
      "1419/1419 [==============================] - 1s 588us/step - loss: 2.0159 - mean_absolute_error: 1.0934\n",
      "Epoch 42/50\n",
      "1419/1419 [==============================] - 1s 580us/step - loss: 2.0097 - mean_absolute_error: 1.0992\n",
      "Epoch 43/50\n",
      "1419/1419 [==============================] - 1s 584us/step - loss: 2.0322 - mean_absolute_error: 1.0985\n",
      "Epoch 44/50\n",
      "1419/1419 [==============================] - 1s 587us/step - loss: 1.9709 - mean_absolute_error: 1.0906\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419/1419 [==============================] - 1s 544us/step - loss: 1.9404 - mean_absolute_error: 1.0831\n",
      "Epoch 46/50\n",
      "1419/1419 [==============================] - 1s 493us/step - loss: 1.8944 - mean_absolute_error: 1.0818\n",
      "Epoch 47/50\n",
      "1419/1419 [==============================] - 1s 498us/step - loss: 1.8144 - mean_absolute_error: 1.0545\n",
      "Epoch 48/50\n",
      "1419/1419 [==============================] - 1s 490us/step - loss: 1.8458 - mean_absolute_error: 1.0460\n",
      "Epoch 49/50\n",
      "1419/1419 [==============================] - 1s 495us/step - loss: 1.8485 - mean_absolute_error: 1.0461\n",
      "Epoch 50/50\n",
      "1419/1419 [==============================] - 1s 492us/step - loss: 1.7385 - mean_absolute_error: 1.0211\n",
      "QWK:  0.7843643681591578\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.7757\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_41 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 9s 6ms/step - loss: 10.0262 - mean_absolute_error: 2.4071\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 1s 468us/step - loss: 3.0507 - mean_absolute_error: 1.3942\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 1s 473us/step - loss: 2.7178 - mean_absolute_error: 1.3111\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 1s 467us/step - loss: 2.7384 - mean_absolute_error: 1.3174\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 1s 470us/step - loss: 2.5194 - mean_absolute_error: 1.2653\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 1s 477us/step - loss: 2.4801 - mean_absolute_error: 1.2644\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 1s 490us/step - loss: 2.3997 - mean_absolute_error: 1.2577\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 1s 489us/step - loss: 2.3311 - mean_absolute_error: 1.2322\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 1s 489us/step - loss: 2.2331 - mean_absolute_error: 1.1934\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 1s 492us/step - loss: 2.1885 - mean_absolute_error: 1.1964\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 1s 489us/step - loss: 2.2737 - mean_absolute_error: 1.2110\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 1s 638us/step - loss: 2.2309 - mean_absolute_error: 1.2093\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 1s 639us/step - loss: 2.2697 - mean_absolute_error: 1.2103\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 1s 649us/step - loss: 2.2663 - mean_absolute_error: 1.2100\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 1s 640us/step - loss: 2.2859 - mean_absolute_error: 1.2243\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 1s 631us/step - loss: 2.1918 - mean_absolute_error: 1.1811\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 1s 576us/step - loss: 2.1601 - mean_absolute_error: 1.1833\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 1s 580us/step - loss: 2.0489 - mean_absolute_error: 1.1465\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 1s 575us/step - loss: 2.1353 - mean_absolute_error: 1.1647\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 1s 568us/step - loss: 1.9765 - mean_absolute_error: 1.1187\n",
      "Epoch 21/50\n",
      "1442/1442 [==============================] - 1s 578us/step - loss: 2.1727 - mean_absolute_error: 1.1784\n",
      "Epoch 22/50\n",
      "1442/1442 [==============================] - 1s 515us/step - loss: 2.0449 - mean_absolute_error: 1.1302\n",
      "Epoch 23/50\n",
      "1442/1442 [==============================] - 1s 496us/step - loss: 2.0126 - mean_absolute_error: 1.1162\n",
      "Epoch 24/50\n",
      "1442/1442 [==============================] - 1s 491us/step - loss: 1.9726 - mean_absolute_error: 1.1292\n",
      "Epoch 25/50\n",
      "1442/1442 [==============================] - 1s 491us/step - loss: 1.9152 - mean_absolute_error: 1.0913\n",
      "Epoch 26/50\n",
      "1442/1442 [==============================] - 1s 494us/step - loss: 1.9784 - mean_absolute_error: 1.1132\n",
      "Epoch 27/50\n",
      "1442/1442 [==============================] - 1s 497us/step - loss: 1.9064 - mean_absolute_error: 1.0956\n",
      "Epoch 28/50\n",
      "1442/1442 [==============================] - 1s 577us/step - loss: 1.8206 - mean_absolute_error: 1.0677\n",
      "Epoch 29/50\n",
      "1442/1442 [==============================] - 1s 573us/step - loss: 1.8410 - mean_absolute_error: 1.0846\n",
      "Epoch 30/50\n",
      "1442/1442 [==============================] - 1s 582us/step - loss: 1.8386 - mean_absolute_error: 1.0689\n",
      "Epoch 31/50\n",
      "1442/1442 [==============================] - 1s 580us/step - loss: 1.8885 - mean_absolute_error: 1.0869\n",
      "Epoch 32/50\n",
      "1442/1442 [==============================] - 1s 615us/step - loss: 1.8152 - mean_absolute_error: 1.0742\n",
      "Epoch 33/50\n",
      "1442/1442 [==============================] - 1s 724us/step - loss: 1.5715 - mean_absolute_error: 1.0003\n",
      "Epoch 34/50\n",
      "1442/1442 [==============================] - 1s 732us/step - loss: 1.6937 - mean_absolute_error: 1.0340\n",
      "Epoch 35/50\n",
      "1442/1442 [==============================] - 1s 744us/step - loss: 1.6755 - mean_absolute_error: 1.0277\n",
      "Epoch 36/50\n",
      "1442/1442 [==============================] - 1s 703us/step - loss: 1.6559 - mean_absolute_error: 1.0255\n",
      "Epoch 37/50\n",
      "1442/1442 [==============================] - 1s 659us/step - loss: 1.6778 - mean_absolute_error: 1.0222\n",
      "Epoch 38/50\n",
      "1442/1442 [==============================] - 1s 659us/step - loss: 1.6639 - mean_absolute_error: 1.0180\n",
      "Epoch 39/50\n",
      "1442/1442 [==============================] - 1s 641us/step - loss: 1.5828 - mean_absolute_error: 1.0000\n",
      "Epoch 40/50\n",
      "1442/1442 [==============================] - 1s 651us/step - loss: 1.5448 - mean_absolute_error: 0.9882\n",
      "Epoch 41/50\n",
      "1442/1442 [==============================] - 1s 574us/step - loss: 1.5449 - mean_absolute_error: 0.9857\n",
      "Epoch 42/50\n",
      "1442/1442 [==============================] - 1s 580us/step - loss: 1.4715 - mean_absolute_error: 0.9522\n",
      "Epoch 43/50\n",
      "1442/1442 [==============================] - 1s 579us/step - loss: 1.5056 - mean_absolute_error: 0.9781\n",
      "Epoch 44/50\n",
      "1442/1442 [==============================] - 1s 574us/step - loss: 1.4615 - mean_absolute_error: 0.9535\n",
      "Epoch 45/50\n",
      "1442/1442 [==============================] - 1s 544us/step - loss: 1.4605 - mean_absolute_error: 0.9555\n",
      "Epoch 46/50\n",
      "1442/1442 [==============================] - 1s 498us/step - loss: 1.4684 - mean_absolute_error: 0.9570\n",
      "Epoch 47/50\n",
      "1442/1442 [==============================] - 1s 497us/step - loss: 1.4179 - mean_absolute_error: 0.9458\n",
      "Epoch 48/50\n",
      "1442/1442 [==============================] - 1s 496us/step - loss: 1.4980 - mean_absolute_error: 0.9773\n",
      "Epoch 49/50\n",
      "1442/1442 [==============================] - 1s 499us/step - loss: 1.4015 - mean_absolute_error: 0.9437\n",
      "Epoch 50/50\n",
      "1442/1442 [==============================] - 1s 500us/step - loss: 1.3789 - mean_absolute_error: 0.9334\n",
      "QWK:  0.7696383178750934\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_43 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 10s 7ms/step - loss: 10.0326 - mean_absolute_error: 2.3966\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 1s 531us/step - loss: 3.0864 - mean_absolute_error: 1.3917\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 1s 529us/step - loss: 2.8149 - mean_absolute_error: 1.3289\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 1s 537us/step - loss: 2.6320 - mean_absolute_error: 1.2796\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 1s 606us/step - loss: 2.6074 - mean_absolute_error: 1.2787\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 1s 577us/step - loss: 2.5820 - mean_absolute_error: 1.2662\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 1s 566us/step - loss: 2.4567 - mean_absolute_error: 1.2491\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 1s 560us/step - loss: 2.4990 - mean_absolute_error: 1.2546\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 1s 546us/step - loss: 2.4623 - mean_absolute_error: 1.2476\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 1s 488us/step - loss: 2.3437 - mean_absolute_error: 1.2162\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 1s 490us/step - loss: 2.3692 - mean_absolute_error: 1.2374\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 1s 495us/step - loss: 2.2654 - mean_absolute_error: 1.1959\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 1s 494us/step - loss: 2.3690 - mean_absolute_error: 1.2159\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 1s 491us/step - loss: 2.2133 - mean_absolute_error: 1.1844\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 1s 539us/step - loss: 2.2723 - mean_absolute_error: 1.1889\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 1s 565us/step - loss: 2.2276 - mean_absolute_error: 1.1965\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 1s 570us/step - loss: 2.2678 - mean_absolute_error: 1.2050\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 1s 570us/step - loss: 2.2021 - mean_absolute_error: 1.1874\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 1s 571us/step - loss: 2.2213 - mean_absolute_error: 1.1847\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 1s 691us/step - loss: 2.0628 - mean_absolute_error: 1.1444\n",
      "Epoch 21/50\n",
      "1442/1442 [==============================] - 1s 717us/step - loss: 2.0519 - mean_absolute_error: 1.1444\n",
      "Epoch 22/50\n",
      "1442/1442 [==============================] - 1s 732us/step - loss: 1.9747 - mean_absolute_error: 1.1150\n",
      "Epoch 23/50\n",
      "1442/1442 [==============================] - 1s 713us/step - loss: 2.0568 - mean_absolute_error: 1.1435\n",
      "Epoch 24/50\n",
      "1442/1442 [==============================] - 1s 646us/step - loss: 2.0688 - mean_absolute_error: 1.1501\n",
      "Epoch 25/50\n",
      "1442/1442 [==============================] - 1s 639us/step - loss: 1.9850 - mean_absolute_error: 1.1268\n",
      "Epoch 26/50\n",
      "1442/1442 [==============================] - 1s 629us/step - loss: 1.8199 - mean_absolute_error: 1.0672\n",
      "Epoch 27/50\n",
      "1442/1442 [==============================] - 1s 636us/step - loss: 1.9777 - mean_absolute_error: 1.1219\n",
      "Epoch 28/50\n",
      "1442/1442 [==============================] - 1s 594us/step - loss: 1.8723 - mean_absolute_error: 1.0958\n",
      "Epoch 29/50\n",
      "1442/1442 [==============================] - 1s 571us/step - loss: 1.8945 - mean_absolute_error: 1.0983\n",
      "Epoch 30/50\n",
      "1442/1442 [==============================] - 1s 575us/step - loss: 1.9249 - mean_absolute_error: 1.1113\n",
      "Epoch 31/50\n",
      "1442/1442 [==============================] - 1s 576us/step - loss: 1.8413 - mean_absolute_error: 1.0889\n",
      "Epoch 32/50\n",
      "1442/1442 [==============================] - 1s 570us/step - loss: 1.8422 - mean_absolute_error: 1.0712\n",
      "Epoch 33/50\n",
      "1442/1442 [==============================] - 1s 517us/step - loss: 1.6552 - mean_absolute_error: 1.0175\n",
      "Epoch 34/50\n",
      "1442/1442 [==============================] - 1s 493us/step - loss: 1.7244 - mean_absolute_error: 1.0441\n",
      "Epoch 35/50\n",
      "1442/1442 [==============================] - 1s 496us/step - loss: 1.7671 - mean_absolute_error: 1.0569\n",
      "Epoch 36/50\n",
      "1442/1442 [==============================] - 1s 497us/step - loss: 1.7106 - mean_absolute_error: 1.0335\n",
      "Epoch 37/50\n",
      "1442/1442 [==============================] - 1s 495us/step - loss: 1.6170 - mean_absolute_error: 1.0057\n",
      "Epoch 38/50\n",
      "1442/1442 [==============================] - 1s 506us/step - loss: 1.6492 - mean_absolute_error: 1.0288\n",
      "Epoch 39/50\n",
      "1442/1442 [==============================] - 1s 577us/step - loss: 1.6975 - mean_absolute_error: 1.0289\n",
      "Epoch 40/50\n",
      "1442/1442 [==============================] - 1s 574us/step - loss: 1.6017 - mean_absolute_error: 1.0036\n",
      "Epoch 41/50\n",
      "1442/1442 [==============================] - 1s 569us/step - loss: 1.5002 - mean_absolute_error: 0.9678\n",
      "Epoch 42/50\n",
      "1442/1442 [==============================] - 1s 573us/step - loss: 1.5954 - mean_absolute_error: 1.0025\n",
      "Epoch 43/50\n",
      "1442/1442 [==============================] - 1s 549us/step - loss: 1.5044 - mean_absolute_error: 0.9757\n",
      "Epoch 44/50\n",
      "1442/1442 [==============================] - 1s 499us/step - loss: 1.5296 - mean_absolute_error: 0.9681\n",
      "Epoch 45/50\n",
      "1442/1442 [==============================] - 1s 497us/step - loss: 1.4680 - mean_absolute_error: 0.9561\n",
      "Epoch 46/50\n",
      "1442/1442 [==============================] - 1s 496us/step - loss: 1.3512 - mean_absolute_error: 0.9222\n",
      "Epoch 47/50\n",
      "1442/1442 [==============================] - 1s 498us/step - loss: 1.3608 - mean_absolute_error: 0.9294\n",
      "Epoch 48/50\n",
      "1442/1442 [==============================] - 1s 499us/step - loss: 1.4110 - mean_absolute_error: 0.9359\n",
      "Epoch 49/50\n",
      "1442/1442 [==============================] - 1s 560us/step - loss: 1.4440 - mean_absolute_error: 0.9446\n",
      "Epoch 50/50\n",
      "1442/1442 [==============================] - 1s 572us/step - loss: 1.3759 - mean_absolute_error: 0.9365\n",
      "QWK:  0.7840031201716094\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_45 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 9s 6ms/step - loss: 9.8521 - mean_absolute_error: 2.3599\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s 488us/step - loss: 3.0300 - mean_absolute_error: 1.3808\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s 480us/step - loss: 2.7685 - mean_absolute_error: 1.3303\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s 480us/step - loss: 2.6180 - mean_absolute_error: 1.3104\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s 488us/step - loss: 2.6022 - mean_absolute_error: 1.2953\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s 483us/step - loss: 2.5288 - mean_absolute_error: 1.2744\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s 484us/step - loss: 2.5146 - mean_absolute_error: 1.2764\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s 481us/step - loss: 2.4738 - mean_absolute_error: 1.2609\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s 481us/step - loss: 2.3174 - mean_absolute_error: 1.2112\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s 565us/step - loss: 2.3661 - mean_absolute_error: 1.2374\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s 584us/step - loss: 2.3552 - mean_absolute_error: 1.2234\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 1s 588us/step - loss: 2.3406 - mean_absolute_error: 1.2152\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s 598us/step - loss: 2.3198 - mean_absolute_error: 1.2026\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s 607us/step - loss: 2.3111 - mean_absolute_error: 1.2115\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s 548us/step - loss: 2.4160 - mean_absolute_error: 1.2291\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s 536us/step - loss: 2.1571 - mean_absolute_error: 1.1847\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s 533us/step - loss: 2.1725 - mean_absolute_error: 1.1719\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s 530us/step - loss: 2.1272 - mean_absolute_error: 1.1510\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 1s 509us/step - loss: 2.1981 - mean_absolute_error: 1.1787\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s 568us/step - loss: 2.1904 - mean_absolute_error: 1.1830\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 1s 563us/step - loss: 2.0957 - mean_absolute_error: 1.1471\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 1s 562us/step - loss: 2.1589 - mean_absolute_error: 1.1753\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 1s 568us/step - loss: 2.0578 - mean_absolute_error: 1.1336\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 1s 560us/step - loss: 1.9754 - mean_absolute_error: 1.1152\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 1s 487us/step - loss: 1.9041 - mean_absolute_error: 1.0909\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 1s 499us/step - loss: 1.9905 - mean_absolute_error: 1.1280\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 1s 491us/step - loss: 1.9292 - mean_absolute_error: 1.1157\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 1s 490us/step - loss: 1.9113 - mean_absolute_error: 1.0882\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 1s 493us/step - loss: 1.8839 - mean_absolute_error: 1.0999\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 1s 526us/step - loss: 1.8894 - mean_absolute_error: 1.0865\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 1s 563us/step - loss: 1.7890 - mean_absolute_error: 1.0648\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 1s 572us/step - loss: 1.8292 - mean_absolute_error: 1.0856\n",
      "Epoch 33/50\n",
      "1444/1444 [==============================] - 1s 573us/step - loss: 1.6815 - mean_absolute_error: 1.0348\n",
      "Epoch 34/50\n",
      "1444/1444 [==============================] - 1s 575us/step - loss: 1.7874 - mean_absolute_error: 1.0594\n",
      "Epoch 35/50\n",
      "1444/1444 [==============================] - 1s 674us/step - loss: 1.6856 - mean_absolute_error: 1.0194\n",
      "Epoch 36/50\n",
      "1444/1444 [==============================] - 1s 736us/step - loss: 1.6742 - mean_absolute_error: 1.0331\n",
      "Epoch 37/50\n",
      "1444/1444 [==============================] - 1s 730us/step - loss: 1.6997 - mean_absolute_error: 1.0323\n",
      "Epoch 38/50\n",
      "1444/1444 [==============================] - 1s 818us/step - loss: 1.6167 - mean_absolute_error: 1.0078\n",
      "Epoch 39/50\n",
      "1444/1444 [==============================] - 1s 646us/step - loss: 1.5334 - mean_absolute_error: 0.9763\n",
      "Epoch 40/50\n",
      "1444/1444 [==============================] - 1s 652us/step - loss: 1.6012 - mean_absolute_error: 1.0101\n",
      "Epoch 41/50\n",
      "1444/1444 [==============================] - 1s 650us/step - loss: 1.6635 - mean_absolute_error: 1.0266\n",
      "Epoch 42/50\n",
      "1444/1444 [==============================] - 1s 658us/step - loss: 1.6110 - mean_absolute_error: 1.0072\n",
      "Epoch 43/50\n",
      "1444/1444 [==============================] - 1s 590us/step - loss: 1.5691 - mean_absolute_error: 0.9962\n",
      "Epoch 44/50\n",
      "1444/1444 [==============================] - 1s 576us/step - loss: 1.4681 - mean_absolute_error: 0.9532\n",
      "Epoch 45/50\n",
      "1444/1444 [==============================] - 1s 581us/step - loss: 1.4666 - mean_absolute_error: 0.9562\n",
      "Epoch 46/50\n",
      "1444/1444 [==============================] - 1s 574us/step - loss: 1.4785 - mean_absolute_error: 0.9564\n",
      "Epoch 47/50\n",
      "1444/1444 [==============================] - 1s 569us/step - loss: 1.3978 - mean_absolute_error: 0.9353\n",
      "Epoch 48/50\n",
      "1444/1444 [==============================] - 1s 514us/step - loss: 1.3838 - mean_absolute_error: 0.9345\n",
      "Epoch 49/50\n",
      "1444/1444 [==============================] - 1s 496us/step - loss: 1.4208 - mean_absolute_error: 0.9479\n",
      "Epoch 50/50\n",
      "1444/1444 [==============================] - 1s 495us/step - loss: 1.5594 - mean_absolute_error: 0.9740\n",
      "QWK:  0.8134613386616136\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_47 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_48 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1445/1445 [==============================] - 10s 7ms/step - loss: 10.6948 - mean_absolute_error: 2.4651\n",
      "Epoch 2/50\n",
      "1445/1445 [==============================] - 1s 480us/step - loss: 2.9430 - mean_absolute_error: 1.3656\n",
      "Epoch 3/50\n",
      "1445/1445 [==============================] - 1s 481us/step - loss: 2.7714 - mean_absolute_error: 1.3269\n",
      "Epoch 4/50\n",
      "1445/1445 [==============================] - 1s 475us/step - loss: 2.6297 - mean_absolute_error: 1.2842\n",
      "Epoch 5/50\n",
      "1445/1445 [==============================] - 1s 468us/step - loss: 2.5298 - mean_absolute_error: 1.2722\n",
      "Epoch 6/50\n",
      "1445/1445 [==============================] - 1s 485us/step - loss: 2.5625 - mean_absolute_error: 1.2663\n",
      "Epoch 7/50\n",
      "1445/1445 [==============================] - 1s 491us/step - loss: 2.5486 - mean_absolute_error: 1.2718\n",
      "Epoch 8/50\n",
      "1445/1445 [==============================] - 1s 501us/step - loss: 2.4755 - mean_absolute_error: 1.2423\n",
      "Epoch 9/50\n",
      "1445/1445 [==============================] - 1s 503us/step - loss: 2.3502 - mean_absolute_error: 1.2197\n",
      "Epoch 10/50\n",
      "1445/1445 [==============================] - 1s 493us/step - loss: 2.3644 - mean_absolute_error: 1.2264\n",
      "Epoch 11/50\n",
      "1445/1445 [==============================] - 1s 517us/step - loss: 2.3309 - mean_absolute_error: 1.2266\n",
      "Epoch 12/50\n",
      "1445/1445 [==============================] - 1s 656us/step - loss: 2.2867 - mean_absolute_error: 1.2113\n",
      "Epoch 13/50\n",
      "1445/1445 [==============================] - 1s 642us/step - loss: 2.2862 - mean_absolute_error: 1.2027\n",
      "Epoch 14/50\n",
      "1445/1445 [==============================] - 1s 652us/step - loss: 2.2694 - mean_absolute_error: 1.2026\n",
      "Epoch 15/50\n",
      "1445/1445 [==============================] - 1s 649us/step - loss: 2.2526 - mean_absolute_error: 1.1889\n",
      "Epoch 16/50\n",
      "1445/1445 [==============================] - 1s 601us/step - loss: 2.2336 - mean_absolute_error: 1.1793\n",
      "Epoch 17/50\n",
      "1445/1445 [==============================] - 1s 580us/step - loss: 2.3261 - mean_absolute_error: 1.2036\n",
      "Epoch 18/50\n",
      "1445/1445 [==============================] - 1s 584us/step - loss: 2.1481 - mean_absolute_error: 1.1609\n",
      "Epoch 19/50\n",
      "1445/1445 [==============================] - 1s 575us/step - loss: 2.1449 - mean_absolute_error: 1.1698\n",
      "Epoch 20/50\n",
      "1445/1445 [==============================] - 1s 574us/step - loss: 2.2028 - mean_absolute_error: 1.1884\n",
      "Epoch 21/50\n",
      "1445/1445 [==============================] - 1s 504us/step - loss: 2.0139 - mean_absolute_error: 1.1306\n",
      "Epoch 22/50\n",
      "1445/1445 [==============================] - 1s 505us/step - loss: 2.1100 - mean_absolute_error: 1.1554\n",
      "Epoch 23/50\n",
      "1445/1445 [==============================] - 1s 491us/step - loss: 2.0973 - mean_absolute_error: 1.1552\n",
      "Epoch 24/50\n",
      "1445/1445 [==============================] - 1s 501us/step - loss: 1.9393 - mean_absolute_error: 1.1074\n",
      "Epoch 25/50\n",
      "1445/1445 [==============================] - 1s 503us/step - loss: 2.0014 - mean_absolute_error: 1.1141\n",
      "Epoch 26/50\n",
      "1445/1445 [==============================] - 1s 556us/step - loss: 1.9710 - mean_absolute_error: 1.1129\n",
      "Epoch 27/50\n",
      "1445/1445 [==============================] - 1s 579us/step - loss: 1.8062 - mean_absolute_error: 1.0696\n",
      "Epoch 28/50\n",
      "1445/1445 [==============================] - 1s 589us/step - loss: 1.8719 - mean_absolute_error: 1.1037\n",
      "Epoch 29/50\n",
      "1445/1445 [==============================] - 1s 611us/step - loss: 1.8836 - mean_absolute_error: 1.0873\n",
      "Epoch 30/50\n",
      "1445/1445 [==============================] - 1s 587us/step - loss: 1.7572 - mean_absolute_error: 1.0548\n",
      "Epoch 31/50\n",
      "1445/1445 [==============================] - 1s 509us/step - loss: 1.8200 - mean_absolute_error: 1.0597\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445/1445 [==============================] - 1s 493us/step - loss: 1.7130 - mean_absolute_error: 1.0450\n",
      "Epoch 33/50\n",
      "1445/1445 [==============================] - 1s 498us/step - loss: 1.6542 - mean_absolute_error: 1.0159\n",
      "Epoch 34/50\n",
      "1445/1445 [==============================] - 1s 494us/step - loss: 1.7615 - mean_absolute_error: 1.0313\n",
      "Epoch 35/50\n",
      "1445/1445 [==============================] - 1s 490us/step - loss: 1.7287 - mean_absolute_error: 1.0459\n",
      "Epoch 36/50\n",
      "1445/1445 [==============================] - 1s 518us/step - loss: 1.7369 - mean_absolute_error: 1.0574\n",
      "Epoch 37/50\n",
      "1445/1445 [==============================] - 1s 566us/step - loss: 1.6140 - mean_absolute_error: 1.0074\n",
      "Epoch 38/50\n",
      "1445/1445 [==============================] - 1s 562us/step - loss: 1.6242 - mean_absolute_error: 1.0075\n",
      "Epoch 39/50\n",
      "1445/1445 [==============================] - 1s 574us/step - loss: 1.6832 - mean_absolute_error: 1.0135\n",
      "Epoch 40/50\n",
      "1445/1445 [==============================] - 1s 564us/step - loss: 1.6226 - mean_absolute_error: 1.0013\n",
      "Epoch 41/50\n",
      "1445/1445 [==============================] - 1s 562us/step - loss: 1.4946 - mean_absolute_error: 0.9669\n",
      "Epoch 42/50\n",
      "1445/1445 [==============================] - 1s 511us/step - loss: 1.5856 - mean_absolute_error: 0.9835\n",
      "Epoch 43/50\n",
      "1445/1445 [==============================] - 1s 533us/step - loss: 1.4858 - mean_absolute_error: 0.9625\n",
      "Epoch 44/50\n",
      "1445/1445 [==============================] - 1s 503us/step - loss: 1.6641 - mean_absolute_error: 1.0131\n",
      "Epoch 45/50\n",
      "1445/1445 [==============================] - 1s 499us/step - loss: 1.4333 - mean_absolute_error: 0.9392\n",
      "Epoch 46/50\n",
      "1445/1445 [==============================] - 1s 499us/step - loss: 1.3875 - mean_absolute_error: 0.9341\n",
      "Epoch 47/50\n",
      "1445/1445 [==============================] - 1s 585us/step - loss: 1.5866 - mean_absolute_error: 0.9935\n",
      "Epoch 48/50\n",
      "1445/1445 [==============================] - 1s 575us/step - loss: 1.3831 - mean_absolute_error: 0.9307\n",
      "Epoch 49/50\n",
      "1445/1445 [==============================] - 1s 571us/step - loss: 1.4258 - mean_absolute_error: 0.9350\n",
      "Epoch 50/50\n",
      "1445/1445 [==============================] - 1s 599us/step - loss: 1.4420 - mean_absolute_error: 0.9500\n",
      "QWK:  0.8078588648061211\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_49 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1447/1447 [==============================] - 10s 7ms/step - loss: 10.0370 - mean_absolute_error: 2.4278\n",
      "Epoch 2/50\n",
      "1447/1447 [==============================] - 1s 529us/step - loss: 3.0373 - mean_absolute_error: 1.3882\n",
      "Epoch 3/50\n",
      "1447/1447 [==============================] - 1s 606us/step - loss: 2.8943 - mean_absolute_error: 1.3648\n",
      "Epoch 4/50\n",
      "1447/1447 [==============================] - 1s 670us/step - loss: 2.6064 - mean_absolute_error: 1.2896\n",
      "Epoch 5/50\n",
      "1447/1447 [==============================] - 1s 673us/step - loss: 2.5750 - mean_absolute_error: 1.2893\n",
      "Epoch 6/50\n",
      "1447/1447 [==============================] - 1s 697us/step - loss: 2.4772 - mean_absolute_error: 1.2644\n",
      "Epoch 7/50\n",
      "1447/1447 [==============================] - 1s 660us/step - loss: 2.5304 - mean_absolute_error: 1.2573\n",
      "Epoch 8/50\n",
      "1447/1447 [==============================] - 1s 620us/step - loss: 2.4460 - mean_absolute_error: 1.2433\n",
      "Epoch 9/50\n",
      "1447/1447 [==============================] - 1s 605us/step - loss: 2.5440 - mean_absolute_error: 1.2802\n",
      "Epoch 10/50\n",
      "1447/1447 [==============================] - 1s 603us/step - loss: 2.4366 - mean_absolute_error: 1.2291\n",
      "Epoch 11/50\n",
      "1447/1447 [==============================] - 1s 600us/step - loss: 2.3561 - mean_absolute_error: 1.2205\n",
      "Epoch 12/50\n",
      "1447/1447 [==============================] - 1s 540us/step - loss: 2.3088 - mean_absolute_error: 1.2150\n",
      "Epoch 13/50\n",
      "1447/1447 [==============================] - 1s 523us/step - loss: 2.2323 - mean_absolute_error: 1.1926\n",
      "Epoch 14/50\n",
      "1447/1447 [==============================] - 1s 533us/step - loss: 2.3133 - mean_absolute_error: 1.2104\n",
      "Epoch 15/50\n",
      "1447/1447 [==============================] - 1s 518us/step - loss: 2.1953 - mean_absolute_error: 1.1953\n",
      "Epoch 16/50\n",
      "1447/1447 [==============================] - 1s 528us/step - loss: 2.2086 - mean_absolute_error: 1.1802\n",
      "Epoch 17/50\n",
      "1447/1447 [==============================] - 1s 581us/step - loss: 2.1333 - mean_absolute_error: 1.1596\n",
      "Epoch 18/50\n",
      "1447/1447 [==============================] - 1s 611us/step - loss: 2.2189 - mean_absolute_error: 1.1611\n",
      "Epoch 19/50\n",
      "1447/1447 [==============================] - 1s 599us/step - loss: 2.0994 - mean_absolute_error: 1.1507\n",
      "Epoch 20/50\n",
      "1447/1447 [==============================] - 1s 607us/step - loss: 2.2754 - mean_absolute_error: 1.1973\n",
      "Epoch 21/50\n",
      "1447/1447 [==============================] - 1s 596us/step - loss: 2.0090 - mean_absolute_error: 1.1294\n",
      "Epoch 22/50\n",
      "1447/1447 [==============================] - 1s 525us/step - loss: 2.0406 - mean_absolute_error: 1.1392\n",
      "Epoch 23/50\n",
      "1447/1447 [==============================] - 1s 523us/step - loss: 2.0352 - mean_absolute_error: 1.1303\n",
      "Epoch 24/50\n",
      "1447/1447 [==============================] - 1s 524us/step - loss: 2.0113 - mean_absolute_error: 1.1234\n",
      "Epoch 25/50\n",
      "1447/1447 [==============================] - 1s 529us/step - loss: 1.8797 - mean_absolute_error: 1.0883\n",
      "Epoch 26/50\n",
      "1447/1447 [==============================] - 1s 527us/step - loss: 1.9087 - mean_absolute_error: 1.0924\n",
      "Epoch 27/50\n",
      "1447/1447 [==============================] - 1s 600us/step - loss: 1.9774 - mean_absolute_error: 1.1054\n",
      "Epoch 28/50\n",
      "1447/1447 [==============================] - 1s 606us/step - loss: 1.8995 - mean_absolute_error: 1.0818\n",
      "Epoch 29/50\n",
      "1447/1447 [==============================] - 1s 617us/step - loss: 1.7629 - mean_absolute_error: 1.0620\n",
      "Epoch 30/50\n",
      "1447/1447 [==============================] - 1s 616us/step - loss: 1.8894 - mean_absolute_error: 1.0966\n",
      "Epoch 31/50\n",
      "1447/1447 [==============================] - 1s 579us/step - loss: 1.8470 - mean_absolute_error: 1.0803\n",
      "Epoch 32/50\n",
      "1447/1447 [==============================] - 1s 539us/step - loss: 1.8111 - mean_absolute_error: 1.0799\n",
      "Epoch 33/50\n",
      "1447/1447 [==============================] - 1s 535us/step - loss: 1.6289 - mean_absolute_error: 1.0071\n",
      "Epoch 34/50\n",
      "1447/1447 [==============================] - 1s 522us/step - loss: 1.7508 - mean_absolute_error: 1.0597\n",
      "Epoch 35/50\n",
      "1447/1447 [==============================] - 1s 524us/step - loss: 1.7437 - mean_absolute_error: 1.0436\n",
      "Epoch 36/50\n",
      "1447/1447 [==============================] - 1s 541us/step - loss: 1.6781 - mean_absolute_error: 1.0334\n",
      "Epoch 37/50\n",
      "1447/1447 [==============================] - 1s 609us/step - loss: 1.6601 - mean_absolute_error: 1.0192\n",
      "Epoch 38/50\n",
      "1447/1447 [==============================] - 1s 619us/step - loss: 1.6448 - mean_absolute_error: 1.0087\n",
      "Epoch 39/50\n",
      "1447/1447 [==============================] - 1s 612us/step - loss: 1.6290 - mean_absolute_error: 1.0088\n",
      "Epoch 40/50\n",
      "1447/1447 [==============================] - 1s 655us/step - loss: 1.5865 - mean_absolute_error: 0.9932\n",
      "Epoch 41/50\n",
      "1447/1447 [==============================] - 1s 553us/step - loss: 1.5028 - mean_absolute_error: 0.9799\n",
      "Epoch 42/50\n",
      "1447/1447 [==============================] - 1s 527us/step - loss: 1.5830 - mean_absolute_error: 0.9907\n",
      "Epoch 43/50\n",
      "1447/1447 [==============================] - 1s 528us/step - loss: 1.5915 - mean_absolute_error: 0.9980\n",
      "Epoch 44/50\n",
      "1447/1447 [==============================] - 1s 530us/step - loss: 1.4680 - mean_absolute_error: 0.9510\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1447/1447 [==============================] - 1s 501us/step - loss: 1.4844 - mean_absolute_error: 0.9676\n",
      "Epoch 46/50\n",
      "1447/1447 [==============================] - 1s 521us/step - loss: 1.5071 - mean_absolute_error: 0.9659\n",
      "Epoch 47/50\n",
      "1447/1447 [==============================] - 1s 574us/step - loss: 1.3881 - mean_absolute_error: 0.9399\n",
      "Epoch 48/50\n",
      "1447/1447 [==============================] - 1s 569us/step - loss: 1.4418 - mean_absolute_error: 0.9407\n",
      "Epoch 49/50\n",
      "1447/1447 [==============================] - 1s 570us/step - loss: 1.3580 - mean_absolute_error: 0.9214\n",
      "Epoch 50/50\n",
      "1447/1447 [==============================] - 1s 560us/step - loss: 1.3441 - mean_absolute_error: 0.9250\n",
      "QWK:  0.7945521518565959\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.7939\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_51 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_52 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1438/1438 [==============================] - 10s 7ms/step - loss: 11.6726 - mean_absolute_error: 2.5771\n",
      "Epoch 2/50\n",
      "1438/1438 [==============================] - 1s 479us/step - loss: 3.2706 - mean_absolute_error: 1.4346\n",
      "Epoch 3/50\n",
      "1438/1438 [==============================] - 1s 477us/step - loss: 3.0090 - mean_absolute_error: 1.3906\n",
      "Epoch 4/50\n",
      "1438/1438 [==============================] - 1s 488us/step - loss: 2.9662 - mean_absolute_error: 1.3752\n",
      "Epoch 5/50\n",
      "1438/1438 [==============================] - 1s 493us/step - loss: 2.9143 - mean_absolute_error: 1.3855\n",
      "Epoch 6/50\n",
      "1438/1438 [==============================] - 1s 503us/step - loss: 2.7551 - mean_absolute_error: 1.3177\n",
      "Epoch 7/50\n",
      "1438/1438 [==============================] - 1s 495us/step - loss: 2.7196 - mean_absolute_error: 1.3152\n",
      "Epoch 8/50\n",
      "1438/1438 [==============================] - 1s 505us/step - loss: 2.8295 - mean_absolute_error: 1.3443\n",
      "Epoch 9/50\n",
      "1438/1438 [==============================] - 1s 495us/step - loss: 2.7376 - mean_absolute_error: 1.3266\n",
      "Epoch 10/50\n",
      "1438/1438 [==============================] - 1s 612us/step - loss: 2.7933 - mean_absolute_error: 1.3130\n",
      "Epoch 11/50\n",
      "1438/1438 [==============================] - 1s 642us/step - loss: 2.7491 - mean_absolute_error: 1.3162\n",
      "Epoch 12/50\n",
      "1438/1438 [==============================] - 1s 652us/step - loss: 2.7669 - mean_absolute_error: 1.3257\n",
      "Epoch 13/50\n",
      "1438/1438 [==============================] - 1s 646us/step - loss: 2.6177 - mean_absolute_error: 1.2925\n",
      "Epoch 14/50\n",
      "1438/1438 [==============================] - 1s 620us/step - loss: 2.5575 - mean_absolute_error: 1.2759\n",
      "Epoch 15/50\n",
      "1438/1438 [==============================] - 1s 584us/step - loss: 2.4166 - mean_absolute_error: 1.2420\n",
      "Epoch 16/50\n",
      "1438/1438 [==============================] - 1s 579us/step - loss: 2.4167 - mean_absolute_error: 1.2365\n",
      "Epoch 17/50\n",
      "1438/1438 [==============================] - 1s 579us/step - loss: 2.4303 - mean_absolute_error: 1.2434\n",
      "Epoch 18/50\n",
      "1438/1438 [==============================] - 1s 579us/step - loss: 2.3496 - mean_absolute_error: 1.2119\n",
      "Epoch 19/50\n",
      "1438/1438 [==============================] - 1s 526us/step - loss: 2.3841 - mean_absolute_error: 1.2355\n",
      "Epoch 20/50\n",
      "1438/1438 [==============================] - 1s 505us/step - loss: 2.3394 - mean_absolute_error: 1.1986\n",
      "Epoch 21/50\n",
      "1438/1438 [==============================] - 1s 509us/step - loss: 2.4222 - mean_absolute_error: 1.2452\n",
      "Epoch 22/50\n",
      "1438/1438 [==============================] - 1s 500us/step - loss: 2.3297 - mean_absolute_error: 1.2179\n",
      "Epoch 23/50\n",
      "1438/1438 [==============================] - 1s 498us/step - loss: 2.2897 - mean_absolute_error: 1.2101\n",
      "Epoch 24/50\n",
      "1438/1438 [==============================] - 1s 522us/step - loss: 2.2650 - mean_absolute_error: 1.1921\n",
      "Epoch 25/50\n",
      "1438/1438 [==============================] - 1s 584us/step - loss: 2.1981 - mean_absolute_error: 1.1703\n",
      "Epoch 26/50\n",
      "1438/1438 [==============================] - 1s 580us/step - loss: 2.1822 - mean_absolute_error: 1.1739\n",
      "Epoch 27/50\n",
      "1438/1438 [==============================] - 1s 585us/step - loss: 2.3554 - mean_absolute_error: 1.2082\n",
      "Epoch 28/50\n",
      "1438/1438 [==============================] - 1s 580us/step - loss: 2.1946 - mean_absolute_error: 1.1612\n",
      "Epoch 29/50\n",
      "1438/1438 [==============================] - 1s 635us/step - loss: 2.0182 - mean_absolute_error: 1.1316\n",
      "Epoch 30/50\n",
      "1438/1438 [==============================] - 1s 732us/step - loss: 2.1672 - mean_absolute_error: 1.1683\n",
      "Epoch 31/50\n",
      "1438/1438 [==============================] - 1s 743us/step - loss: 2.0179 - mean_absolute_error: 1.1314\n",
      "Epoch 32/50\n",
      "1438/1438 [==============================] - 1s 720us/step - loss: 2.1049 - mean_absolute_error: 1.1519\n",
      "Epoch 33/50\n",
      "1438/1438 [==============================] - 1s 688us/step - loss: 2.1018 - mean_absolute_error: 1.1447\n",
      "Epoch 34/50\n",
      "1438/1438 [==============================] - 1s 663us/step - loss: 2.1214 - mean_absolute_error: 1.1479\n",
      "Epoch 35/50\n",
      "1438/1438 [==============================] - 1s 657us/step - loss: 2.0813 - mean_absolute_error: 1.1466\n",
      "Epoch 36/50\n",
      "1438/1438 [==============================] - 1s 655us/step - loss: 1.9436 - mean_absolute_error: 1.0890\n",
      "Epoch 37/50\n",
      "1438/1438 [==============================] - 1s 627us/step - loss: 2.0446 - mean_absolute_error: 1.1342\n",
      "Epoch 38/50\n",
      "1438/1438 [==============================] - 1s 587us/step - loss: 1.8201 - mean_absolute_error: 1.0542\n",
      "Epoch 39/50\n",
      "1438/1438 [==============================] - 1s 588us/step - loss: 1.9464 - mean_absolute_error: 1.1224\n",
      "Epoch 40/50\n",
      "1438/1438 [==============================] - 1s 578us/step - loss: 1.8612 - mean_absolute_error: 1.0683\n",
      "Epoch 41/50\n",
      "1438/1438 [==============================] - 1s 598us/step - loss: 1.9285 - mean_absolute_error: 1.0925\n",
      "Epoch 42/50\n",
      "1438/1438 [==============================] - 1s 536us/step - loss: 1.8658 - mean_absolute_error: 1.0848\n",
      "Epoch 43/50\n",
      "1438/1438 [==============================] - 1s 502us/step - loss: 1.8773 - mean_absolute_error: 1.0778\n",
      "Epoch 44/50\n",
      "1438/1438 [==============================] - 1s 504us/step - loss: 1.8737 - mean_absolute_error: 1.0861\n",
      "Epoch 45/50\n",
      "1438/1438 [==============================] - 1s 513us/step - loss: 1.8357 - mean_absolute_error: 1.0744\n",
      "Epoch 46/50\n",
      "1438/1438 [==============================] - 1s 507us/step - loss: 1.8218 - mean_absolute_error: 1.0769\n",
      "Epoch 47/50\n",
      "1438/1438 [==============================] - 1s 525us/step - loss: 1.8274 - mean_absolute_error: 1.0605\n",
      "Epoch 48/50\n",
      "1438/1438 [==============================] - 1s 590us/step - loss: 1.8778 - mean_absolute_error: 1.0805\n",
      "Epoch 49/50\n",
      "1438/1438 [==============================] - 1s 585us/step - loss: 1.6960 - mean_absolute_error: 1.0200\n",
      "Epoch 50/50\n",
      "1438/1438 [==============================] - 1s 585us/step - loss: 1.8293 - mean_absolute_error: 1.0639\n",
      "QWK:  0.793754623382869\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_53 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_54 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1438/1438 [==============================] - 12s 8ms/step - loss: 11.8425 - mean_absolute_error: 2.6085\n",
      "Epoch 2/50\n",
      "1438/1438 [==============================] - 1s 522us/step - loss: 3.5659 - mean_absolute_error: 1.5209\n",
      "Epoch 3/50\n",
      "1438/1438 [==============================] - 1s 530us/step - loss: 3.1390 - mean_absolute_error: 1.4220\n",
      "Epoch 4/50\n",
      "1438/1438 [==============================] - 1s 546us/step - loss: 2.9559 - mean_absolute_error: 1.3693\n",
      "Epoch 5/50\n",
      "1438/1438 [==============================] - 1s 533us/step - loss: 2.9029 - mean_absolute_error: 1.3653\n",
      "Epoch 6/50\n",
      "1438/1438 [==============================] - 1s 502us/step - loss: 2.7160 - mean_absolute_error: 1.3190\n",
      "Epoch 7/50\n",
      "1438/1438 [==============================] - 1s 503us/step - loss: 2.7635 - mean_absolute_error: 1.3153\n",
      "Epoch 8/50\n",
      "1438/1438 [==============================] - 1s 624us/step - loss: 2.7893 - mean_absolute_error: 1.3319\n",
      "Epoch 9/50\n",
      "1438/1438 [==============================] - 1s 635us/step - loss: 2.7256 - mean_absolute_error: 1.3246\n",
      "Epoch 10/50\n",
      "1438/1438 [==============================] - 1s 675us/step - loss: 2.6582 - mean_absolute_error: 1.3139\n",
      "Epoch 11/50\n",
      "1438/1438 [==============================] - 1s 640us/step - loss: 2.6562 - mean_absolute_error: 1.2983\n",
      "Epoch 12/50\n",
      "1438/1438 [==============================] - 1s 603us/step - loss: 2.6607 - mean_absolute_error: 1.2953\n",
      "Epoch 13/50\n",
      "1438/1438 [==============================] - 1s 569us/step - loss: 2.4980 - mean_absolute_error: 1.2671\n",
      "Epoch 14/50\n",
      "1438/1438 [==============================] - 1s 573us/step - loss: 2.6044 - mean_absolute_error: 1.2971\n",
      "Epoch 15/50\n",
      "1438/1438 [==============================] - 1s 573us/step - loss: 2.4969 - mean_absolute_error: 1.2525\n",
      "Epoch 16/50\n",
      "1438/1438 [==============================] - 1s 573us/step - loss: 2.4259 - mean_absolute_error: 1.2361\n",
      "Epoch 17/50\n",
      "1438/1438 [==============================] - 1s 528us/step - loss: 2.5585 - mean_absolute_error: 1.2728\n",
      "Epoch 18/50\n",
      "1438/1438 [==============================] - 1s 500us/step - loss: 2.3829 - mean_absolute_error: 1.2434\n",
      "Epoch 19/50\n",
      "1438/1438 [==============================] - 1s 495us/step - loss: 2.2987 - mean_absolute_error: 1.2041\n",
      "Epoch 20/50\n",
      "1438/1438 [==============================] - 1s 501us/step - loss: 2.5529 - mean_absolute_error: 1.2549\n",
      "Epoch 21/50\n",
      "1438/1438 [==============================] - 1s 501us/step - loss: 2.2767 - mean_absolute_error: 1.2029\n",
      "Epoch 22/50\n",
      "1438/1438 [==============================] - 1s 512us/step - loss: 2.1929 - mean_absolute_error: 1.1880\n",
      "Epoch 23/50\n",
      "1438/1438 [==============================] - 1s 576us/step - loss: 2.2178 - mean_absolute_error: 1.1862\n",
      "Epoch 24/50\n",
      "1438/1438 [==============================] - 1s 573us/step - loss: 2.2293 - mean_absolute_error: 1.1776\n",
      "Epoch 25/50\n",
      "1438/1438 [==============================] - 1s 575us/step - loss: 2.2773 - mean_absolute_error: 1.1874\n",
      "Epoch 26/50\n",
      "1438/1438 [==============================] - 1s 579us/step - loss: 2.2669 - mean_absolute_error: 1.1982\n",
      "Epoch 27/50\n",
      "1438/1438 [==============================] - 1s 551us/step - loss: 2.1832 - mean_absolute_error: 1.1564\n",
      "Epoch 28/50\n",
      "1438/1438 [==============================] - 1s 494us/step - loss: 2.1071 - mean_absolute_error: 1.1354\n",
      "Epoch 29/50\n",
      "1438/1438 [==============================] - 1s 507us/step - loss: 2.0248 - mean_absolute_error: 1.1176\n",
      "Epoch 30/50\n",
      "1438/1438 [==============================] - 1s 499us/step - loss: 2.0874 - mean_absolute_error: 1.1340\n",
      "Epoch 31/50\n",
      "1438/1438 [==============================] - 1s 503us/step - loss: 2.2027 - mean_absolute_error: 1.1761\n",
      "Epoch 32/50\n",
      "1438/1438 [==============================] - 1s 499us/step - loss: 2.0912 - mean_absolute_error: 1.1490\n",
      "Epoch 33/50\n",
      "1438/1438 [==============================] - 1s 571us/step - loss: 1.8872 - mean_absolute_error: 1.0832\n",
      "Epoch 34/50\n",
      "1438/1438 [==============================] - 1s 576us/step - loss: 1.9778 - mean_absolute_error: 1.1006\n",
      "Epoch 35/50\n",
      "1438/1438 [==============================] - 1s 572us/step - loss: 1.9779 - mean_absolute_error: 1.1174\n",
      "Epoch 36/50\n",
      "1438/1438 [==============================] - 1s 580us/step - loss: 1.9079 - mean_absolute_error: 1.0895\n",
      "Epoch 37/50\n",
      "1438/1438 [==============================] - 1s 573us/step - loss: 1.9972 - mean_absolute_error: 1.1171\n",
      "Epoch 38/50\n",
      "1438/1438 [==============================] - 1s 722us/step - loss: 1.9816 - mean_absolute_error: 1.1026\n",
      "Epoch 39/50\n",
      "1438/1438 [==============================] - 1s 724us/step - loss: 1.7513 - mean_absolute_error: 1.0424\n",
      "Epoch 40/50\n",
      "1438/1438 [==============================] - 1s 736us/step - loss: 1.8273 - mean_absolute_error: 1.0592\n",
      "Epoch 41/50\n",
      "1438/1438 [==============================] - 1s 717us/step - loss: 1.9158 - mean_absolute_error: 1.1068\n",
      "Epoch 42/50\n",
      "1438/1438 [==============================] - 1s 660us/step - loss: 1.8298 - mean_absolute_error: 1.0704\n",
      "Epoch 43/50\n",
      "1438/1438 [==============================] - 1s 687us/step - loss: 1.7805 - mean_absolute_error: 1.0566\n",
      "Epoch 44/50\n",
      "1438/1438 [==============================] - 1s 700us/step - loss: 1.8068 - mean_absolute_error: 1.0601\n",
      "Epoch 45/50\n",
      "1438/1438 [==============================] - 1s 703us/step - loss: 1.7000 - mean_absolute_error: 1.0288\n",
      "Epoch 46/50\n",
      "1438/1438 [==============================] - 1s 603us/step - loss: 1.8082 - mean_absolute_error: 1.0690\n",
      "Epoch 47/50\n",
      "1438/1438 [==============================] - 1s 635us/step - loss: 1.8235 - mean_absolute_error: 1.0637\n",
      "Epoch 48/50\n",
      "1438/1438 [==============================] - 1s 668us/step - loss: 1.6624 - mean_absolute_error: 1.0263\n",
      "Epoch 49/50\n",
      "1438/1438 [==============================] - 1s 663us/step - loss: 1.7512 - mean_absolute_error: 1.0325\n",
      "Epoch 50/50\n",
      "1438/1438 [==============================] - 1s 573us/step - loss: 1.6315 - mean_absolute_error: 1.0108\n",
      "QWK:  0.7585776546657534\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_55 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1441/1441 [==============================] - 11s 7ms/step - loss: 10.4808 - mean_absolute_error: 2.4422\n",
      "Epoch 2/50\n",
      "1441/1441 [==============================] - 1s 519us/step - loss: 3.4470 - mean_absolute_error: 1.4988\n",
      "Epoch 3/50\n",
      "1441/1441 [==============================] - 1s 516us/step - loss: 3.0052 - mean_absolute_error: 1.3711\n",
      "Epoch 4/50\n",
      "1441/1441 [==============================] - 1s 515us/step - loss: 2.9890 - mean_absolute_error: 1.3905\n",
      "Epoch 5/50\n",
      "1441/1441 [==============================] - 1s 514us/step - loss: 2.9893 - mean_absolute_error: 1.3916\n",
      "Epoch 6/50\n",
      "1441/1441 [==============================] - 1s 514us/step - loss: 2.7794 - mean_absolute_error: 1.3537\n",
      "Epoch 7/50\n",
      "1441/1441 [==============================] - 1s 518us/step - loss: 2.8194 - mean_absolute_error: 1.3430\n",
      "Epoch 8/50\n",
      "1441/1441 [==============================] - 1s 517us/step - loss: 2.6184 - mean_absolute_error: 1.3018\n",
      "Epoch 9/50\n",
      "1441/1441 [==============================] - 1s 583us/step - loss: 2.6748 - mean_absolute_error: 1.3190\n",
      "Epoch 10/50\n",
      "1441/1441 [==============================] - 1s 615us/step - loss: 2.6616 - mean_absolute_error: 1.2946\n",
      "Epoch 11/50\n",
      "1441/1441 [==============================] - 1s 629us/step - loss: 2.6744 - mean_absolute_error: 1.3097\n",
      "Epoch 12/50\n",
      "1441/1441 [==============================] - 1s 621us/step - loss: 2.5906 - mean_absolute_error: 1.2728\n",
      "Epoch 13/50\n",
      "1441/1441 [==============================] - 1s 610us/step - loss: 2.6333 - mean_absolute_error: 1.2954\n",
      "Epoch 14/50\n",
      "1441/1441 [==============================] - 1s 859us/step - loss: 2.6767 - mean_absolute_error: 1.2961\n",
      "Epoch 15/50\n",
      "1441/1441 [==============================] - 1s 864us/step - loss: 2.4885 - mean_absolute_error: 1.2693\n",
      "Epoch 16/50\n",
      "1441/1441 [==============================] - 1s 856us/step - loss: 2.3523 - mean_absolute_error: 1.2175\n",
      "Epoch 17/50\n",
      "1441/1441 [==============================] - 1s 712us/step - loss: 2.3041 - mean_absolute_error: 1.2125\n",
      "Epoch 18/50\n",
      "1441/1441 [==============================] - 1s 704us/step - loss: 2.3619 - mean_absolute_error: 1.2180\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441/1441 [==============================] - 1s 656us/step - loss: 2.3277 - mean_absolute_error: 1.2278\n",
      "Epoch 20/50\n",
      "1441/1441 [==============================] - 1s 638us/step - loss: 2.3252 - mean_absolute_error: 1.2191\n",
      "Epoch 21/50\n",
      "1441/1441 [==============================] - 1s 575us/step - loss: 2.2824 - mean_absolute_error: 1.1994\n",
      "Epoch 22/50\n",
      "1441/1441 [==============================] - 1s 569us/step - loss: 2.3562 - mean_absolute_error: 1.2015\n",
      "Epoch 23/50\n",
      "1441/1441 [==============================] - 1s 567us/step - loss: 2.0868 - mean_absolute_error: 1.1557\n",
      "Epoch 24/50\n",
      "1441/1441 [==============================] - 1s 570us/step - loss: 2.2924 - mean_absolute_error: 1.2051\n",
      "Epoch 25/50\n",
      "1441/1441 [==============================] - 1s 575us/step - loss: 2.1292 - mean_absolute_error: 1.1601\n",
      "Epoch 26/50\n",
      "1441/1441 [==============================] - 1s 508us/step - loss: 2.1970 - mean_absolute_error: 1.1721\n",
      "Epoch 27/50\n",
      "1441/1441 [==============================] - 1s 491us/step - loss: 2.1585 - mean_absolute_error: 1.1619\n",
      "Epoch 28/50\n",
      "1441/1441 [==============================] - 1s 499us/step - loss: 2.2189 - mean_absolute_error: 1.1686\n",
      "Epoch 29/50\n",
      "1441/1441 [==============================] - 1s 490us/step - loss: 1.9899 - mean_absolute_error: 1.1152\n",
      "Epoch 30/50\n",
      "1441/1441 [==============================] - 1s 493us/step - loss: 1.9482 - mean_absolute_error: 1.1214\n",
      "Epoch 31/50\n",
      "1441/1441 [==============================] - 1s 515us/step - loss: 2.0398 - mean_absolute_error: 1.1281\n",
      "Epoch 32/50\n",
      "1441/1441 [==============================] - 1s 571us/step - loss: 2.0348 - mean_absolute_error: 1.1360\n",
      "Epoch 33/50\n",
      "1441/1441 [==============================] - 1s 577us/step - loss: 1.9412 - mean_absolute_error: 1.1070\n",
      "Epoch 34/50\n",
      "1441/1441 [==============================] - 1s 572us/step - loss: 1.9567 - mean_absolute_error: 1.1038\n",
      "Epoch 35/50\n",
      "1441/1441 [==============================] - 1s 589us/step - loss: 2.0308 - mean_absolute_error: 1.1139\n",
      "Epoch 36/50\n",
      "1441/1441 [==============================] - 1s 676us/step - loss: 1.9492 - mean_absolute_error: 1.1083\n",
      "Epoch 37/50\n",
      "1441/1441 [==============================] - 1s 826us/step - loss: 1.8597 - mean_absolute_error: 1.0787\n",
      "Epoch 38/50\n",
      "1441/1441 [==============================] - 1s 847us/step - loss: 2.0734 - mean_absolute_error: 1.1272\n",
      "Epoch 39/50\n",
      "1441/1441 [==============================] - 1s 839us/step - loss: 1.8432 - mean_absolute_error: 1.0636\n",
      "Epoch 40/50\n",
      "1441/1441 [==============================] - 1s 669us/step - loss: 1.8671 - mean_absolute_error: 1.0876\n",
      "Epoch 41/50\n",
      "1441/1441 [==============================] - 1s 713us/step - loss: 1.7469 - mean_absolute_error: 1.0412\n",
      "Epoch 42/50\n",
      "1441/1441 [==============================] - 1s 664us/step - loss: 1.8550 - mean_absolute_error: 1.0716\n",
      "Epoch 43/50\n",
      "1441/1441 [==============================] - 1s 676us/step - loss: 1.7023 - mean_absolute_error: 1.0299\n",
      "Epoch 44/50\n",
      "1441/1441 [==============================] - 1s 606us/step - loss: 1.7263 - mean_absolute_error: 1.0378\n",
      "Epoch 45/50\n",
      "1441/1441 [==============================] - 1s 627us/step - loss: 1.6790 - mean_absolute_error: 1.0239\n",
      "Epoch 46/50\n",
      "1441/1441 [==============================] - 1s 638us/step - loss: 1.8362 - mean_absolute_error: 1.0711\n",
      "Epoch 47/50\n",
      "1441/1441 [==============================] - 1s 637us/step - loss: 1.6425 - mean_absolute_error: 1.0009\n",
      "Epoch 48/50\n",
      "1441/1441 [==============================] - 1s 551us/step - loss: 1.6105 - mean_absolute_error: 1.0012\n",
      "Epoch 49/50\n",
      "1441/1441 [==============================] - 1s 515us/step - loss: 1.7617 - mean_absolute_error: 1.0332\n",
      "Epoch 50/50\n",
      "1441/1441 [==============================] - 1s 540us/step - loss: 1.5739 - mean_absolute_error: 0.9901\n",
      "QWK:  0.7651599026218296\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_57 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1441/1441 [==============================] - 11s 8ms/step - loss: 11.4537 - mean_absolute_error: 2.5840\n",
      "Epoch 2/50\n",
      "1441/1441 [==============================] - 1s 513us/step - loss: 3.6609 - mean_absolute_error: 1.5322\n",
      "Epoch 3/50\n",
      "1441/1441 [==============================] - 1s 509us/step - loss: 3.0949 - mean_absolute_error: 1.4095\n",
      "Epoch 4/50\n",
      "1441/1441 [==============================] - 1s 514us/step - loss: 2.8452 - mean_absolute_error: 1.3440\n",
      "Epoch 5/50\n",
      "1441/1441 [==============================] - 1s 509us/step - loss: 2.7134 - mean_absolute_error: 1.3118\n",
      "Epoch 6/50\n",
      "1441/1441 [==============================] - 1s 580us/step - loss: 2.7458 - mean_absolute_error: 1.3303\n",
      "Epoch 7/50\n",
      "1441/1441 [==============================] - 1s 619us/step - loss: 2.6840 - mean_absolute_error: 1.3190\n",
      "Epoch 8/50\n",
      "1441/1441 [==============================] - 1s 604us/step - loss: 2.7673 - mean_absolute_error: 1.3444\n",
      "Epoch 9/50\n",
      "1441/1441 [==============================] - 1s 616us/step - loss: 2.7372 - mean_absolute_error: 1.3326\n",
      "Epoch 10/50\n",
      "1441/1441 [==============================] - 1s 627us/step - loss: 2.6579 - mean_absolute_error: 1.2911\n",
      "Epoch 11/50\n",
      "1441/1441 [==============================] - 1s 829us/step - loss: 2.5213 - mean_absolute_error: 1.2505\n",
      "Epoch 12/50\n",
      "1441/1441 [==============================] - 1s 825us/step - loss: 2.5098 - mean_absolute_error: 1.2496\n",
      "Epoch 13/50\n",
      "1441/1441 [==============================] - 1s 839us/step - loss: 2.3908 - mean_absolute_error: 1.2389\n",
      "Epoch 14/50\n",
      "1441/1441 [==============================] - 1s 736us/step - loss: 2.5781 - mean_absolute_error: 1.2725\n",
      "Epoch 15/50\n",
      "1441/1441 [==============================] - 1s 698us/step - loss: 2.3616 - mean_absolute_error: 1.2262\n",
      "Epoch 16/50\n",
      "1441/1441 [==============================] - 1s 713us/step - loss: 2.5588 - mean_absolute_error: 1.2844\n",
      "Epoch 17/50\n",
      "1441/1441 [==============================] - 1s 697us/step - loss: 2.3334 - mean_absolute_error: 1.2057\n",
      "Epoch 18/50\n",
      "1441/1441 [==============================] - 1s 641us/step - loss: 2.4913 - mean_absolute_error: 1.2565\n",
      "Epoch 19/50\n",
      "1441/1441 [==============================] - 1s 631us/step - loss: 2.2456 - mean_absolute_error: 1.1786\n",
      "Epoch 20/50\n",
      "1441/1441 [==============================] - 1s 623us/step - loss: 2.3795 - mean_absolute_error: 1.2317\n",
      "Epoch 21/50\n",
      "1441/1441 [==============================] - 1s 613us/step - loss: 2.2916 - mean_absolute_error: 1.2001\n",
      "Epoch 22/50\n",
      "1441/1441 [==============================] - 1s 589us/step - loss: 2.2246 - mean_absolute_error: 1.1891\n",
      "Epoch 23/50\n",
      "1441/1441 [==============================] - 1s 530us/step - loss: 2.1804 - mean_absolute_error: 1.1759\n",
      "Epoch 24/50\n",
      "1441/1441 [==============================] - 1s 533us/step - loss: 2.2113 - mean_absolute_error: 1.1691\n",
      "Epoch 25/50\n",
      "1441/1441 [==============================] - 1s 531us/step - loss: 2.2077 - mean_absolute_error: 1.1759\n",
      "Epoch 26/50\n",
      "1441/1441 [==============================] - 1s 532us/step - loss: 2.2156 - mean_absolute_error: 1.1858\n",
      "Epoch 27/50\n",
      "1441/1441 [==============================] - 1s 544us/step - loss: 2.2137 - mean_absolute_error: 1.1925\n",
      "Epoch 28/50\n",
      "1441/1441 [==============================] - 1s 614us/step - loss: 2.1775 - mean_absolute_error: 1.1673\n",
      "Epoch 29/50\n",
      "1441/1441 [==============================] - 1s 634us/step - loss: 2.1940 - mean_absolute_error: 1.1661\n",
      "Epoch 30/50\n",
      "1441/1441 [==============================] - 1s 616us/step - loss: 2.0465 - mean_absolute_error: 1.1226\n",
      "Epoch 31/50\n",
      "1441/1441 [==============================] - 1s 616us/step - loss: 2.0628 - mean_absolute_error: 1.1238\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441/1441 [==============================] - 1s 538us/step - loss: 2.0287 - mean_absolute_error: 1.1287\n",
      "Epoch 33/50\n",
      "1441/1441 [==============================] - 1s 489us/step - loss: 2.0395 - mean_absolute_error: 1.1211\n",
      "Epoch 34/50\n",
      "1441/1441 [==============================] - 1s 494us/step - loss: 2.0917 - mean_absolute_error: 1.1346\n",
      "Epoch 35/50\n",
      "1441/1441 [==============================] - 1s 495us/step - loss: 2.0116 - mean_absolute_error: 1.1144\n",
      "Epoch 36/50\n",
      "1441/1441 [==============================] - 1s 490us/step - loss: 1.9841 - mean_absolute_error: 1.1193\n",
      "Epoch 37/50\n",
      "1441/1441 [==============================] - 1s 498us/step - loss: 1.9208 - mean_absolute_error: 1.0881\n",
      "Epoch 38/50\n",
      "1441/1441 [==============================] - 1s 574us/step - loss: 1.9294 - mean_absolute_error: 1.0931\n",
      "Epoch 39/50\n",
      "1441/1441 [==============================] - 1s 571us/step - loss: 1.8821 - mean_absolute_error: 1.0718\n",
      "Epoch 40/50\n",
      "1441/1441 [==============================] - 1s 569us/step - loss: 1.9875 - mean_absolute_error: 1.1172\n",
      "Epoch 41/50\n",
      "1441/1441 [==============================] - 1s 570us/step - loss: 1.8186 - mean_absolute_error: 1.0645\n",
      "Epoch 42/50\n",
      "1441/1441 [==============================] - 1s 561us/step - loss: 1.8948 - mean_absolute_error: 1.0888\n",
      "Epoch 43/50\n",
      "1441/1441 [==============================] - 1s 491us/step - loss: 1.9036 - mean_absolute_error: 1.0962\n",
      "Epoch 44/50\n",
      "1441/1441 [==============================] - 1s 490us/step - loss: 1.7652 - mean_absolute_error: 1.0566\n",
      "Epoch 45/50\n",
      "1441/1441 [==============================] - 1s 491us/step - loss: 1.8030 - mean_absolute_error: 1.0443\n",
      "Epoch 46/50\n",
      "1441/1441 [==============================] - 1s 492us/step - loss: 1.7706 - mean_absolute_error: 1.0338\n",
      "Epoch 47/50\n",
      "1441/1441 [==============================] - 1s 494us/step - loss: 1.8432 - mean_absolute_error: 1.0794\n",
      "Epoch 48/50\n",
      "1441/1441 [==============================] - 1s 536us/step - loss: 1.7745 - mean_absolute_error: 1.0411\n",
      "Epoch 49/50\n",
      "1441/1441 [==============================] - 1s 572us/step - loss: 1.5856 - mean_absolute_error: 0.9786\n",
      "Epoch 50/50\n",
      "1441/1441 [==============================] - 1s 578us/step - loss: 1.6240 - mean_absolute_error: 1.0059\n",
      "QWK:  0.7720102731730096\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_59 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 12s 8ms/step - loss: 11.1585 - mean_absolute_error: 2.5373\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 1s 482us/step - loss: 3.3331 - mean_absolute_error: 1.4608\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 1s 478us/step - loss: 3.1956 - mean_absolute_error: 1.4160\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 1s 477us/step - loss: 2.8808 - mean_absolute_error: 1.3724\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 1s 490us/step - loss: 2.7725 - mean_absolute_error: 1.3450\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 1s 577us/step - loss: 2.7285 - mean_absolute_error: 1.3294\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 1s 580us/step - loss: 2.6896 - mean_absolute_error: 1.3130\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 1s 582us/step - loss: 2.6038 - mean_absolute_error: 1.2946\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 1s 579us/step - loss: 2.6952 - mean_absolute_error: 1.3159\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 1s 623us/step - loss: 2.5439 - mean_absolute_error: 1.2935\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 1s 743us/step - loss: 2.5966 - mean_absolute_error: 1.2769\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 1s 753us/step - loss: 2.5115 - mean_absolute_error: 1.2831\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 1s 738us/step - loss: 2.5945 - mean_absolute_error: 1.2880\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 1s 711us/step - loss: 2.6301 - mean_absolute_error: 1.2936\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 1s 645us/step - loss: 2.4580 - mean_absolute_error: 1.2522\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 1s 647us/step - loss: 2.5382 - mean_absolute_error: 1.2742\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 1s 652us/step - loss: 2.3433 - mean_absolute_error: 1.2256\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 1s 635us/step - loss: 2.4539 - mean_absolute_error: 1.2497\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 1s 574us/step - loss: 2.3587 - mean_absolute_error: 1.2166\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 1s 584us/step - loss: 2.4226 - mean_absolute_error: 1.2512\n",
      "Epoch 21/50\n",
      "1442/1442 [==============================] - 1s 583us/step - loss: 2.2060 - mean_absolute_error: 1.1626\n",
      "Epoch 22/50\n",
      "1442/1442 [==============================] - 1s 576us/step - loss: 2.3922 - mean_absolute_error: 1.2379\n",
      "Epoch 23/50\n",
      "1442/1442 [==============================] - 1s 555us/step - loss: 2.2953 - mean_absolute_error: 1.2155\n",
      "Epoch 24/50\n",
      "1442/1442 [==============================] - 1s 503us/step - loss: 2.0815 - mean_absolute_error: 1.1489\n",
      "Epoch 25/50\n",
      "1442/1442 [==============================] - 1s 502us/step - loss: 2.2000 - mean_absolute_error: 1.1850\n",
      "Epoch 26/50\n",
      "1442/1442 [==============================] - 1s 502us/step - loss: 2.0880 - mean_absolute_error: 1.1426\n",
      "Epoch 27/50\n",
      "1442/1442 [==============================] - 1s 503us/step - loss: 2.1908 - mean_absolute_error: 1.1780\n",
      "Epoch 28/50\n",
      "1442/1442 [==============================] - 1s 497us/step - loss: 2.1416 - mean_absolute_error: 1.1674\n",
      "Epoch 29/50\n",
      "1442/1442 [==============================] - 1s 583us/step - loss: 2.0757 - mean_absolute_error: 1.1473\n",
      "Epoch 30/50\n",
      "1442/1442 [==============================] - 1s 585us/step - loss: 2.1053 - mean_absolute_error: 1.1514\n",
      "Epoch 31/50\n",
      "1442/1442 [==============================] - 1s 582us/step - loss: 2.1715 - mean_absolute_error: 1.1762\n",
      "Epoch 32/50\n",
      "1442/1442 [==============================] - 1s 578us/step - loss: 2.0470 - mean_absolute_error: 1.1290\n",
      "Epoch 33/50\n",
      "1442/1442 [==============================] - 1s 566us/step - loss: 1.9685 - mean_absolute_error: 1.1191\n",
      "Epoch 34/50\n",
      "1442/1442 [==============================] - 1s 497us/step - loss: 2.0518 - mean_absolute_error: 1.1351\n",
      "Epoch 35/50\n",
      "1442/1442 [==============================] - 1s 502us/step - loss: 1.9774 - mean_absolute_error: 1.1209\n",
      "Epoch 36/50\n",
      "1442/1442 [==============================] - 1s 509us/step - loss: 1.9470 - mean_absolute_error: 1.0970\n",
      "Epoch 37/50\n",
      "1442/1442 [==============================] - 1s 520us/step - loss: 1.9679 - mean_absolute_error: 1.1250\n",
      "Epoch 38/50\n",
      "1442/1442 [==============================] - 1s 512us/step - loss: 1.9968 - mean_absolute_error: 1.1144\n",
      "Epoch 39/50\n",
      "1442/1442 [==============================] - 1s 586us/step - loss: 1.8995 - mean_absolute_error: 1.0737\n",
      "Epoch 40/50\n",
      "1442/1442 [==============================] - 1s 647us/step - loss: 1.8683 - mean_absolute_error: 1.0790\n",
      "Epoch 41/50\n",
      "1442/1442 [==============================] - 1s 662us/step - loss: 1.8714 - mean_absolute_error: 1.0837\n",
      "Epoch 42/50\n",
      "1442/1442 [==============================] - 1s 628us/step - loss: 1.9127 - mean_absolute_error: 1.0900\n",
      "Epoch 43/50\n",
      "1442/1442 [==============================] - 1s 577us/step - loss: 1.8956 - mean_absolute_error: 1.0970\n",
      "Epoch 44/50\n",
      "1442/1442 [==============================] - 1s 535us/step - loss: 1.8643 - mean_absolute_error: 1.0787\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442/1442 [==============================] - 1s 496us/step - loss: 1.7971 - mean_absolute_error: 1.0599\n",
      "Epoch 46/50\n",
      "1442/1442 [==============================] - 1s 492us/step - loss: 1.7711 - mean_absolute_error: 1.0525\n",
      "Epoch 47/50\n",
      "1442/1442 [==============================] - 1s 487us/step - loss: 1.7910 - mean_absolute_error: 1.0431\n",
      "Epoch 48/50\n",
      "1442/1442 [==============================] - 1s 496us/step - loss: 1.6195 - mean_absolute_error: 1.0037\n",
      "Epoch 49/50\n",
      "1442/1442 [==============================] - 1s 561us/step - loss: 1.7929 - mean_absolute_error: 1.0443\n",
      "Epoch 50/50\n",
      "1442/1442 [==============================] - 1s 565us/step - loss: 1.6993 - mean_absolute_error: 1.0252\n",
      "QWK:  0.7751205961236618\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.7729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_61 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1252/1252 [==============================] - 15s 12ms/step - loss: 11.9348 - mean_absolute_error: 2.7514\n",
      "Epoch 2/50\n",
      "1252/1252 [==============================] - 1s 509us/step - loss: 3.1566 - mean_absolute_error: 1.4053\n",
      "Epoch 3/50\n",
      "1252/1252 [==============================] - 1s 571us/step - loss: 2.7490 - mean_absolute_error: 1.3164\n",
      "Epoch 4/50\n",
      "1252/1252 [==============================] - 1s 581us/step - loss: 2.4159 - mean_absolute_error: 1.2435\n",
      "Epoch 5/50\n",
      "1252/1252 [==============================] - 1s 577us/step - loss: 2.3498 - mean_absolute_error: 1.2200\n",
      "Epoch 6/50\n",
      "1252/1252 [==============================] - 1s 605us/step - loss: 2.3212 - mean_absolute_error: 1.2182\n",
      "Epoch 7/50\n",
      "1252/1252 [==============================] - 1s 580us/step - loss: 2.2288 - mean_absolute_error: 1.1852\n",
      "Epoch 8/50\n",
      "1252/1252 [==============================] - 1s 524us/step - loss: 2.2673 - mean_absolute_error: 1.1914\n",
      "Epoch 9/50\n",
      "1252/1252 [==============================] - 1s 503us/step - loss: 2.1355 - mean_absolute_error: 1.1543\n",
      "Epoch 10/50\n",
      "1252/1252 [==============================] - 1s 493us/step - loss: 2.1290 - mean_absolute_error: 1.1686\n",
      "Epoch 11/50\n",
      "1252/1252 [==============================] - 1s 494us/step - loss: 2.1027 - mean_absolute_error: 1.1554\n",
      "Epoch 12/50\n",
      "1252/1252 [==============================] - 1s 502us/step - loss: 2.0000 - mean_absolute_error: 1.1248\n",
      "Epoch 13/50\n",
      "1252/1252 [==============================] - 1s 502us/step - loss: 1.9621 - mean_absolute_error: 1.1411\n",
      "Epoch 14/50\n",
      "1252/1252 [==============================] - 1s 516us/step - loss: 2.0074 - mean_absolute_error: 1.1422\n",
      "Epoch 15/50\n",
      "1252/1252 [==============================] - 1s 579us/step - loss: 2.1227 - mean_absolute_error: 1.1522\n",
      "Epoch 16/50\n",
      "1252/1252 [==============================] - 1s 575us/step - loss: 1.8953 - mean_absolute_error: 1.0933\n",
      "Epoch 17/50\n",
      "1252/1252 [==============================] - 1s 577us/step - loss: 1.9863 - mean_absolute_error: 1.1197\n",
      "Epoch 18/50\n",
      "1252/1252 [==============================] - 1s 577us/step - loss: 1.8977 - mean_absolute_error: 1.1020\n",
      "Epoch 19/50\n",
      "1252/1252 [==============================] - 1s 576us/step - loss: 1.9046 - mean_absolute_error: 1.0862\n",
      "Epoch 20/50\n",
      "1252/1252 [==============================] - 1s 518us/step - loss: 1.8787 - mean_absolute_error: 1.0898\n",
      "Epoch 21/50\n",
      "1252/1252 [==============================] - 1s 504us/step - loss: 1.8166 - mean_absolute_error: 1.0501\n",
      "Epoch 22/50\n",
      "1252/1252 [==============================] - 1s 498us/step - loss: 1.8699 - mean_absolute_error: 1.0912\n",
      "Epoch 23/50\n",
      "1252/1252 [==============================] - 1s 498us/step - loss: 1.8613 - mean_absolute_error: 1.0812\n",
      "Epoch 24/50\n",
      "1252/1252 [==============================] - 1s 498us/step - loss: 1.6947 - mean_absolute_error: 1.0299\n",
      "Epoch 25/50\n",
      "1252/1252 [==============================] - 1s 495us/step - loss: 1.8280 - mean_absolute_error: 1.0736\n",
      "Epoch 26/50\n",
      "1252/1252 [==============================] - 1s 518us/step - loss: 1.5891 - mean_absolute_error: 0.9905\n",
      "Epoch 27/50\n",
      "1252/1252 [==============================] - 1s 586us/step - loss: 1.7083 - mean_absolute_error: 1.0134\n",
      "Epoch 28/50\n",
      "1252/1252 [==============================] - 1s 580us/step - loss: 1.6756 - mean_absolute_error: 1.0228\n",
      "Epoch 29/50\n",
      "1252/1252 [==============================] - 1s 584us/step - loss: 1.6688 - mean_absolute_error: 1.0276\n",
      "Epoch 30/50\n",
      "1252/1252 [==============================] - 1s 589us/step - loss: 1.6348 - mean_absolute_error: 1.0043\n",
      "Epoch 31/50\n",
      "1252/1252 [==============================] - 1s 577us/step - loss: 1.6928 - mean_absolute_error: 1.0355\n",
      "Epoch 32/50\n",
      "1252/1252 [==============================] - 1s 702us/step - loss: 1.5979 - mean_absolute_error: 0.9985\n",
      "Epoch 33/50\n",
      "1252/1252 [==============================] - 1s 730us/step - loss: 1.6541 - mean_absolute_error: 1.0110\n",
      "Epoch 34/50\n",
      "1252/1252 [==============================] - 1s 753us/step - loss: 1.5187 - mean_absolute_error: 0.9771\n",
      "Epoch 35/50\n",
      "1252/1252 [==============================] - 1s 719us/step - loss: 1.6176 - mean_absolute_error: 1.0012\n",
      "Epoch 36/50\n",
      "1252/1252 [==============================] - 1s 705us/step - loss: 1.5007 - mean_absolute_error: 0.9740\n",
      "Epoch 37/50\n",
      "1252/1252 [==============================] - 1s 656us/step - loss: 1.3998 - mean_absolute_error: 0.9385\n",
      "Epoch 38/50\n",
      "1252/1252 [==============================] - 1s 662us/step - loss: 1.6181 - mean_absolute_error: 0.9943\n",
      "Epoch 39/50\n",
      "1252/1252 [==============================] - 1s 653us/step - loss: 1.4452 - mean_absolute_error: 0.9580\n",
      "Epoch 40/50\n",
      "1252/1252 [==============================] - 1s 653us/step - loss: 1.4397 - mean_absolute_error: 0.9470\n",
      "Epoch 41/50\n",
      "1252/1252 [==============================] - 1s 609us/step - loss: 1.4140 - mean_absolute_error: 0.9448\n",
      "Epoch 42/50\n",
      "1252/1252 [==============================] - 1s 585us/step - loss: 1.3999 - mean_absolute_error: 0.9419\n",
      "Epoch 43/50\n",
      "1252/1252 [==============================] - 1s 582us/step - loss: 1.3857 - mean_absolute_error: 0.9318\n",
      "Epoch 44/50\n",
      "1252/1252 [==============================] - 1s 577us/step - loss: 1.3608 - mean_absolute_error: 0.9148\n",
      "Epoch 45/50\n",
      "1252/1252 [==============================] - 1s 600us/step - loss: 1.3704 - mean_absolute_error: 0.9222\n",
      "Epoch 46/50\n",
      "1252/1252 [==============================] - 1s 570us/step - loss: 1.4052 - mean_absolute_error: 0.9388\n",
      "Epoch 47/50\n",
      "1252/1252 [==============================] - 1s 497us/step - loss: 1.3951 - mean_absolute_error: 0.9373\n",
      "Epoch 48/50\n",
      "1252/1252 [==============================] - 1s 506us/step - loss: 1.4246 - mean_absolute_error: 0.9316\n",
      "Epoch 49/50\n",
      "1252/1252 [==============================] - 1s 507us/step - loss: 1.3115 - mean_absolute_error: 0.8990\n",
      "Epoch 50/50\n",
      "1252/1252 [==============================] - 1s 502us/step - loss: 1.3238 - mean_absolute_error: 0.9073\n",
      "QWK:  0.6380109021179411\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_63 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_64 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1252/1252 [==============================] - 14s 11ms/step - loss: 11.3897 - mean_absolute_error: 2.6768\n",
      "Epoch 2/50\n",
      "1252/1252 [==============================] - 1s 498us/step - loss: 3.1998 - mean_absolute_error: 1.4149\n",
      "Epoch 3/50\n",
      "1252/1252 [==============================] - 1s 492us/step - loss: 2.5456 - mean_absolute_error: 1.2795\n",
      "Epoch 4/50\n",
      "1252/1252 [==============================] - 1s 518us/step - loss: 2.5293 - mean_absolute_error: 1.2506\n",
      "Epoch 5/50\n",
      "1252/1252 [==============================] - 1s 538us/step - loss: 2.4400 - mean_absolute_error: 1.2401\n",
      "Epoch 6/50\n",
      "1252/1252 [==============================] - 1s 523us/step - loss: 2.3258 - mean_absolute_error: 1.2270\n",
      "Epoch 7/50\n",
      "1252/1252 [==============================] - 1s 507us/step - loss: 2.2963 - mean_absolute_error: 1.1893\n",
      "Epoch 8/50\n",
      "1252/1252 [==============================] - 1s 522us/step - loss: 2.2259 - mean_absolute_error: 1.1880\n",
      "Epoch 9/50\n",
      "1252/1252 [==============================] - 1s 549us/step - loss: 2.1985 - mean_absolute_error: 1.1800\n",
      "Epoch 10/50\n",
      "1252/1252 [==============================] - 1s 540us/step - loss: 2.1217 - mean_absolute_error: 1.1600\n",
      "Epoch 11/50\n",
      "1252/1252 [==============================] - 1s 640us/step - loss: 1.9767 - mean_absolute_error: 1.1321\n",
      "Epoch 12/50\n",
      "1252/1252 [==============================] - 1s 707us/step - loss: 1.9726 - mean_absolute_error: 1.1194\n",
      "Epoch 13/50\n",
      "1252/1252 [==============================] - 1s 683us/step - loss: 1.9867 - mean_absolute_error: 1.1314\n",
      "Epoch 14/50\n",
      "1252/1252 [==============================] - 1s 697us/step - loss: 1.9982 - mean_absolute_error: 1.1269\n",
      "Epoch 15/50\n",
      "1252/1252 [==============================] - 1s 679us/step - loss: 2.0212 - mean_absolute_error: 1.1319\n",
      "Epoch 16/50\n",
      "1252/1252 [==============================] - 1s 627us/step - loss: 1.9537 - mean_absolute_error: 1.1154\n",
      "Epoch 17/50\n",
      "1252/1252 [==============================] - 1s 606us/step - loss: 1.8502 - mean_absolute_error: 1.0776\n",
      "Epoch 18/50\n",
      "1252/1252 [==============================] - 1s 618us/step - loss: 1.7485 - mean_absolute_error: 1.0382\n",
      "Epoch 19/50\n",
      "1252/1252 [==============================] - 1s 611us/step - loss: 1.8376 - mean_absolute_error: 1.0744\n",
      "Epoch 20/50\n",
      "1252/1252 [==============================] - 1s 608us/step - loss: 1.8407 - mean_absolute_error: 1.0868\n",
      "Epoch 21/50\n",
      "1252/1252 [==============================] - 1s 547us/step - loss: 1.7131 - mean_absolute_error: 1.0465\n",
      "Epoch 22/50\n",
      "1252/1252 [==============================] - 1s 550us/step - loss: 1.9449 - mean_absolute_error: 1.1132\n",
      "Epoch 23/50\n",
      "1252/1252 [==============================] - 1s 533us/step - loss: 1.6523 - mean_absolute_error: 1.0287\n",
      "Epoch 24/50\n",
      "1252/1252 [==============================] - 1s 536us/step - loss: 1.7386 - mean_absolute_error: 1.0587\n",
      "Epoch 25/50\n",
      "1252/1252 [==============================] - 1s 543us/step - loss: 1.6921 - mean_absolute_error: 1.0348\n",
      "Epoch 26/50\n",
      "1252/1252 [==============================] - 1s 529us/step - loss: 1.7755 - mean_absolute_error: 1.0623\n",
      "Epoch 27/50\n",
      "1252/1252 [==============================] - 1s 532us/step - loss: 1.6374 - mean_absolute_error: 1.0256\n",
      "Epoch 28/50\n",
      "1252/1252 [==============================] - 1s 565us/step - loss: 1.5691 - mean_absolute_error: 0.9930\n",
      "Epoch 29/50\n",
      "1252/1252 [==============================] - 1s 614us/step - loss: 1.7325 - mean_absolute_error: 1.0628\n",
      "Epoch 30/50\n",
      "1252/1252 [==============================] - 1s 617us/step - loss: 1.5978 - mean_absolute_error: 1.0021\n",
      "Epoch 31/50\n",
      "1252/1252 [==============================] - 1s 632us/step - loss: 1.5717 - mean_absolute_error: 0.9955\n",
      "Epoch 32/50\n",
      "1252/1252 [==============================] - 1s 618us/step - loss: 1.5908 - mean_absolute_error: 1.0018\n",
      "Epoch 33/50\n",
      "1252/1252 [==============================] - 1s 603us/step - loss: 1.4898 - mean_absolute_error: 0.9697\n",
      "Epoch 34/50\n",
      "1252/1252 [==============================] - 1s 534us/step - loss: 1.5260 - mean_absolute_error: 0.9966\n",
      "Epoch 35/50\n",
      "1252/1252 [==============================] - 1s 534us/step - loss: 1.4500 - mean_absolute_error: 0.9555\n",
      "Epoch 36/50\n",
      "1252/1252 [==============================] - 1s 539us/step - loss: 1.4316 - mean_absolute_error: 0.9502\n",
      "Epoch 37/50\n",
      "1252/1252 [==============================] - 1s 534us/step - loss: 1.5468 - mean_absolute_error: 0.9819\n",
      "Epoch 38/50\n",
      "1252/1252 [==============================] - 1s 533us/step - loss: 1.4090 - mean_absolute_error: 0.9461\n",
      "Epoch 39/50\n",
      "1252/1252 [==============================] - 1s 542us/step - loss: 1.4125 - mean_absolute_error: 0.9409\n",
      "Epoch 40/50\n",
      "1252/1252 [==============================] - 1s 624us/step - loss: 1.3466 - mean_absolute_error: 0.9223\n",
      "Epoch 41/50\n",
      "1252/1252 [==============================] - 1s 618us/step - loss: 1.2935 - mean_absolute_error: 0.9133\n",
      "Epoch 42/50\n",
      "1252/1252 [==============================] - 1s 611us/step - loss: 1.4449 - mean_absolute_error: 0.9573\n",
      "Epoch 43/50\n",
      "1252/1252 [==============================] - 1s 610us/step - loss: 1.3961 - mean_absolute_error: 0.9222\n",
      "Epoch 44/50\n",
      "1252/1252 [==============================] - 1s 584us/step - loss: 1.2947 - mean_absolute_error: 0.9062\n",
      "Epoch 45/50\n",
      "1252/1252 [==============================] - 1s 548us/step - loss: 1.3468 - mean_absolute_error: 0.9172\n",
      "Epoch 46/50\n",
      "1252/1252 [==============================] - 1s 540us/step - loss: 1.3246 - mean_absolute_error: 0.9103\n",
      "Epoch 47/50\n",
      "1252/1252 [==============================] - 1s 541us/step - loss: 1.4192 - mean_absolute_error: 0.9257\n",
      "Epoch 48/50\n",
      "1252/1252 [==============================] - 1s 537us/step - loss: 1.2811 - mean_absolute_error: 0.8745\n",
      "Epoch 49/50\n",
      "1252/1252 [==============================] - 1s 547us/step - loss: 1.3028 - mean_absolute_error: 0.9066\n",
      "Epoch 50/50\n",
      "1252/1252 [==============================] - 1s 547us/step - loss: 1.2495 - mean_absolute_error: 0.8912\n",
      "QWK:  0.6618362172870977\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_65 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1256/1256 [==============================] - 12s 10ms/step - loss: 12.3798 - mean_absolute_error: 2.8190\n",
      "Epoch 2/50\n",
      "1256/1256 [==============================] - 1s 591us/step - loss: 3.2512 - mean_absolute_error: 1.4270\n",
      "Epoch 3/50\n",
      "1256/1256 [==============================] - 1s 641us/step - loss: 2.6834 - mean_absolute_error: 1.3091\n",
      "Epoch 4/50\n",
      "1256/1256 [==============================] - 1s 624us/step - loss: 2.7072 - mean_absolute_error: 1.3143\n",
      "Epoch 5/50\n",
      "1256/1256 [==============================] - 1s 643us/step - loss: 2.5175 - mean_absolute_error: 1.2697\n",
      "Epoch 6/50\n",
      "1256/1256 [==============================] - 1s 646us/step - loss: 2.2216 - mean_absolute_error: 1.1628\n",
      "Epoch 7/50\n",
      "1256/1256 [==============================] - 1s 815us/step - loss: 2.2903 - mean_absolute_error: 1.2051\n",
      "Epoch 8/50\n",
      "1256/1256 [==============================] - 1s 873us/step - loss: 2.1180 - mean_absolute_error: 1.1482\n",
      "Epoch 9/50\n",
      "1256/1256 [==============================] - 1s 871us/step - loss: 2.0434 - mean_absolute_error: 1.1431\n",
      "Epoch 10/50\n",
      "1256/1256 [==============================] - 1s 840us/step - loss: 2.1567 - mean_absolute_error: 1.1762\n",
      "Epoch 11/50\n",
      "1256/1256 [==============================] - 1s 768us/step - loss: 2.0705 - mean_absolute_error: 1.1495 0s - loss: 1.7890 - mean_absolute_er\n",
      "Epoch 12/50\n",
      "1256/1256 [==============================] - 1s 729us/step - loss: 2.0278 - mean_absolute_error: 1.1258\n",
      "Epoch 13/50\n",
      "1256/1256 [==============================] - 1s 689us/step - loss: 1.9767 - mean_absolute_error: 1.1273\n",
      "Epoch 14/50\n",
      "1256/1256 [==============================] - 1s 728us/step - loss: 1.9713 - mean_absolute_error: 1.0989\n",
      "Epoch 15/50\n",
      "1256/1256 [==============================] - 1s 694us/step - loss: 1.9248 - mean_absolute_error: 1.0837\n",
      "Epoch 16/50\n",
      "1256/1256 [==============================] - 1s 651us/step - loss: 1.9580 - mean_absolute_error: 1.1196\n",
      "Epoch 17/50\n",
      "1256/1256 [==============================] - 1s 680us/step - loss: 1.8293 - mean_absolute_error: 1.0703\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256/1256 [==============================] - 1s 602us/step - loss: 1.7718 - mean_absolute_error: 1.0560\n",
      "Epoch 19/50\n",
      "1256/1256 [==============================] - 1s 586us/step - loss: 1.8326 - mean_absolute_error: 1.0683\n",
      "Epoch 20/50\n",
      "1256/1256 [==============================] - 1s 540us/step - loss: 1.8941 - mean_absolute_error: 1.0895\n",
      "Epoch 21/50\n",
      "1256/1256 [==============================] - 1s 496us/step - loss: 1.7608 - mean_absolute_error: 1.0557\n",
      "Epoch 22/50\n",
      "1256/1256 [==============================] - 1s 512us/step - loss: 1.7740 - mean_absolute_error: 1.0608\n",
      "Epoch 23/50\n",
      "1256/1256 [==============================] - 1s 502us/step - loss: 1.7876 - mean_absolute_error: 1.0721\n",
      "Epoch 24/50\n",
      "1256/1256 [==============================] - 1s 492us/step - loss: 1.8208 - mean_absolute_error: 1.0590\n",
      "Epoch 25/50\n",
      "1256/1256 [==============================] - 1s 511us/step - loss: 1.7284 - mean_absolute_error: 1.0397\n",
      "Epoch 26/50\n",
      "1256/1256 [==============================] - 1s 505us/step - loss: 1.7010 - mean_absolute_error: 1.0234\n",
      "Epoch 27/50\n",
      "1256/1256 [==============================] - 1s 573us/step - loss: 1.6784 - mean_absolute_error: 1.0271\n",
      "Epoch 28/50\n",
      "1256/1256 [==============================] - 1s 580us/step - loss: 1.6434 - mean_absolute_error: 1.0256\n",
      "Epoch 29/50\n",
      "1256/1256 [==============================] - 1s 580us/step - loss: 1.5675 - mean_absolute_error: 0.9858\n",
      "Epoch 30/50\n",
      "1256/1256 [==============================] - 1s 574us/step - loss: 1.6681 - mean_absolute_error: 1.0249\n",
      "Epoch 31/50\n",
      "1256/1256 [==============================] - 1s 577us/step - loss: 1.5849 - mean_absolute_error: 1.0088\n",
      "Epoch 32/50\n",
      "1256/1256 [==============================] - 1s 532us/step - loss: 1.5832 - mean_absolute_error: 0.9989\n",
      "Epoch 33/50\n",
      "1256/1256 [==============================] - 1s 506us/step - loss: 1.6731 - mean_absolute_error: 1.0310\n",
      "Epoch 34/50\n",
      "1256/1256 [==============================] - 1s 500us/step - loss: 1.5469 - mean_absolute_error: 0.9910\n",
      "Epoch 35/50\n",
      "1256/1256 [==============================] - 1s 497us/step - loss: 1.5417 - mean_absolute_error: 0.9678\n",
      "Epoch 36/50\n",
      "1256/1256 [==============================] - 1s 493us/step - loss: 1.4570 - mean_absolute_error: 0.9535\n",
      "Epoch 37/50\n",
      "1256/1256 [==============================] - 1s 496us/step - loss: 1.4564 - mean_absolute_error: 0.9440\n",
      "Epoch 38/50\n",
      "1256/1256 [==============================] - 1s 519us/step - loss: 1.5637 - mean_absolute_error: 0.9887\n",
      "Epoch 39/50\n",
      "1256/1256 [==============================] - 1s 573us/step - loss: 1.5531 - mean_absolute_error: 0.9880\n",
      "Epoch 40/50\n",
      "1256/1256 [==============================] - 1s 581us/step - loss: 1.3973 - mean_absolute_error: 0.9269\n",
      "Epoch 41/50\n",
      "1256/1256 [==============================] - 1s 580us/step - loss: 1.3916 - mean_absolute_error: 0.9424\n",
      "Epoch 42/50\n",
      "1256/1256 [==============================] - 1s 575us/step - loss: 1.4395 - mean_absolute_error: 0.9470\n",
      "Epoch 43/50\n",
      "1256/1256 [==============================] - 1s 583us/step - loss: 1.3847 - mean_absolute_error: 0.9330\n",
      "Epoch 44/50\n",
      "1256/1256 [==============================] - 1s 681us/step - loss: 1.3786 - mean_absolute_error: 0.9227\n",
      "Epoch 45/50\n",
      "1256/1256 [==============================] - 1s 728us/step - loss: 1.3853 - mean_absolute_error: 0.9322\n",
      "Epoch 46/50\n",
      "1256/1256 [==============================] - 1s 732us/step - loss: 1.3637 - mean_absolute_error: 0.9212\n",
      "Epoch 47/50\n",
      "1256/1256 [==============================] - 1s 749us/step - loss: 1.2835 - mean_absolute_error: 0.8963\n",
      "Epoch 48/50\n",
      "1256/1256 [==============================] - 1s 687us/step - loss: 1.2607 - mean_absolute_error: 0.8949\n",
      "Epoch 49/50\n",
      "1256/1256 [==============================] - 1s 648us/step - loss: 1.2731 - mean_absolute_error: 0.8881\n",
      "Epoch 50/50\n",
      "1256/1256 [==============================] - 1s 658us/step - loss: 1.3360 - mean_absolute_error: 0.9136\n",
      "QWK:  0.6305055061989768\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_67 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1256/1256 [==============================] - 13s 11ms/step - loss: 12.4852 - mean_absolute_error: 2.8425\n",
      "Epoch 2/50\n",
      "1256/1256 [==============================] - 1s 512us/step - loss: 3.1334 - mean_absolute_error: 1.4060\n",
      "Epoch 3/50\n",
      "1256/1256 [==============================] - 1s 515us/step - loss: 2.6919 - mean_absolute_error: 1.2977\n",
      "Epoch 4/50\n",
      "1256/1256 [==============================] - 1s 510us/step - loss: 2.7026 - mean_absolute_error: 1.3064\n",
      "Epoch 5/50\n",
      "1256/1256 [==============================] - 1s 504us/step - loss: 2.3703 - mean_absolute_error: 1.2227\n",
      "Epoch 6/50\n",
      "1256/1256 [==============================] - 1s 511us/step - loss: 2.2375 - mean_absolute_error: 1.1957\n",
      "Epoch 7/50\n",
      "1256/1256 [==============================] - 1s 620us/step - loss: 2.2344 - mean_absolute_error: 1.1959\n",
      "Epoch 8/50\n",
      "1256/1256 [==============================] - 1s 625us/step - loss: 2.1247 - mean_absolute_error: 1.1518\n",
      "Epoch 9/50\n",
      "1256/1256 [==============================] - 1s 613us/step - loss: 2.1243 - mean_absolute_error: 1.1479\n",
      "Epoch 10/50\n",
      "1256/1256 [==============================] - 1s 615us/step - loss: 2.1316 - mean_absolute_error: 1.1666\n",
      "Epoch 11/50\n",
      "1256/1256 [==============================] - 1s 620us/step - loss: 2.0289 - mean_absolute_error: 1.1295\n",
      "Epoch 12/50\n",
      "1256/1256 [==============================] - 1s 805us/step - loss: 2.0051 - mean_absolute_error: 1.1394\n",
      "Epoch 13/50\n",
      "1256/1256 [==============================] - 1s 836us/step - loss: 2.0363 - mean_absolute_error: 1.1376\n",
      "Epoch 14/50\n",
      "1256/1256 [==============================] - 1s 845us/step - loss: 1.9678 - mean_absolute_error: 1.1068\n",
      "Epoch 15/50\n",
      "1256/1256 [==============================] - 1s 846us/step - loss: 1.9643 - mean_absolute_error: 1.1110\n",
      "Epoch 16/50\n",
      "1256/1256 [==============================] - 1s 707us/step - loss: 1.9536 - mean_absolute_error: 1.1062\n",
      "Epoch 17/50\n",
      "1256/1256 [==============================] - 1s 694us/step - loss: 1.9079 - mean_absolute_error: 1.0909\n",
      "Epoch 18/50\n",
      "1256/1256 [==============================] - 1s 694us/step - loss: 1.9210 - mean_absolute_error: 1.1125\n",
      "Epoch 19/50\n",
      "1256/1256 [==============================] - 1s 697us/step - loss: 1.8976 - mean_absolute_error: 1.1114\n",
      "Epoch 20/50\n",
      "1256/1256 [==============================] - 1s 652us/step - loss: 1.9007 - mean_absolute_error: 1.0855\n",
      "Epoch 21/50\n",
      "1256/1256 [==============================] - 1s 620us/step - loss: 1.7595 - mean_absolute_error: 1.0626\n",
      "Epoch 22/50\n",
      "1256/1256 [==============================] - 1s 620us/step - loss: 1.7034 - mean_absolute_error: 1.0330\n",
      "Epoch 23/50\n",
      "1256/1256 [==============================] - 1s 617us/step - loss: 1.7875 - mean_absolute_error: 1.0483\n",
      "Epoch 24/50\n",
      "1256/1256 [==============================] - 1s 626us/step - loss: 1.6933 - mean_absolute_error: 1.0366\n",
      "Epoch 25/50\n",
      "1256/1256 [==============================] - 1s 651us/step - loss: 1.7490 - mean_absolute_error: 1.0620\n",
      "Epoch 26/50\n",
      "1256/1256 [==============================] - 1s 707us/step - loss: 1.7307 - mean_absolute_error: 1.0432\n",
      "Epoch 27/50\n",
      "1256/1256 [==============================] - 1s 700us/step - loss: 1.7782 - mean_absolute_error: 1.0678\n",
      "Epoch 28/50\n",
      "1256/1256 [==============================] - 1s 696us/step - loss: 1.6689 - mean_absolute_error: 1.0118\n",
      "Epoch 29/50\n",
      "1256/1256 [==============================] - 1s 745us/step - loss: 1.6966 - mean_absolute_error: 1.0481\n",
      "Epoch 30/50\n",
      "1256/1256 [==============================] - 1s 637us/step - loss: 1.5833 - mean_absolute_error: 1.0071\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256/1256 [==============================] - 1s 583us/step - loss: 1.6674 - mean_absolute_error: 1.0130\n",
      "Epoch 32/50\n",
      "1256/1256 [==============================] - 1s 582us/step - loss: 1.5885 - mean_absolute_error: 1.0010\n",
      "Epoch 33/50\n",
      "1256/1256 [==============================] - 1s 574us/step - loss: 1.6361 - mean_absolute_error: 1.0073\n",
      "Epoch 34/50\n",
      "1256/1256 [==============================] - 1s 586us/step - loss: 1.5114 - mean_absolute_error: 0.9756\n",
      "Epoch 35/50\n",
      "1256/1256 [==============================] - 1s 603us/step - loss: 1.5745 - mean_absolute_error: 0.9967\n",
      "Epoch 36/50\n",
      "1256/1256 [==============================] - 1s 637us/step - loss: 1.4860 - mean_absolute_error: 0.9582\n",
      "Epoch 37/50\n",
      "1256/1256 [==============================] - 1s 646us/step - loss: 1.5065 - mean_absolute_error: 0.9544\n",
      "Epoch 38/50\n",
      "1256/1256 [==============================] - 1s 646us/step - loss: 1.5151 - mean_absolute_error: 0.9883\n",
      "Epoch 39/50\n",
      "1256/1256 [==============================] - 1s 645us/step - loss: 1.5322 - mean_absolute_error: 0.9714\n",
      "Epoch 40/50\n",
      "1256/1256 [==============================] - 1s 600us/step - loss: 1.5198 - mean_absolute_error: 0.9751\n",
      "Epoch 41/50\n",
      "1256/1256 [==============================] - 1s 578us/step - loss: 1.4173 - mean_absolute_error: 0.9436\n",
      "Epoch 42/50\n",
      "1256/1256 [==============================] - 1s 572us/step - loss: 1.4645 - mean_absolute_error: 0.9556\n",
      "Epoch 43/50\n",
      "1256/1256 [==============================] - 1s 571us/step - loss: 1.3975 - mean_absolute_error: 0.9435\n",
      "Epoch 44/50\n",
      "1256/1256 [==============================] - 1s 572us/step - loss: 1.2932 - mean_absolute_error: 0.8923\n",
      "Epoch 45/50\n",
      "1256/1256 [==============================] - 1s 582us/step - loss: 1.4014 - mean_absolute_error: 0.9413\n",
      "Epoch 46/50\n",
      "1256/1256 [==============================] - 1s 499us/step - loss: 1.3731 - mean_absolute_error: 0.9210\n",
      "Epoch 47/50\n",
      "1256/1256 [==============================] - 1s 484us/step - loss: 1.2568 - mean_absolute_error: 0.8828\n",
      "Epoch 48/50\n",
      "1256/1256 [==============================] - 1s 491us/step - loss: 1.3711 - mean_absolute_error: 0.9222\n",
      "Epoch 49/50\n",
      "1256/1256 [==============================] - 1s 490us/step - loss: 1.3475 - mean_absolute_error: 0.9210\n",
      "Epoch 50/50\n",
      "1256/1256 [==============================] - 1s 498us/step - loss: 1.3278 - mean_absolute_error: 0.9090\n",
      "QWK:  0.6722860295740455\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_69 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_70 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1260/1260 [==============================] - 14s 11ms/step - loss: 12.5559 - mean_absolute_error: 2.8132\n",
      "Epoch 2/50\n",
      "1260/1260 [==============================] - 1s 506us/step - loss: 3.1316 - mean_absolute_error: 1.4158\n",
      "Epoch 3/50\n",
      "1260/1260 [==============================] - 1s 512us/step - loss: 2.7796 - mean_absolute_error: 1.3394\n",
      "Epoch 4/50\n",
      "1260/1260 [==============================] - 1s 517us/step - loss: 2.7058 - mean_absolute_error: 1.2999\n",
      "Epoch 5/50\n",
      "1260/1260 [==============================] - 1s 512us/step - loss: 2.3624 - mean_absolute_error: 1.2167\n",
      "Epoch 6/50\n",
      "1260/1260 [==============================] - 1s 512us/step - loss: 2.5008 - mean_absolute_error: 1.2400\n",
      "Epoch 7/50\n",
      "1260/1260 [==============================] - 1s 514us/step - loss: 2.2108 - mean_absolute_error: 1.1997\n",
      "Epoch 8/50\n",
      "1260/1260 [==============================] - 1s 516us/step - loss: 2.1101 - mean_absolute_error: 1.1794\n",
      "Epoch 9/50\n",
      "1260/1260 [==============================] - 1s 522us/step - loss: 2.0842 - mean_absolute_error: 1.1553\n",
      "Epoch 10/50\n",
      "1260/1260 [==============================] - 1s 558us/step - loss: 2.3025 - mean_absolute_error: 1.2190\n",
      "Epoch 11/50\n",
      "1260/1260 [==============================] - 1s 615us/step - loss: 2.1576 - mean_absolute_error: 1.1607\n",
      "Epoch 12/50\n",
      "1260/1260 [==============================] - 1s 629us/step - loss: 2.1007 - mean_absolute_error: 1.1393\n",
      "Epoch 13/50\n",
      "1260/1260 [==============================] - 1s 615us/step - loss: 2.0075 - mean_absolute_error: 1.1126\n",
      "Epoch 14/50\n",
      "1260/1260 [==============================] - 1s 615us/step - loss: 1.9674 - mean_absolute_error: 1.1080\n",
      "Epoch 15/50\n",
      "1260/1260 [==============================] - 1s 657us/step - loss: 1.9960 - mean_absolute_error: 1.1174\n",
      "Epoch 16/50\n",
      "1260/1260 [==============================] - 1s 832us/step - loss: 1.9609 - mean_absolute_error: 1.1095\n",
      "Epoch 17/50\n",
      "1260/1260 [==============================] - 1s 836us/step - loss: 1.8813 - mean_absolute_error: 1.1012\n",
      "Epoch 18/50\n",
      "1260/1260 [==============================] - 1s 850us/step - loss: 1.9759 - mean_absolute_error: 1.1203\n",
      "Epoch 19/50\n",
      "1260/1260 [==============================] - 1s 789us/step - loss: 1.8807 - mean_absolute_error: 1.0953\n",
      "Epoch 20/50\n",
      "1260/1260 [==============================] - 1s 700us/step - loss: 1.9558 - mean_absolute_error: 1.0964\n",
      "Epoch 21/50\n",
      "1260/1260 [==============================] - 1s 699us/step - loss: 1.8522 - mean_absolute_error: 1.0797\n",
      "Epoch 22/50\n",
      "1260/1260 [==============================] - 1s 687us/step - loss: 1.8575 - mean_absolute_error: 1.0697\n",
      "Epoch 23/50\n",
      "1260/1260 [==============================] - 1s 726us/step - loss: 1.9347 - mean_absolute_error: 1.0997\n",
      "Epoch 24/50\n",
      "1260/1260 [==============================] - 1s 650us/step - loss: 1.8332 - mean_absolute_error: 1.0870\n",
      "Epoch 25/50\n",
      "1260/1260 [==============================] - 1s 631us/step - loss: 1.7904 - mean_absolute_error: 1.0626\n",
      "Epoch 26/50\n",
      "1260/1260 [==============================] - 1s 620us/step - loss: 1.7813 - mean_absolute_error: 1.0466\n",
      "Epoch 27/50\n",
      "1260/1260 [==============================] - 1s 628us/step - loss: 1.7266 - mean_absolute_error: 1.0472\n",
      "Epoch 28/50\n",
      "1260/1260 [==============================] - 1s 620us/step - loss: 1.6988 - mean_absolute_error: 1.0191\n",
      "Epoch 29/50\n",
      "1260/1260 [==============================] - 1s 678us/step - loss: 1.6523 - mean_absolute_error: 1.0294\n",
      "Epoch 30/50\n",
      "1260/1260 [==============================] - 1s 724us/step - loss: 1.6075 - mean_absolute_error: 0.9948\n",
      "Epoch 31/50\n",
      "1260/1260 [==============================] - 1s 705us/step - loss: 1.8070 - mean_absolute_error: 1.0767\n",
      "Epoch 32/50\n",
      "1260/1260 [==============================] - 1s 708us/step - loss: 1.6014 - mean_absolute_error: 1.0100\n",
      "Epoch 33/50\n",
      "1260/1260 [==============================] - 1s 663us/step - loss: 1.6497 - mean_absolute_error: 1.0318\n",
      "Epoch 34/50\n",
      "1260/1260 [==============================] - 1s 633us/step - loss: 1.5443 - mean_absolute_error: 0.9691\n",
      "Epoch 35/50\n",
      "1260/1260 [==============================] - 1s 637us/step - loss: 1.5486 - mean_absolute_error: 0.9845\n",
      "Epoch 36/50\n",
      "1260/1260 [==============================] - 1s 622us/step - loss: 1.6009 - mean_absolute_error: 0.9996\n",
      "Epoch 37/50\n",
      "1260/1260 [==============================] - 1s 616us/step - loss: 1.5349 - mean_absolute_error: 0.9673\n",
      "Epoch 38/50\n",
      "1260/1260 [==============================] - 1s 651us/step - loss: 1.5219 - mean_absolute_error: 0.9773\n",
      "Epoch 39/50\n",
      "1260/1260 [==============================] - 1s 697us/step - loss: 1.4825 - mean_absolute_error: 0.9628\n",
      "Epoch 40/50\n",
      "1260/1260 [==============================] - 1s 700us/step - loss: 1.5051 - mean_absolute_error: 0.9739\n",
      "Epoch 41/50\n",
      "1260/1260 [==============================] - 1s 696us/step - loss: 1.3699 - mean_absolute_error: 0.9255\n",
      "Epoch 42/50\n",
      "1260/1260 [==============================] - 1s 709us/step - loss: 1.5571 - mean_absolute_error: 0.9939\n",
      "Epoch 43/50\n",
      "1260/1260 [==============================] - 1s 1ms/step - loss: 1.3342 - mean_absolute_error: 0.9141\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260/1260 [==============================] - 1s 950us/step - loss: 1.4191 - mean_absolute_error: 0.9384\n",
      "Epoch 45/50\n",
      "1260/1260 [==============================] - 1s 956us/step - loss: 1.4345 - mean_absolute_error: 0.9491\n",
      "Epoch 46/50\n",
      "1260/1260 [==============================] - 1s 873us/step - loss: 1.4582 - mean_absolute_error: 0.9442\n",
      "Epoch 47/50\n",
      "1260/1260 [==============================] - 1s 723us/step - loss: 1.3549 - mean_absolute_error: 0.9106\n",
      "Epoch 48/50\n",
      "1260/1260 [==============================] - 1s 700us/step - loss: 1.4055 - mean_absolute_error: 0.9361\n",
      "Epoch 49/50\n",
      "1260/1260 [==============================] - 1s 722us/step - loss: 1.3096 - mean_absolute_error: 0.8982\n",
      "Epoch 50/50\n",
      "1260/1260 [==============================] - 1s 712us/step - loss: 1.3302 - mean_absolute_error: 0.9030\n",
      "QWK:  0.68662718858555\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.6579\n",
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/nikshubha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_71 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_72 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "577/577 [==============================] - 14s 24ms/step - loss: 12.0938 - mean_absolute_error: 2.8611\n",
      "Epoch 2/50\n",
      "577/577 [==============================] - 0s 522us/step - loss: 1.8653 - mean_absolute_error: 1.0896\n",
      "Epoch 3/50\n",
      "577/577 [==============================] - 0s 531us/step - loss: 1.5154 - mean_absolute_error: 0.9758\n",
      "Epoch 4/50\n",
      "577/577 [==============================] - 0s 529us/step - loss: 1.4967 - mean_absolute_error: 0.9760\n",
      "Epoch 5/50\n",
      "577/577 [==============================] - 0s 543us/step - loss: 1.3411 - mean_absolute_error: 0.9039\n",
      "Epoch 6/50\n",
      "577/577 [==============================] - 0s 641us/step - loss: 1.1975 - mean_absolute_error: 0.8550\n",
      "Epoch 7/50\n",
      "577/577 [==============================] - 0s 638us/step - loss: 1.3110 - mean_absolute_error: 0.9148\n",
      "Epoch 8/50\n",
      "577/577 [==============================] - 0s 631us/step - loss: 1.1012 - mean_absolute_error: 0.8304\n",
      "Epoch 9/50\n",
      "577/577 [==============================] - 0s 627us/step - loss: 1.0219 - mean_absolute_error: 0.8056\n",
      "Epoch 10/50\n",
      "577/577 [==============================] - 0s 627us/step - loss: 1.1192 - mean_absolute_error: 0.8339\n",
      "Epoch 11/50\n",
      "577/577 [==============================] - 0s 629us/step - loss: 1.0714 - mean_absolute_error: 0.8185\n",
      "Epoch 12/50\n",
      "577/577 [==============================] - 0s 619us/step - loss: 1.0763 - mean_absolute_error: 0.8248\n",
      "Epoch 13/50\n",
      "577/577 [==============================] - 0s 620us/step - loss: 1.0656 - mean_absolute_error: 0.8120\n",
      "Epoch 14/50\n",
      "577/577 [==============================] - 0s 640us/step - loss: 0.9881 - mean_absolute_error: 0.7962\n",
      "Epoch 15/50\n",
      "577/577 [==============================] - 0s 628us/step - loss: 1.0074 - mean_absolute_error: 0.7815\n",
      "Epoch 16/50\n",
      "577/577 [==============================] - 0s 657us/step - loss: 1.0951 - mean_absolute_error: 0.8115\n",
      "Epoch 17/50\n",
      "577/577 [==============================] - 0s 776us/step - loss: 0.9629 - mean_absolute_error: 0.7780\n",
      "Epoch 18/50\n",
      "577/577 [==============================] - 0s 795us/step - loss: 1.0604 - mean_absolute_error: 0.8239\n",
      "Epoch 19/50\n",
      "577/577 [==============================] - 0s 811us/step - loss: 1.0283 - mean_absolute_error: 0.7949\n",
      "Epoch 20/50\n",
      "577/577 [==============================] - 0s 808us/step - loss: 1.0795 - mean_absolute_error: 0.8444\n",
      "Epoch 21/50\n",
      "577/577 [==============================] - 0s 798us/step - loss: 0.9087 - mean_absolute_error: 0.7652\n",
      "Epoch 22/50\n",
      "577/577 [==============================] - 0s 786us/step - loss: 0.9185 - mean_absolute_error: 0.7471\n",
      "Epoch 23/50\n",
      "577/577 [==============================] - 0s 772us/step - loss: 1.0706 - mean_absolute_error: 0.8195\n",
      "Epoch 24/50\n",
      "577/577 [==============================] - 0s 797us/step - loss: 0.9446 - mean_absolute_error: 0.7637\n",
      "Epoch 25/50\n",
      "577/577 [==============================] - 0s 734us/step - loss: 0.9177 - mean_absolute_error: 0.7565\n",
      "Epoch 26/50\n",
      "577/577 [==============================] - 0s 695us/step - loss: 0.8375 - mean_absolute_error: 0.7300\n",
      "Epoch 27/50\n",
      "577/577 [==============================] - 0s 721us/step - loss: 0.8386 - mean_absolute_error: 0.7162\n",
      "Epoch 28/50\n",
      "577/577 [==============================] - 0s 703us/step - loss: 0.9614 - mean_absolute_error: 0.7706\n",
      "Epoch 29/50\n",
      "577/577 [==============================] - 0s 726us/step - loss: 0.8676 - mean_absolute_error: 0.7374\n",
      "Epoch 30/50\n",
      "577/577 [==============================] - 0s 711us/step - loss: 0.8271 - mean_absolute_error: 0.7193\n",
      "Epoch 31/50\n",
      "577/577 [==============================] - 0s 710us/step - loss: 0.8314 - mean_absolute_error: 0.7284\n",
      "Epoch 32/50\n",
      "577/577 [==============================] - 0s 740us/step - loss: 0.7863 - mean_absolute_error: 0.7059\n",
      "Epoch 33/50\n",
      "577/577 [==============================] - 0s 708us/step - loss: 0.9283 - mean_absolute_error: 0.7413\n",
      "Epoch 34/50\n",
      "577/577 [==============================] - 0s 694us/step - loss: 0.9109 - mean_absolute_error: 0.7503\n",
      "Epoch 35/50\n",
      "577/577 [==============================] - 0s 658us/step - loss: 0.9106 - mean_absolute_error: 0.7519\n",
      "Epoch 36/50\n",
      "577/577 [==============================] - 0s 629us/step - loss: 0.7248 - mean_absolute_error: 0.6721\n",
      "Epoch 37/50\n",
      "577/577 [==============================] - 0s 627us/step - loss: 0.8645 - mean_absolute_error: 0.7355\n",
      "Epoch 38/50\n",
      "577/577 [==============================] - 0s 642us/step - loss: 0.9225 - mean_absolute_error: 0.7664\n",
      "Epoch 39/50\n",
      "577/577 [==============================] - 0s 628us/step - loss: 0.8999 - mean_absolute_error: 0.7337\n",
      "Epoch 40/50\n",
      "577/577 [==============================] - 0s 635us/step - loss: 0.7514 - mean_absolute_error: 0.6937\n",
      "Epoch 41/50\n",
      "577/577 [==============================] - 0s 639us/step - loss: 0.7673 - mean_absolute_error: 0.6804\n",
      "Epoch 42/50\n",
      "577/577 [==============================] - 0s 635us/step - loss: 0.8278 - mean_absolute_error: 0.7071\n",
      "Epoch 43/50\n",
      "577/577 [==============================] - 0s 642us/step - loss: 0.8414 - mean_absolute_error: 0.7271\n",
      "Epoch 44/50\n",
      "577/577 [==============================] - 0s 640us/step - loss: 0.7766 - mean_absolute_error: 0.6938\n",
      "Epoch 45/50\n",
      "577/577 [==============================] - 0s 627us/step - loss: 0.8185 - mean_absolute_error: 0.7022\n",
      "Epoch 46/50\n",
      "577/577 [==============================] - 0s 710us/step - loss: 0.7778 - mean_absolute_error: 0.6905\n",
      "Epoch 47/50\n",
      "577/577 [==============================] - 0s 702us/step - loss: 0.7404 - mean_absolute_error: 0.6735\n",
      "Epoch 48/50\n",
      "577/577 [==============================] - 0s 730us/step - loss: 0.8093 - mean_absolute_error: 0.6982\n",
      "Epoch 49/50\n",
      "577/577 [==============================] - 0s 707us/step - loss: 0.7553 - mean_absolute_error: 0.6841\n",
      "Epoch 50/50\n",
      "577/577 [==============================] - 0s 704us/step - loss: 0.8254 - mean_absolute_error: 0.7152\n",
      "QWK:  0.36485719689805185\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_73 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "573/573 [==============================] - 15s 26ms/step - loss: 13.1271 - mean_absolute_error: 3.0844\n",
      "Epoch 2/50\n",
      "573/573 [==============================] - 0s 556us/step - loss: 1.6861 - mean_absolute_error: 1.0609\n",
      "Epoch 3/50\n",
      "573/573 [==============================] - 0s 590us/step - loss: 1.4838 - mean_absolute_error: 0.9839\n",
      "Epoch 4/50\n",
      "573/573 [==============================] - 0s 632us/step - loss: 1.4310 - mean_absolute_error: 0.9432\n",
      "Epoch 5/50\n",
      "573/573 [==============================] - 0s 552us/step - loss: 1.2890 - mean_absolute_error: 0.9022\n",
      "Epoch 6/50\n",
      "573/573 [==============================] - 0s 614us/step - loss: 1.2042 - mean_absolute_error: 0.8804\n",
      "Epoch 7/50\n",
      "573/573 [==============================] - 0s 583us/step - loss: 1.1279 - mean_absolute_error: 0.8391\n",
      "Epoch 8/50\n",
      "573/573 [==============================] - 0s 642us/step - loss: 1.1365 - mean_absolute_error: 0.8448\n",
      "Epoch 9/50\n",
      "573/573 [==============================] - 0s 680us/step - loss: 1.0429 - mean_absolute_error: 0.8134\n",
      "Epoch 10/50\n",
      "573/573 [==============================] - 0s 660us/step - loss: 1.0081 - mean_absolute_error: 0.8003\n",
      "Epoch 11/50\n",
      "573/573 [==============================] - 0s 667us/step - loss: 1.0402 - mean_absolute_error: 0.8104\n",
      "Epoch 12/50\n",
      "573/573 [==============================] - 0s 654us/step - loss: 1.1458 - mean_absolute_error: 0.8424\n",
      "Epoch 13/50\n",
      "573/573 [==============================] - 0s 662us/step - loss: 0.9704 - mean_absolute_error: 0.7825\n",
      "Epoch 14/50\n",
      "573/573 [==============================] - 0s 660us/step - loss: 1.0016 - mean_absolute_error: 0.8175\n",
      "Epoch 15/50\n",
      "573/573 [==============================] - 0s 688us/step - loss: 0.9064 - mean_absolute_error: 0.7493\n",
      "Epoch 16/50\n",
      "573/573 [==============================] - 0s 682us/step - loss: 0.8937 - mean_absolute_error: 0.7437\n",
      "Epoch 17/50\n",
      "573/573 [==============================] - 0s 690us/step - loss: 0.8509 - mean_absolute_error: 0.7256\n",
      "Epoch 18/50\n",
      "573/573 [==============================] - 0s 625us/step - loss: 0.8712 - mean_absolute_error: 0.7418\n",
      "Epoch 19/50\n",
      "573/573 [==============================] - 0s 588us/step - loss: 0.9773 - mean_absolute_error: 0.7904\n",
      "Epoch 20/50\n",
      "573/573 [==============================] - 0s 585us/step - loss: 0.9005 - mean_absolute_error: 0.7513\n",
      "Epoch 21/50\n",
      "573/573 [==============================] - 0s 580us/step - loss: 0.9866 - mean_absolute_error: 0.7920\n",
      "Epoch 22/50\n",
      "573/573 [==============================] - 0s 581us/step - loss: 0.9134 - mean_absolute_error: 0.7653\n",
      "Epoch 23/50\n",
      "573/573 [==============================] - 0s 585us/step - loss: 0.8979 - mean_absolute_error: 0.7473\n",
      "Epoch 24/50\n",
      "573/573 [==============================] - 0s 571us/step - loss: 0.7875 - mean_absolute_error: 0.7105\n",
      "Epoch 25/50\n",
      "573/573 [==============================] - 0s 591us/step - loss: 0.8321 - mean_absolute_error: 0.7146\n",
      "Epoch 26/50\n",
      "573/573 [==============================] - 0s 592us/step - loss: 0.8774 - mean_absolute_error: 0.7431\n",
      "Epoch 27/50\n",
      "573/573 [==============================] - 0s 599us/step - loss: 0.8677 - mean_absolute_error: 0.7319\n",
      "Epoch 28/50\n",
      "573/573 [==============================] - 0s 589us/step - loss: 0.7659 - mean_absolute_error: 0.7134\n",
      "Epoch 29/50\n",
      "573/573 [==============================] - 0s 585us/step - loss: 0.8185 - mean_absolute_error: 0.7291\n",
      "Epoch 30/50\n",
      "573/573 [==============================] - 0s 630us/step - loss: 0.8765 - mean_absolute_error: 0.7441\n",
      "Epoch 31/50\n",
      "573/573 [==============================] - 0s 657us/step - loss: 0.8858 - mean_absolute_error: 0.7497\n",
      "Epoch 32/50\n",
      "573/573 [==============================] - 0s 690us/step - loss: 0.8234 - mean_absolute_error: 0.7139\n",
      "Epoch 33/50\n",
      "573/573 [==============================] - 0s 681us/step - loss: 0.8886 - mean_absolute_error: 0.7376\n",
      "Epoch 34/50\n",
      "573/573 [==============================] - 0s 669us/step - loss: 0.8631 - mean_absolute_error: 0.7303\n",
      "Epoch 35/50\n",
      "573/573 [==============================] - 0s 668us/step - loss: 0.6978 - mean_absolute_error: 0.6568\n",
      "Epoch 36/50\n",
      "573/573 [==============================] - 0s 677us/step - loss: 0.7882 - mean_absolute_error: 0.7165\n",
      "Epoch 37/50\n",
      "573/573 [==============================] - 0s 674us/step - loss: 0.7069 - mean_absolute_error: 0.6703\n",
      "Epoch 38/50\n",
      "573/573 [==============================] - 0s 695us/step - loss: 0.8600 - mean_absolute_error: 0.7348\n",
      "Epoch 39/50\n",
      "573/573 [==============================] - 0s 657us/step - loss: 0.7531 - mean_absolute_error: 0.6897\n",
      "Epoch 40/50\n",
      "573/573 [==============================] - 0s 670us/step - loss: 0.7731 - mean_absolute_error: 0.7095\n",
      "Epoch 41/50\n",
      "573/573 [==============================] - 0s 588us/step - loss: 0.7626 - mean_absolute_error: 0.6861\n",
      "Epoch 42/50\n",
      "573/573 [==============================] - 0s 576us/step - loss: 0.8074 - mean_absolute_error: 0.7052\n",
      "Epoch 43/50\n",
      "573/573 [==============================] - 0s 595us/step - loss: 0.7220 - mean_absolute_error: 0.6739\n",
      "Epoch 44/50\n",
      "573/573 [==============================] - 0s 578us/step - loss: 0.6893 - mean_absolute_error: 0.6545\n",
      "Epoch 45/50\n",
      "573/573 [==============================] - 0s 585us/step - loss: 0.7724 - mean_absolute_error: 0.6839\n",
      "Epoch 46/50\n",
      "573/573 [==============================] - 0s 596us/step - loss: 0.7343 - mean_absolute_error: 0.6829\n",
      "Epoch 47/50\n",
      "573/573 [==============================] - 0s 583us/step - loss: 0.6855 - mean_absolute_error: 0.6546\n",
      "Epoch 48/50\n",
      "573/573 [==============================] - 0s 589us/step - loss: 0.7592 - mean_absolute_error: 0.6971\n",
      "Epoch 49/50\n",
      "573/573 [==============================] - 0s 584us/step - loss: 0.7374 - mean_absolute_error: 0.6744\n",
      "Epoch 50/50\n",
      "573/573 [==============================] - 0s 597us/step - loss: 0.7636 - mean_absolute_error: 0.6996\n",
      "QWK:  0.6024887659868648\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_75 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_76 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "578/578 [==============================] - 17s 29ms/step - loss: 12.2433 - mean_absolute_error: 2.9105\n",
      "Epoch 2/50\n",
      "578/578 [==============================] - 0s 561us/step - loss: 1.7637 - mean_absolute_error: 1.0732\n",
      "Epoch 3/50\n",
      "578/578 [==============================] - 0s 560us/step - loss: 1.6143 - mean_absolute_error: 1.0059\n",
      "Epoch 4/50\n",
      "578/578 [==============================] - 0s 572us/step - loss: 1.4241 - mean_absolute_error: 0.9360\n",
      "Epoch 5/50\n",
      "578/578 [==============================] - 0s 561us/step - loss: 1.3759 - mean_absolute_error: 0.9204\n",
      "Epoch 6/50\n",
      "578/578 [==============================] - 0s 717us/step - loss: 1.2476 - mean_absolute_error: 0.8906\n",
      "Epoch 7/50\n",
      "578/578 [==============================] - 0s 739us/step - loss: 1.1859 - mean_absolute_error: 0.8782\n",
      "Epoch 8/50\n",
      "578/578 [==============================] - 0s 719us/step - loss: 1.2742 - mean_absolute_error: 0.9054\n",
      "Epoch 9/50\n",
      "578/578 [==============================] - 0s 732us/step - loss: 1.1180 - mean_absolute_error: 0.8257\n",
      "Epoch 10/50\n",
      "578/578 [==============================] - 0s 723us/step - loss: 1.0883 - mean_absolute_error: 0.8045\n",
      "Epoch 11/50\n",
      "578/578 [==============================] - 0s 756us/step - loss: 1.0164 - mean_absolute_error: 0.8158\n",
      "Epoch 12/50\n",
      "578/578 [==============================] - 0s 752us/step - loss: 0.9798 - mean_absolute_error: 0.7899\n",
      "Epoch 13/50\n",
      "578/578 [==============================] - 0s 739us/step - loss: 1.0485 - mean_absolute_error: 0.8042\n",
      "Epoch 14/50\n",
      "578/578 [==============================] - 0s 740us/step - loss: 1.0270 - mean_absolute_error: 0.8129\n",
      "Epoch 15/50\n",
      "578/578 [==============================] - 0s 689us/step - loss: 1.1132 - mean_absolute_error: 0.8311\n",
      "Epoch 16/50\n",
      "578/578 [==============================] - 0s 641us/step - loss: 0.9868 - mean_absolute_error: 0.7649\n",
      "Epoch 17/50\n",
      "578/578 [==============================] - 0s 665us/step - loss: 0.9374 - mean_absolute_error: 0.7428\n",
      "Epoch 18/50\n",
      "578/578 [==============================] - 0s 639us/step - loss: 0.8998 - mean_absolute_error: 0.7618\n",
      "Epoch 19/50\n",
      "578/578 [==============================] - 0s 661us/step - loss: 0.8972 - mean_absolute_error: 0.7359\n",
      "Epoch 20/50\n",
      "578/578 [==============================] - 0s 671us/step - loss: 0.9579 - mean_absolute_error: 0.7657\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s 657us/step - loss: 1.0464 - mean_absolute_error: 0.7997\n",
      "Epoch 22/50\n",
      "578/578 [==============================] - 0s 648us/step - loss: 0.9055 - mean_absolute_error: 0.7529\n",
      "Epoch 23/50\n",
      "578/578 [==============================] - 0s 611us/step - loss: 0.9674 - mean_absolute_error: 0.7632\n",
      "Epoch 24/50\n",
      "578/578 [==============================] - 0s 617us/step - loss: 0.9565 - mean_absolute_error: 0.7729\n",
      "Epoch 25/50\n",
      "578/578 [==============================] - 0s 713us/step - loss: 0.8945 - mean_absolute_error: 0.7379\n",
      "Epoch 26/50\n",
      "578/578 [==============================] - 0s 583us/step - loss: 0.8607 - mean_absolute_error: 0.7447\n",
      "Epoch 27/50\n",
      "578/578 [==============================] - 0s 587us/step - loss: 0.8347 - mean_absolute_error: 0.7199\n",
      "Epoch 28/50\n",
      "578/578 [==============================] - 0s 583us/step - loss: 0.8826 - mean_absolute_error: 0.7373\n",
      "Epoch 29/50\n",
      "578/578 [==============================] - 0s 603us/step - loss: 0.8533 - mean_absolute_error: 0.7259\n",
      "Epoch 30/50\n",
      "578/578 [==============================] - 0s 593us/step - loss: 0.9607 - mean_absolute_error: 0.7665\n",
      "Epoch 31/50\n",
      "578/578 [==============================] - 0s 571us/step - loss: 0.8612 - mean_absolute_error: 0.7085\n",
      "Epoch 32/50\n",
      "578/578 [==============================] - 0s 585us/step - loss: 0.9489 - mean_absolute_error: 0.7576\n",
      "Epoch 33/50\n",
      "578/578 [==============================] - 0s 565us/step - loss: 0.9048 - mean_absolute_error: 0.7547\n",
      "Epoch 34/50\n",
      "578/578 [==============================] - 0s 579us/step - loss: 0.9001 - mean_absolute_error: 0.7663\n",
      "Epoch 35/50\n",
      "578/578 [==============================] - 0s 589us/step - loss: 0.7752 - mean_absolute_error: 0.6979\n",
      "Epoch 36/50\n",
      "578/578 [==============================] - 0s 586us/step - loss: 0.8391 - mean_absolute_error: 0.7392\n",
      "Epoch 37/50\n",
      "578/578 [==============================] - 0s 599us/step - loss: 0.7859 - mean_absolute_error: 0.6930\n",
      "Epoch 38/50\n",
      "578/578 [==============================] - 0s 671us/step - loss: 0.8735 - mean_absolute_error: 0.7195\n",
      "Epoch 39/50\n",
      "578/578 [==============================] - 0s 697us/step - loss: 0.8779 - mean_absolute_error: 0.7363\n",
      "Epoch 40/50\n",
      "578/578 [==============================] - 0s 697us/step - loss: 0.9556 - mean_absolute_error: 0.7719\n",
      "Epoch 41/50\n",
      "578/578 [==============================] - 0s 687us/step - loss: 0.7601 - mean_absolute_error: 0.6935\n",
      "Epoch 42/50\n",
      "578/578 [==============================] - 0s 669us/step - loss: 0.8304 - mean_absolute_error: 0.7199\n",
      "Epoch 43/50\n",
      "578/578 [==============================] - 0s 695us/step - loss: 0.8785 - mean_absolute_error: 0.7494\n",
      "Epoch 44/50\n",
      "578/578 [==============================] - 0s 690us/step - loss: 0.7948 - mean_absolute_error: 0.6988\n",
      "Epoch 45/50\n",
      "578/578 [==============================] - 0s 745us/step - loss: 0.8788 - mean_absolute_error: 0.7336\n",
      "Epoch 46/50\n",
      "578/578 [==============================] - 0s 672us/step - loss: 0.7667 - mean_absolute_error: 0.6839\n",
      "Epoch 47/50\n",
      "578/578 [==============================] - 0s 704us/step - loss: 0.7362 - mean_absolute_error: 0.6690\n",
      "Epoch 48/50\n",
      "578/578 [==============================] - 0s 841us/step - loss: 0.7949 - mean_absolute_error: 0.7057\n",
      "Epoch 49/50\n",
      "578/578 [==============================] - 1s 948us/step - loss: 0.8130 - mean_absolute_error: 0.7077\n",
      "Epoch 50/50\n",
      "578/578 [==============================] - 0s 849us/step - loss: 0.7442 - mean_absolute_error: 0.6742\n",
      "QWK:  0.5621841528144169\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_77 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "582/582 [==============================] - 16s 27ms/step - loss: 12.0916 - mean_absolute_error: 2.8784\n",
      "Epoch 2/50\n",
      "582/582 [==============================] - 0s 576us/step - loss: 1.6786 - mean_absolute_error: 1.0139\n",
      "Epoch 3/50\n",
      "582/582 [==============================] - 0s 579us/step - loss: 1.4573 - mean_absolute_error: 0.9518\n",
      "Epoch 4/50\n",
      "582/582 [==============================] - 0s 586us/step - loss: 1.4017 - mean_absolute_error: 0.9386\n",
      "Epoch 5/50\n",
      "582/582 [==============================] - 0s 584us/step - loss: 1.3674 - mean_absolute_error: 0.9187\n",
      "Epoch 6/50\n",
      "582/582 [==============================] - 0s 592us/step - loss: 1.3124 - mean_absolute_error: 0.8996\n",
      "Epoch 7/50\n",
      "582/582 [==============================] - 0s 592us/step - loss: 1.2482 - mean_absolute_error: 0.8854\n",
      "Epoch 8/50\n",
      "582/582 [==============================] - 0s 581us/step - loss: 1.1693 - mean_absolute_error: 0.8591\n",
      "Epoch 9/50\n",
      "582/582 [==============================] - 0s 580us/step - loss: 1.1020 - mean_absolute_error: 0.8522\n",
      "Epoch 10/50\n",
      "582/582 [==============================] - 0s 569us/step - loss: 1.1025 - mean_absolute_error: 0.8425\n",
      "Epoch 11/50\n",
      "582/582 [==============================] - 0s 577us/step - loss: 0.9476 - mean_absolute_error: 0.7597\n",
      "Epoch 12/50\n",
      "582/582 [==============================] - 0s 592us/step - loss: 1.0835 - mean_absolute_error: 0.8180\n",
      "Epoch 13/50\n",
      "582/582 [==============================] - 0s 593us/step - loss: 1.0657 - mean_absolute_error: 0.8175\n",
      "Epoch 14/50\n",
      "582/582 [==============================] - 0s 703us/step - loss: 1.1084 - mean_absolute_error: 0.8297\n",
      "Epoch 15/50\n",
      "582/582 [==============================] - 0s 779us/step - loss: 1.0156 - mean_absolute_error: 0.8041\n",
      "Epoch 16/50\n",
      "582/582 [==============================] - 0s 816us/step - loss: 0.9762 - mean_absolute_error: 0.7914\n",
      "Epoch 17/50\n",
      "582/582 [==============================] - 0s 830us/step - loss: 1.0387 - mean_absolute_error: 0.8090\n",
      "Epoch 18/50\n",
      "582/582 [==============================] - 0s 766us/step - loss: 1.0215 - mean_absolute_error: 0.7930\n",
      "Epoch 19/50\n",
      "582/582 [==============================] - 0s 772us/step - loss: 1.0169 - mean_absolute_error: 0.7921\n",
      "Epoch 20/50\n",
      "582/582 [==============================] - 0s 758us/step - loss: 1.0485 - mean_absolute_error: 0.8064\n",
      "Epoch 21/50\n",
      "582/582 [==============================] - 0s 774us/step - loss: 0.8884 - mean_absolute_error: 0.7224\n",
      "Epoch 22/50\n",
      "582/582 [==============================] - 0s 779us/step - loss: 0.9134 - mean_absolute_error: 0.7538\n",
      "Epoch 23/50\n",
      "582/582 [==============================] - 0s 680us/step - loss: 0.8917 - mean_absolute_error: 0.7460\n",
      "Epoch 24/50\n",
      "582/582 [==============================] - 0s 684us/step - loss: 0.9289 - mean_absolute_error: 0.7557\n",
      "Epoch 25/50\n",
      "582/582 [==============================] - 0s 782us/step - loss: 0.8938 - mean_absolute_error: 0.7284\n",
      "Epoch 26/50\n",
      "582/582 [==============================] - 0s 672us/step - loss: 0.8799 - mean_absolute_error: 0.7376\n",
      "Epoch 27/50\n",
      "582/582 [==============================] - 0s 670us/step - loss: 0.9518 - mean_absolute_error: 0.7555\n",
      "Epoch 28/50\n",
      "582/582 [==============================] - 0s 698us/step - loss: 0.9901 - mean_absolute_error: 0.7981\n",
      "Epoch 29/50\n",
      "582/582 [==============================] - 0s 685us/step - loss: 0.9168 - mean_absolute_error: 0.7523\n",
      "Epoch 30/50\n",
      "582/582 [==============================] - 0s 695us/step - loss: 0.9356 - mean_absolute_error: 0.7612\n",
      "Epoch 31/50\n",
      "582/582 [==============================] - 0s 660us/step - loss: 0.8338 - mean_absolute_error: 0.7059\n",
      "Epoch 32/50\n",
      "582/582 [==============================] - 0s 693us/step - loss: 0.8583 - mean_absolute_error: 0.7315\n",
      "Epoch 33/50\n",
      "582/582 [==============================] - 0s 787us/step - loss: 0.8021 - mean_absolute_error: 0.7046\n",
      "Epoch 34/50\n",
      "582/582 [==============================] - 0s 756us/step - loss: 0.8284 - mean_absolute_error: 0.7255\n",
      "Epoch 35/50\n",
      "582/582 [==============================] - 0s 748us/step - loss: 0.8493 - mean_absolute_error: 0.7311\n",
      "Epoch 36/50\n",
      "582/582 [==============================] - 0s 715us/step - loss: 0.8511 - mean_absolute_error: 0.7492\n",
      "Epoch 37/50\n",
      "582/582 [==============================] - 0s 709us/step - loss: 0.8193 - mean_absolute_error: 0.7045\n",
      "Epoch 38/50\n",
      "582/582 [==============================] - 0s 697us/step - loss: 0.7655 - mean_absolute_error: 0.6850\n",
      "Epoch 39/50\n",
      "582/582 [==============================] - 0s 708us/step - loss: 0.8332 - mean_absolute_error: 0.7223\n",
      "Epoch 40/50\n",
      "582/582 [==============================] - 0s 698us/step - loss: 0.7942 - mean_absolute_error: 0.6997\n",
      "Epoch 41/50\n",
      "582/582 [==============================] - 0s 702us/step - loss: 0.8015 - mean_absolute_error: 0.7046\n",
      "Epoch 42/50\n",
      "582/582 [==============================] - 0s 682us/step - loss: 0.8123 - mean_absolute_error: 0.7132\n",
      "Epoch 43/50\n",
      "582/582 [==============================] - 0s 632us/step - loss: 0.8109 - mean_absolute_error: 0.7137\n",
      "Epoch 44/50\n",
      "582/582 [==============================] - 0s 632us/step - loss: 0.7921 - mean_absolute_error: 0.6905\n",
      "Epoch 45/50\n",
      "582/582 [==============================] - 0s 639us/step - loss: 0.7823 - mean_absolute_error: 0.6877\n",
      "Epoch 46/50\n",
      "582/582 [==============================] - 0s 630us/step - loss: 0.8354 - mean_absolute_error: 0.7046\n",
      "Epoch 47/50\n",
      "582/582 [==============================] - 0s 632us/step - loss: 0.6902 - mean_absolute_error: 0.6599\n",
      "Epoch 48/50\n",
      "582/582 [==============================] - 0s 638us/step - loss: 0.7038 - mean_absolute_error: 0.6737\n",
      "Epoch 49/50\n",
      "582/582 [==============================] - 0s 630us/step - loss: 0.7466 - mean_absolute_error: 0.6925\n",
      "Epoch 50/50\n",
      "582/582 [==============================] - 0s 641us/step - loss: 0.8368 - mean_absolute_error: 0.7261\n",
      "QWK:  0.4815548102953655\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_79 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_80 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "582/582 [==============================] - 18s 31ms/step - loss: 12.9396 - mean_absolute_error: 3.0658\n",
      "Epoch 2/50\n",
      "582/582 [==============================] - 0s 647us/step - loss: 1.7810 - mean_absolute_error: 1.0679\n",
      "Epoch 3/50\n",
      "582/582 [==============================] - 0s 636us/step - loss: 1.4508 - mean_absolute_error: 0.9546\n",
      "Epoch 4/50\n",
      "582/582 [==============================] - 0s 651us/step - loss: 1.3760 - mean_absolute_error: 0.9074\n",
      "Epoch 5/50\n",
      "582/582 [==============================] - 0s 636us/step - loss: 1.3378 - mean_absolute_error: 0.9228\n",
      "Epoch 6/50\n",
      "582/582 [==============================] - 0s 650us/step - loss: 1.2565 - mean_absolute_error: 0.8829\n",
      "Epoch 7/50\n",
      "582/582 [==============================] - 0s 645us/step - loss: 1.2848 - mean_absolute_error: 0.8898\n",
      "Epoch 8/50\n",
      "582/582 [==============================] - 0s 566us/step - loss: 1.2532 - mean_absolute_error: 0.8759\n",
      "Epoch 9/50\n",
      "582/582 [==============================] - 0s 552us/step - loss: 1.1744 - mean_absolute_error: 0.8690\n",
      "Epoch 10/50\n",
      "582/582 [==============================] - 0s 548us/step - loss: 1.0960 - mean_absolute_error: 0.8254\n",
      "Epoch 11/50\n",
      "582/582 [==============================] - 0s 539us/step - loss: 1.1018 - mean_absolute_error: 0.8226\n",
      "Epoch 12/50\n",
      "582/582 [==============================] - 0s 549us/step - loss: 1.0513 - mean_absolute_error: 0.7926\n",
      "Epoch 13/50\n",
      "582/582 [==============================] - 0s 557us/step - loss: 1.0366 - mean_absolute_error: 0.7983\n",
      "Epoch 14/50\n",
      "582/582 [==============================] - 0s 554us/step - loss: 1.0438 - mean_absolute_error: 0.7998\n",
      "Epoch 15/50\n",
      "582/582 [==============================] - 0s 551us/step - loss: 0.9779 - mean_absolute_error: 0.7840\n",
      "Epoch 16/50\n",
      "582/582 [==============================] - 0s 551us/step - loss: 1.0261 - mean_absolute_error: 0.7796\n",
      "Epoch 17/50\n",
      "582/582 [==============================] - 0s 542us/step - loss: 1.0260 - mean_absolute_error: 0.7949\n",
      "Epoch 18/50\n",
      "582/582 [==============================] - 0s 553us/step - loss: 0.9522 - mean_absolute_error: 0.7707\n",
      "Epoch 19/50\n",
      "582/582 [==============================] - 0s 550us/step - loss: 0.9029 - mean_absolute_error: 0.7587\n",
      "Epoch 20/50\n",
      "582/582 [==============================] - 0s 588us/step - loss: 0.9878 - mean_absolute_error: 0.7822\n",
      "Epoch 21/50\n",
      "582/582 [==============================] - 0s 641us/step - loss: 0.9039 - mean_absolute_error: 0.7638\n",
      "Epoch 22/50\n",
      "582/582 [==============================] - 0s 635us/step - loss: 0.8616 - mean_absolute_error: 0.7378\n",
      "Epoch 23/50\n",
      "582/582 [==============================] - 0s 656us/step - loss: 1.0004 - mean_absolute_error: 0.7947\n",
      "Epoch 24/50\n",
      "582/582 [==============================] - 0s 631us/step - loss: 0.9510 - mean_absolute_error: 0.7588\n",
      "Epoch 25/50\n",
      "582/582 [==============================] - 0s 641us/step - loss: 0.8604 - mean_absolute_error: 0.7323\n",
      "Epoch 26/50\n",
      "582/582 [==============================] - 0s 657us/step - loss: 0.8404 - mean_absolute_error: 0.7291\n",
      "Epoch 27/50\n",
      "582/582 [==============================] - 0s 641us/step - loss: 0.8669 - mean_absolute_error: 0.7335\n",
      "Epoch 28/50\n",
      "582/582 [==============================] - 0s 645us/step - loss: 0.8531 - mean_absolute_error: 0.7246\n",
      "Epoch 29/50\n",
      "582/582 [==============================] - 0s 648us/step - loss: 0.9130 - mean_absolute_error: 0.7442\n",
      "Epoch 30/50\n",
      "582/582 [==============================] - 0s 640us/step - loss: 0.8730 - mean_absolute_error: 0.7284\n",
      "Epoch 31/50\n",
      "582/582 [==============================] - 0s 781us/step - loss: 0.8925 - mean_absolute_error: 0.7397\n",
      "Epoch 32/50\n",
      "582/582 [==============================] - 0s 834us/step - loss: 0.8360 - mean_absolute_error: 0.7231\n",
      "Epoch 33/50\n",
      "582/582 [==============================] - 1s 864us/step - loss: 0.8261 - mean_absolute_error: 0.7147\n",
      "Epoch 34/50\n",
      "582/582 [==============================] - 0s 818us/step - loss: 0.7980 - mean_absolute_error: 0.7125\n",
      "Epoch 35/50\n",
      "582/582 [==============================] - 0s 838us/step - loss: 0.8005 - mean_absolute_error: 0.7034\n",
      "Epoch 36/50\n",
      "582/582 [==============================] - 1s 877us/step - loss: 0.8231 - mean_absolute_error: 0.7257\n",
      "Epoch 37/50\n",
      "582/582 [==============================] - 1s 869us/step - loss: 0.8099 - mean_absolute_error: 0.7024\n",
      "Epoch 38/50\n",
      "582/582 [==============================] - 0s 819us/step - loss: 0.8023 - mean_absolute_error: 0.7065\n",
      "Epoch 39/50\n",
      "582/582 [==============================] - 0s 730us/step - loss: 0.8167 - mean_absolute_error: 0.6995\n",
      "Epoch 40/50\n",
      "582/582 [==============================] - 0s 725us/step - loss: 0.9615 - mean_absolute_error: 0.7656\n",
      "Epoch 41/50\n",
      "582/582 [==============================] - 0s 746us/step - loss: 0.8096 - mean_absolute_error: 0.7267\n",
      "Epoch 42/50\n",
      "582/582 [==============================] - 0s 720us/step - loss: 0.8601 - mean_absolute_error: 0.7296\n",
      "Epoch 43/50\n",
      "582/582 [==============================] - 0s 740us/step - loss: 0.8130 - mean_absolute_error: 0.7173\n",
      "Epoch 44/50\n",
      "582/582 [==============================] - 0s 742us/step - loss: 0.7241 - mean_absolute_error: 0.6747\n",
      "Epoch 45/50\n",
      "582/582 [==============================] - 0s 755us/step - loss: 0.8216 - mean_absolute_error: 0.7181\n",
      "Epoch 46/50\n",
      "582/582 [==============================] - 0s 725us/step - loss: 0.8035 - mean_absolute_error: 0.7156\n",
      "Epoch 47/50\n",
      "582/582 [==============================] - 0s 710us/step - loss: 0.7708 - mean_absolute_error: 0.6793\n",
      "Epoch 48/50\n",
      "582/582 [==============================] - 0s 712us/step - loss: 0.7967 - mean_absolute_error: 0.6943\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582/582 [==============================] - 0s 633us/step - loss: 0.7433 - mean_absolute_error: 0.6772\n",
      "Epoch 50/50\n",
      "582/582 [==============================] - 0s 637us/step - loss: 0.7883 - mean_absolute_error: 0.7095\n",
      "QWK:  0.5268289236638525\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.5076\n"
     ]
    }
   ],
   "source": [
    "Sum=[]\n",
    "for data in Set: \n",
    "#     print(data.head())\n",
    "    y=pd.DataFrame()\n",
    "    X=pd.DataFrame()\n",
    "    X = data\n",
    "    y['domain1_score']=X['domain1_score']\n",
    "    X = X.replace('NaN', 0)\n",
    "    y['domain1_score']=y['domain1_score'].fillna(0.0).astype(int)\n",
    "#     print(X.head())\n",
    "#     print(y.head())\n",
    "    dataset = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    dataset = dataset.split(X, y)\n",
    "    results_sum = train_model(X, y, dataset)\n",
    "    results_sum=np.around(np.array(results_sum).mean(),decimals=4)\n",
    "    Sum.append(results_sum)\n",
    "    print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec \",results_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6655, 0.683, 0.5929, 0.7757, 0.7939, 0.7729, 0.6579, 0.5076]\n"
     ]
    }
   ],
   "source": [
    "print(Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Min_Max=[0.6932, 0.6717, 0.5925, 0.747, 0.7897, 0.7866, 0.6623, 0.4241]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Model    set1    set2    set3  \\\n",
      "0  Average Quadratic Weighted Kappa after 5-fold ...  0.6932  0.6717  0.5925   \n",
      "1  Average Quadratic Weighted Kappa after 5-fold ...  0.6655  0.6830  0.5929   \n",
      "\n",
      "     set4    set5    set6    set7    set8  \n",
      "0  0.7470  0.7897  0.7866  0.6623  0.4241  \n",
      "1  0.7757  0.7939  0.7729  0.6579  0.5076  \n"
     ]
    }
   ],
   "source": [
    "results_lstm=pd.DataFrame(columns=['Model','set1','set2','set3','set4','set5','set6','set7','set8'])\n",
    "results_lstm = results_lstm.append(pd.Series(Min_Max, index=['set1','set2','set3','set4','set5','set6','set7','set8']), ignore_index=True)\n",
    "results_lstm = results_lstm.append(pd.Series(Sum, index=['set1','set2','set3','set4','set5','set6','set7','set8']), ignore_index=True)\n",
    "results_lstm.Model=['Average Quadratic Weighted Kappa after 5-fold cross validation for min+max word2vec','Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec ']\n",
    "print (results_lstm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "writer = ExcelWriter('Results_LSTM_setwise.xlsx')\n",
    "results_lstm.to_excel(writer,'Sheet1',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
